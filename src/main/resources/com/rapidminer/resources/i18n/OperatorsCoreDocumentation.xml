<?xml version="1.0" encoding="windows-1252" standalone="no"?>
<operatorHelp lang="en_EN">
   <group>
      <key/>
      <name>Core</name>
   </group>
   <group>
      <key>blending.attributes.names_and_roles</key>
      <name>Names &amp; Roles</name>
   </group>
   	<group>
      <key>data_access.nosql</key>
      <name>NoSQL</name>
   </group>
   <operator>
      <key>write_special</key>
      <name>Write Special Format</name>
      <synopsis>Writes attributes in a special user defined format.</synopsis>
      <help> Using the parameter 'special_format', the user can specify the exact format. The $ sign
         has a special meaning and introduces a command (the following character) Additional
         arguments to this command may be supplied enclosing it in square brackets. &lt;dl&gt;
         &lt;dt&gt;$a:&lt;/dt&gt; &lt;dd&gt; All attributes separated by the default
         separator&lt;/dd&gt; &lt;dt&gt;$a[separator]:&lt;/dt&gt; &lt;dd&gt; All attributes
         separated by separator&lt;/dd&gt; &lt;dt&gt;$s[separator][indexSeparator]:&lt;/dt&gt;
         &lt;dd&gt; Sparse format. For all non zero attributes the following strings are
         concatenated: the column index, the value of indexSeparator, the attribute value.
         Attributes are separated by separator.&lt;/dd&gt; &lt;dt&gt;$v[name]:&lt;/dt&gt; &lt;dd&gt;
         The value of the attribute with the given name (both regular and special
         attributes)&lt;/dd&gt; &lt;dt&gt;$k[index]:&lt;/dt&gt; &lt;dd&gt; The value of the
         attribute with the given index&lt;/dd&gt; &lt;dt&gt;$l:&lt;/dt&gt; &lt;dd&gt; The
         label&lt;/dd&gt; &lt;dt&gt;$p:&lt;/dt&gt; &lt;dd&gt; The predicted label&lt;/dd&gt;
         &lt;dt&gt;$d:&lt;/dt&gt; &lt;dd&gt; All prediction confidences for all classes in the form
         conf(class)=value&lt;/dd&gt; &lt;dt&gt;$d[class]:&lt;/dt&gt; &lt;dd&gt; The prediction
         confidence for the defined class as a simple number&lt;/dd&gt; &lt;dt&gt;$i:&lt;/dt&gt;
         &lt;dd&gt; The id&lt;/dd&gt; &lt;dt&gt;$w:&lt;/dt&gt; &lt;dd&gt; The weight&lt;/dd&gt;
         &lt;dt&gt;$b:&lt;/dt&gt; &lt;dd&gt; The batch number&lt;/dd&gt; &lt;dt&gt;$n:&lt;/dt&gt;
         &lt;dd&gt; The newline character&lt;/dd&gt; &lt;dt&gt;$t:&lt;/dt&gt; &lt;dd&gt; The
         tabulator character&lt;/dd&gt; &lt;dt&gt;$$:&lt;/dt&gt; &lt;dd&gt; The dollar
         sign&lt;/dd&gt; &lt;dt&gt;$[:&lt;/dt&gt; &lt;dd&gt; The '[' character&lt;/dd&gt;
         &lt;dt&gt;$]:&lt;/dt&gt; &lt;dd&gt; The ']' character&lt;/dd&gt; &lt;/dl&gt; Make sure the
         format string ends with $n if you want examples to be separated by newlines. </help>
   </operator>
   <operator>
      <name>ExperimentEmbedder</name>
      <synopsis>This operator embeds a complete experiment previously written into a file.
      </synopsis>
      <help>This operator can be used to embed a complete process definition into the current
         process definition. The process must have been written into a file before and will be
         loaded and executed when the current process reaches this operator. Optionally, the input
         of this operator can be used as input for the embedded process. In both cases, the output
         of the process will be delivered as output of this operator. Please note that validation
         checks will not work for process containing an operator of this type since the check cannot
         be performed without actually loading the process.</help>
   </operator>
   <operator>
      <name>Sort by Pareto Rank</name>
      <synopsis>This operator sorts the given example set according a set of attributes such that
         dominated examples will be sorted after non-dominated examples.</synopsis>
      <help/>
      <key>sort_by_pareto_rank</key>
   </operator>
   <operator>
      <name>BasicRuleLearner</name>
      <synopsis>Learns a set of rules minimizing the training error without pruning.</synopsis>
      <help>This operator builds an unpruned rule set of classification rules. It is based on the
         paper Cendrowska, 1987: PRISM: An algorithm for inducing modular rules.</help>
      <key>basicrulelearner</key>
   </operator>
   <operator>
      <name>Support Vector Machine (LibSVM)</name>
      <synopsis>This operators is an SVM Learner based on the Java libsvm, an SVM learner.</synopsis>
      <help>&lt;p&gt;Applies the &lt;a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm"&gt;libsvm&lt;/a&gt;
         learner by Chih-Chung Chang and Chih-Jen Lin. The SVM is a powerful method for both
         classification and regression. This operator supports the SVM types
         &lt;code&gt;C-SVC&lt;/code&gt; and &lt;code&gt;nu-SVC&lt;/code&gt; for classification tasks
         as well as &lt;code&gt;epsilon-SVR&lt;/code&gt; and &lt;code&gt;nu-SVR&lt;/code&gt; for regression
         tasks.
     &lt;br /&gt;
     Additionally &lt;code&gt;one-class&lt;/code&gt; gives the possibility to learn from just one class of examples and lateron test if new examples match the knowing ones.&lt;/p&gt;
     
     &lt;p&gt;&lt;br/&gt;In contrast to other SVM learners, the libsvm supports also internal multiclass learning and probability estimation based on Platt scaling
         for proper confidence values after applying the learned model on a classification data set.&lt;/p&gt;
      </help>
      <key>support_vector_machine_libsvm</key>
      <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Regression</tag>
         <tag>Model</tag>
         <tag>SVM</tag>
         <tag>Margin</tag>
         <tag>Oneclass</tag>
         <tag>One-class</tag>
      </tags>
      <shortName>SVM</shortName>
   </operator>
   <operator>
      <name>Random Tree</name>
      <synopsis>Learns a single decision tree. For each split only a random subset of attributes is
         available.</synopsis>
      <help>&lt;p&gt; This operator learns decision trees from both nominal and numerical data.
         Decision trees are powerful classification methods which often can also easily be
         understood. The random tree learner works similar to Quinlan's C4.5 or CART but it selects
         a random subset of attributes before it is applied. The size of the subset is defined by
         the parameter subset_ratio. &lt;/p&gt;</help>
      <key>random_tree</key>
      <tags>
         <tag>Id3</tag>
         <tag>J48</tag>
         <tag>J4.8</tag>
         <tag>C45</tag>
         <tag>C4.5</tag>
         <tag>C50</tag>
         <tag>C5.0</tag>
         <tag>Cart</tag>
         <tag>Chaid</tag>
         <tag>Decision</tag>
      </tags>
   </operator>
   <operator>
      <name>Numerical to Polynominal</name>
      <synopsis>Maps all numeric values simply to the corresponding nominal values. Please use one
         of the discretization operators if you need more sophisticated nominalization methods.
      </synopsis>
      <help>Converts all numerical attributes to nominal ones. Each numerical value is simply used
         as nominal value of the new attribute. If the value is missing, the new value will be
         missing. Please note that this operator might drastically increase memory usage if many
         different numerical values are used. Please use the available discretization operators
         then.</help>
      <key>numerical_to_polynominal</key>
      <tags>
         <tag>Continous</tag>
         <tag>Categorical</tag>
         <tag>Nominal</tag>
         <tag>Polynominal</tag>
         <tag>Ordinary</tag>
         <tag>Types</tag>
         <tag>Qualitative</tag>
         <tag>Quantitative</tag>
      </tags>
   </operator>
   <operator>
      <name>Loop Batches</name>
      <synopsis>Creates batches from the input examples and performs its inner operators on each of
         these batches which might be useful for applying methods on very large data sets directly
         in databases.</synopsis>
      <help>This operator groups the input examples into batches of the specified size and performs
         the inner operators on all batches subsequently. This might be useful for very large data
         sets which cannot be load into memory but must be handled in a database. In these cases,
         preprocessing methods or model applications and other tasks can be performed on each batch
         and the result might be again written into a database table (by using the
         DatabaseExampleSetWriter in its append mode). &lt;br/&gt; Note that the output of this
         operator is not composed of the results of the nested subprocess. In fact the subprocess
         does not need to deliver any output since it operates on a subset view of the input example
         set.</help>
      <key>loop_batches</key>
   </operator>
   <operator>
        <name>Generate Prediction</name>
        <synopsis>Generates a prediction attribute from given confidence attributes.</synopsis>
        <help>This operator generates a prediction attribute from given confidence attributes. As
        	  a prediction, the value that has the highest associated confidence attribute value is set
        	  for each example.</help>
        <key>generate_prediction</key>
   </operator>
   <operator>
        <name>Generate Prediction Ranking</name>
        <synopsis>Generates attributes containing the most probable classes and their corresponding confidence values.</synopsis>
        <help>This operator will use the confidence values generated during model application to create new 
        attributes which contains a ranking of the most probable classes and their corresponding confidence values.&lt;br /&gt;
        The size of the ranking (i.e. number of classes stored per example) can be specified. Additionally, you can choose if
        the old prediction and confidence attributes should be removed after the creation of the ranking attributes.</help>
        <key>generate_prediction_ranking</key>
   </operator>
   <operator>
      <name>Clone Parameters</name>
      <synopsis>Applies a set of parameters of a source operator on a target operator.</synopsis>
      <help>Sets a list of parameters using existing parameter values. &lt;br/&gt; The operator is
         similar to &lt;i&gt;ParameterSetter&lt;/i&gt;, but differs from that in not requiring a
         ParameterSet input. It simply reads a parameter value from a source and uses it to set the
         parameter value of a target parameter. Both, source and target, are given in the format
         'operator'.'parameter'. &lt;br/&gt; This operator is more general than ParameterSetter and
         could completely replace it. It is most useful, if you need a parameter which is optimized
         more than once within the optimization loop - ParameterSetter cannot be used here.
         &lt;br/&gt; These parameters can either be generated by a
         &lt;i&gt;ParameterOptimizationOperator&lt;/i&gt; or read by a
         &lt;i&gt;ParameterSetLoader&lt;/i&gt;. This operator is useful, e.g. in the following
         scenario. If one wants to find the best parameters for a certain learning scheme, one
         usually is also interested in the model generated with this parameters. While the first is
         easily possible using a &lt;i&gt;ParameterOptimizationOperator&lt;/i&gt;, the latter is not
         possible because the &lt;i&gt;ParameterOptimizationOperator&lt;/i&gt; does not return the
         IOObjects produced within, but only a parameter set. This is, because the parameter
         optimization operator knows nothing about models, but only about the performance vectors
         produced within. Producing performance vectors does not necessarily require a model.
         &lt;br/&gt; To solve this problem, one can use a &lt;code&gt;ParameterSetter&lt;/code&gt;.
         Usually, a process definition with a &lt;code&gt;ParameterSetter&lt;/code&gt; contains at
         least two operators of the same type, typically a learner. One learner may be an inner
         operator of the &lt;i&gt;ParameterOptimizationOperator&lt;/i&gt; and may be named
         &amp;quot;Learner&amp;quot;, whereas a second learner of the same type named
         &amp;quot;OptimalLearner&amp;quot; follows the parameter optimization and should use the
         optimal parameter set found by the optimization. In order to make the
         &lt;code&gt;ParameterSetter&lt;/code&gt; set the optimal parameters of the right operator,
         one must specify its name. Therefore, the parameter list &lt;var&gt;name_map&lt;/var&gt;
         was introduced. Each parameter in this list maps the name of an operator that was used
         during optimization (in our case this is &amp;quot;Learner&amp;quot;) to an operator that
         should now use these parameters (in our case this is &amp;quot;OptimalLearner&amp;quot;).
      </help>
      <key>clone_parameters</key>
   </operator>
    <operator>
        <name>Weight by PCA</name>
        <synopsis>This operator uses the factors of a PCA component (usually
            the first) as feature weights.</synopsis>
        <help>Uses the factors of one of the principal components (default is
            the first) as feature weights. Please note that the PCA weighting
            operator is currently the only one which also works on data sets
            without a label, i.e. for unsupervised learning.</help>
    <key>weight_by_pca</key>
  </operator>
  <operator>
    <name>pca</name>
    <synopsis>Deprecated variant of Principal Component Analysis</synopsis>
    <help>Deprecated variant of Principal Component Analysis</help>
    <key>pca</key>
    <deprecation>Since 5.0</deprecation>
  </operator>
    <operator>
        <name>FixedSplitValidation</name>
        <synopsis>A FixedSplitValidation splits up the example set at a fixed
            point into a training and test set and evaluates the model.
        </synopsis>
        <help>&lt;p&gt; A FixedSplitValidationChain splits up the example set
            at a fixed point into a training and test set and evaluates the model
            (linear sampling). For non-linear sampling methods, i.e. the data is
            shuffled, the specified amounts of data are used as training and test
            set. The sum of both must be smaller than the input example set size.
            &lt;/p&gt; &lt;p&gt; At least either the training set size must be
            specified (rest is used for testing) or the test set size must be
            specified (rest is used for training). If both are specified, the
            rest is not used at all. &lt;/p&gt; &lt;p&gt; The first inner
            operator must accept an &lt;i&gt;ExampleSet&lt;/i&gt; while the
            second must accept an &lt;i&gt;ExampleSet&lt;/i&gt; and the output of
            the first (which in most cases is a &lt;i&gt;Model&lt;/i&gt;) and
            must produce a &lt;i&gt;PerformanceVector&lt;/i&gt;. &lt;/p&gt;
            &lt;p&gt;This validation operator provides several values which can
            be logged by means of a &lt;i&gt;ProcessLogOperator&lt;/i&gt;. All
            performance estimation operators of RapidMiner provide access to the
            average values calculated during the estimation. Since the operator
            cannot ensure the names of the delivered criteria, the ProcessLog
            operator can access the values via the generic value names:&lt;/p&gt;
            &lt;ul&gt; &lt;li&gt;performance: the value for the main criterion
            calculated by this validation operator&lt;/li&gt;
            &lt;li&gt;performance1: the value of the first criterion of the
            performance vector calculated&lt;/li&gt; &lt;li&gt;performance2: the
            value of the second criterion of the performance vector
            calculated&lt;/li&gt; &lt;li&gt;performance3: the value of the third
            criterion of the performance vector calculated&lt;/li&gt;
            &lt;li&gt;for the main criterion, also the variance and the standard
            deviation can be accessed where applicable.&lt;/li&gt; &lt;/ul&gt;
        </help>
    <key>fixedsplitvalidation</key>
  </operator>
    <operator>
        <name>Print to Console</name>
        <key>print_to_console</key>
        <synopsis>Prints the given parameter value to the console.</synopsis>
        <help>This operator prints the given parameter value to the consoles. The parameter value
        may contain macros. E.g. you may print the operator apply count using the %{a} macro.
        This operator is escpecially helpful when processes are executed in the background (i.e.
        without a graphical user interface) or as services in order to protocol the process progress.</help>
    </operator>
    <operator>
        <name>Remove Unused Values</name>
        <key>remove_unused_values</key>
        <synopsis>This operator will remove each nominal value that is not assigned to any example.</synopsis>
        <help>This operator will remove each nominal value that is not assigned to any example. For reproducing this step during application time, you can apply the returned preprocessing model. If unknown nominal values will occur during application time, they will be set to missing. Please keep that in mind.
        &lt;br/&gt;
        Additionally you can select if the kept nominal values should be sorted alphabetically. This can be used for plotting purpose. 
        </help>
    </operator>
    <operator>
        <name>Delay</name>
        <key>delay</key>
        <tags>
        	<tag>Pause</tag>
        	<tag>Wait</tag>
       	</tags>
        <synopsis>Delays process execution.</synopsis>
        <help>This operator delays the process execution by a fixed or random amount of time.</help>
    </operator>
    <operator>
        <name>Batch-X-Validation</name>
        <synopsis>A batched cross-validation in order to estimate the
            performance of a learning operator according to predefined example
            batches.</synopsis>
        <help>&lt;p&gt; &lt;code&gt;BatchXValidation&lt;/code&gt; encapsulates
            a cross-validation process. The example set &lt;i&gt;S&lt;/i&gt; is
            split up into &lt;var&gt; number_of_validations&lt;/var&gt; subsets
            &lt;i&gt;S_i&lt;/i&gt;. The inner operators are applied
            &lt;var&gt;number_of_validations&lt;/var&gt; times using
            &lt;i&gt;S_i&lt;/i&gt; as the test set (input of the second inner
            operator) and &lt;i&gt;Sbackslash S_i&lt;/i&gt; training set (input
            of the first inner operator). &lt;/p&gt; &lt;p&gt;In contrast to the
            usual cross validation operator (see &lt;i&gt;XValidation&lt;/i&gt;)
            this operator does not (randomly) split the data itself but uses the
            partition defined by the special attribute &amp;quot;batch&amp;quot;.
            This can be an arbitrary nominal or integer attribute where each
            possible value occurs at least once (since many learning schemes
            depend on this minimum number of examples). &lt;/p&gt; &lt;p&gt; The
            first inner operator must accept an &lt;i&gt;ExampleSet&lt;/i&gt;
            while the second must accept an &lt;i&gt;ExampleSet&lt;/i&gt; and the
            output of the first (which is in most cases a
            &lt;i&gt;Model&lt;/i&gt;) and must produce a
            &lt;i&gt;PerformanceVector&lt;/i&gt;. &lt;/p&gt; &lt;p&gt;The cross
            validation operator provides several values which can be logged by
            means of a &lt;i&gt;ProcessLogOperator&lt;/i&gt;. Of course the
            number of the current iteration can be logged which might be useful
            for ProcessLog operators wrapped inside a cross validation. Beside
            that, all performance estimation operators of RapidMiner provide
            access to the average values calculated during the estimation. Since
            the operator cannot ensure the names of the delivered criteria, the
            ProcessLog operator can access the values via the generic value
            names:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;performance: the value for the
            main criterion calculated by this validation operator&lt;/li&gt;
            &lt;li&gt;performance1: the value of the first criterion of the
            performance vector calculated&lt;/li&gt; &lt;li&gt;performance2: the
            value of the second criterion of the performance vector
            calculated&lt;/li&gt; &lt;li&gt;performance3: the value of the third
            criterion of the performance vector calculated&lt;/li&gt;
            &lt;li&gt;for the main criterion, also the variance and the standard
            deviation can be accessed where applicable.&lt;/li&gt; &lt;/ul&gt;
        </help>
    <key>batch_x_validation</key>
    <shortName>Validation</shortName>
  </operator>
    <operator>
        <name>Rename by Replacing</name>
        <synopsis>This operator can be used to rename a set of attributes by
            replacing parts of the attribute names by a specified replacement.
        </synopsis>
        <help>&lt;p&gt;This operator replaces parts of the attribute names
            (like whitespaces, parentheses, or other unwanted characters) by a
            specified replacement. The replace_what parameter can be defined as a
            regular expression (please refer to the annex of the RapidMiner
            tutorial for a description). The replace_by parameter can be defined
            as an arbitrary string. Empty strings are also allowed. Capturing
            groups of the defined regular expression can be accessed with $1, $2,
            $3...&lt;/p&gt;</help>
    <key>rename_by_replacing</key>
    <tags>
         <tag>Names</tag>
         <tag>Regex</tag>
         <tag>Regular expressions</tag>
    </tags>
  </operator>
    <operator>
        <name>Rename by Constructions</name>
        <synopsis>This operator renames all regular attributes by their
            construction descriptions if available.</synopsis>
        <help>&lt;p&gt; This operator replaces the names of the regular
            attributes by the corresponding construction descriptions if the
            attribute was constructed at all. &lt;/p&gt;</help>
    <key>rename_by_constructions</key>
  </operator>
    <operator>
        <name>Select by Random</name>
        <synopsis>This operator simply selects a random or a predefined number
            of random features.</synopsis>
        <help>This operator selects a randomly chosen number of features
            randomly from the input example set. This can be useful in
            combination with a ParameterIteration operator or can be used as a
            baseline for significance test comparisons for feature selection
            techniques.</help>
    <key>select_by_random</key>
  </operator>
  <operator>
  	<name>Item Sets to Data</name>
  	<synopsis>Converts frequent item sets into data set.</synopsis>
  	<help>Converts frequent item sets into a data set which contains both
  		the rules, support and frequency values, a score measuring set interestingness
  		as well as (optionally) indicators for item inclusion.
  	</help>
  	<key>item_sets_to_data</key>
  </operator>
    <operator>
        <name>Unify Item Sets</name>
        <synopsis>Compares sets of frequent item sets and removes common not
            unique sets.</synopsis>
        <help>This operator compares a number of FrequentItemSet sets and
            removes every not unique FrequentItemSet.</help>
    <key>unify_item_sets</key>
  </operator>
    <operator>
        <name>Generate Massive Data</name>
        <synopsis>Generates huge amounts of data for testing purposes.
        </synopsis>
        <help>Generates huge amounts of data in either sparse or dense format.
            This operator can be used to check if huge amounts of data can be
            handled by RapidMiner for a given process setup without creating the
            correct format / writing special purpose input operators.</help>
    <key>generate_massive_data</key>
  </operator>
    <operator>
        <name>OperatorEnabler</name>
        <synopsis>This operator can be used to automatically enable or disable
            inner operators.</synopsis>
        <help>This operator can be used to enable and disable other operators.
            The operator which should be enabled or disabled must be a child
            operator of this one. Together with one of the parameter optimizing
            or iterating operators this operator can be used to dynamically
            change the process setup which might be useful in order to test
            different layouts, e.g. the gain by using different preprocessing
            steps.</help>
    <key>operatorenabler</key>
  </operator>
  <operator>
        <name>Subprocess</name>
        <synopsis>A chain of operators that is subsequently applied.
        </synopsis>
        <help>A simple operator chain which can have an arbitrary number of
            inner operators. The operators are subsequently applied and their
            output is used as input for the succeeding operator. The input of the
            operator chain is used as input for the first inner operator and the
            output of the last operator is used as the output of the operator
            chain.</help>
    <key>subprocess</key>
    <tags>
         <tag>Group</tag>
         <tag>Sub-process</tag>
         <tag>Routine</tag>
         <tag>Function</tag>
         <tag>Building</tag>
         <tag>Block</tag>
         <tag>Process</tag>
    </tags>
  </operator>
    <operator>
        <name>Loop</name>
        <synopsis>Performs its inner operators k times.</synopsis>
        <help>Performs its inner operators for the defined number of times.
            The input of this operator will be the input of the first operator in
            the first iteration. The output of each nested operator is the input
            for the following one, the output of the last inner operator will be
            the input for the first child in the next iteration. The output of
            the last operator in the last iteration will be the output of this
            operator.</help>
    <key>loop</key>
    <tags>
         <tag>Iterate</tag>
         <tag>Iteration</tag>
    </tags>
  </operator>
    <operator>
        <name>Drop Uncertain Predictions</name>
        <synopsis>Sets all predictions to 'unknown' (missing value) if the
            corresponding confidence is smaller than the specified value.
        </synopsis>
        <help>This operator sets all predictions which do not have a higher
            confidence than the specified one to &amp;quot;unknown&amp;quot;
            (missing value). This operator is a quite simple version of the
            CostBasedThresholdLearner which might be useful in simple binominal
            classification settings (although it does also work for polynominal
            classifications).</help>
    <key>drop_uncertain_predictions</key>
  </operator>
    <operator>
        <name>ConditionedFeatureGeneration</name>
        <synopsis>The conditioned feature generation operator allows to
            generate features with values dependent on conditions.</synopsis>
        <help>Generates a new attribute and sets the attributes values
            according to the fulfilling of the specified conditions. Sets the
            attribute value as the one which corresponds to the first matching
            condition.</help>
    <key>conditionedfeaturegeneration</key>
  </operator>
    <operator>
        <name>Filter Stopwords (Dictionary)</name>
        <synopsis>Filters terms based on a list of expressions provided in an
            external file.</synopsis>
        <help>Filters terms specified in an external file. The file must
            contain one term per line.</help>
    <key>filter_stopwords_dictionary</key>
  </operator>
    <operator>
        <name>Transpose</name>
        <synopsis>Transposes the input example set similar to the matrix
            operator transpose.</synopsis>
        <help>&lt;p&gt;This operator transposes an example set, i.e. the
            columns with become the new rows and the old rows will become the
            columns. Hence, this operator works very similar to the well know
            transpose operation for matrices.&lt;/p&gt; &lt;p&gt;If an Id
            attribute is part of the given example set, the ids will become the
            names of the new attributes. The names of the old attributes will be
            transformed into the id values of a new special Id attribute. Since
            no other &amp;quot;special&amp;quot; examples or data rows exist, all
            other new attributes will be regular after the transformation. You
            can use the &lt;i&gt;ChangeAttributeType&lt;/i&gt; operator in order
            to change one of these into a special type afterwards.&lt;/p&gt;
            &lt;p&gt;If all old attribute have the same value type, all new
            attributes will have this value type. Otherwise, the new value types
            will all be &amp;quot;nominal&amp;quot; if at least one nominal
            attribute was part of the given example set and
            &amp;quot;real&amp;quot; if the types contained mixed
            numbers.&lt;/p&gt; &lt;p&gt;This operator produces a copy of the data
            in the main memory and it therefore not suggested to use it on very
            large data sets.&lt;/p&gt;</help>
    </operator>
    <operator>
        <name>Set Parameters</name>
        <synopsis>Applies a set of parameters. Operator names may be remapped.
        </synopsis>
        <help>Sets a set of parameters. These parameters can either be
            generated by a &lt;i&gt;ParameterOptimizationOperator&lt;/i&gt; or
            read by a &lt;i&gt;ParameterSetLoader&lt;/i&gt;. This operator is
            useful, e.g. in the following scenario. If one wants to find the best
            parameters for a certain learning scheme, one usually is also
            interested in the model generated with this parameters. While the
            first is easily possible using a
            &lt;i&gt;ParameterOptimizationOperator&lt;/i&gt;, the latter is not
            possible because the &lt;i&gt;ParameterOptimizationOperator&lt;/i&gt;
            does not return the IOObjects produced within, but only a parameter
            set. This is, because the parameter optimization operator knows
            nothing about models, but only about the performance vectors produced
            within. Producing performance vectors does not necessarily require a
            model. &lt;br/&gt; To solve this problem, one can use a
            &lt;code&gt;ParameterSetter&lt;/code&gt;. Usually, a process with a
            &lt;code&gt;ParameterSetter&lt;/code&gt; contains at least two
            operators of the same type, typically a learner. One learner may be
            an inner operator of the
            &lt;i&gt;ParameterOptimizationOperator&lt;/i&gt; and may be named
            &amp;quot;Learner&amp;quot;, whereas a second learner of the same
            type named &amp;quot;OptimalLearner&amp;quot; follows the parameter
            optimization and should use the optimal parameter set found by the
            optimization. In order to make the
            &lt;code&gt;ParameterSetter&lt;/code&gt; set the optimal parameters
            of the right operator, one must specify its name. Therefore, the
            parameter list &lt;var&gt;name_map&lt;/var&gt; was introduced. Each
            parameter in this list maps the name of an operator that was used
            during optimization (in our case this is &amp;quot;Learner&amp;quot;)
            to an operator that should now use these parameters (in our case this
            is &amp;quot;OptimalLearner&amp;quot;).</help>
    <key>set_parameters</key>
  </operator>
    <operator>
        <name>Append</name>
        <synopsis>Build a merged example set from two or more compatible
            example sets by adding all examples into a combined set.</synopsis>
        <help>&lt;p&gt;This operator merges two or more given example sets by
            adding all examples in one example table containing all data rows.
            Please note that the new example table is built in memory and this
            operator might therefore not be applicable for merging huge data set
            tables from a database. In that case other preprocessing tools should
            be used which aggregates, joins, and merges tables into one table
            which is then used by RapidMiner.&lt;/p&gt; &lt;p&gt;All input
            example sets must provide the same attribute signature. That means
            that all examples sets must have the same number of (special)
            attributes and attribute names. If this is true this operator simply
            merges all example sets by adding all examples of all table into a
            new set which is then returned.&lt;/p&gt;</help>
    <key>append</key>
    <tags>
         <tag>Add</tag>
         <tag>Merge</tag>
         <tag>Combine</tag>
         <tag>Concatenate</tag>
         <tag>Concatenation</tag>
    </tags>
  </operator>
        <operator>
        <name>Filter Stopwords (English)</name>
        <synopsis>Standard stopwords list for English texts.</synopsis>
        <help>An operator removing every token occurring on a standard English
            stopword list.</help>
    <key>filter_stopwords_english</key>
  </operator>
    <operator>
        <name>Linear Discriminant Analysis</name>
        <synopsis>A linear discriminant function for binominal labels and
            numerical attributes.</synopsis>
        <help>&lt;p&gt;This operator performs a linear discriminant analysis
            (LDA). This method tries to find the linear combination of features
            which best separate two or more classes of examples. The resulting
            combination is then used as a linear classifier. LDA is closely
            related to ANOVA (analysis of variance) and regression analysis,
            which also attempt to express one dependent variable as a linear
            combination of other features or measurements. In the other two
            methods however, the dependent variable is a numerical quantity,
            while for LDA it is a categorical variable (i.e. the class
            label).&lt;/p&gt; &lt;p&gt;LDA is also closely related to principal
            component analysis (PCA) and factor analysis in that both look for
            linear combinations of variables which best explain the data. LDA
            explicitly attempts to model the difference between the classes of
            data. PCA on the other hand does not take into account any difference
            in class.&lt;/p&gt;</help>
    <key>linear_discriminant_analysis</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Combinations</tag>
         <tag>Components</tag>
    </tags>
    <shortName>LDA</shortName>
  </operator>
        <operator>
        <name>Sample (Stratified)</name>
        <synopsis>Creates a stratified sample from an example set by drawing a
            fraction.</synopsis>
        <help>Stratified sampling operator. This operator performs a random
            sampling of a given fraction. In contrast to the simple sampling
            operator, this operator performs a stratified sampling for data sets
            with nominal label attributes, i.e. the class distributions remains
            (almost) the same after sampling. Hence, this operator cannot be
            applied on data sets without a label or with a numerical label. In
            these cases a simple sampling without stratification is performed.
        </help>
    <key>sample_stratified</key>
    <tags>
         <tag>Subsets</tag>
         <tag>Random</tag>
         <tag>Ratio</tag>
         <tag>Stratified</tag>
         <tag>Stratification</tag>
         <tag>Bootstrap</tag>
         <tag>Population</tag>
         <tag>Downsample</tag>
    </tags>
  </operator>
    <operator>
        <name>Sample (Bootstrapping)</name>
        <synopsis>Creates a bootstrapped sample by sampling with replacement.
        </synopsis>
        <help>This operator constructs a bootstrapped sample from the given
            example set. That means that a sampling with replacement will be
            performed. The usual sample size is the number of original examples.
            This operator also offers the possibility to create the inverse
            example set, i.e. an example set containing all examples which are
            not part of the bootstrapped example set. This inverse example set
            might be used for a bootstrapped validation (together with an
            &lt;i&gt;IteratingPerformanceAverage&lt;/i&gt; operator.</help>
    <key>sample_bootstrapping</key>
  </operator>
    <operator>
        <name>CSVReader</name>
        <synopsis>This operator can read csv files.</synopsis>
        <help>&lt;p&gt;This operator can read csv files. All values must be
            separated by &amp;quot;,&amp;quot; or by &amp;quot;;&amp;quot;,
            followed by an arbitrary amount of white space. This means that also
            only the separator characters are allowed. The first line is used for
            the attribute names as default. Empty values and the question mark
            will be read as missing values. You can quote the values (including
            the column separators) with a double quote (&amp;quot;). You can
            escape the quoting character with a backslash, i.e.
            \&amp;quot;.&lt;/p&gt; &lt;p&gt;For other file formats or column
            separators you can use in almost all cases the operator
            &lt;i&gt;SimpleSource&lt;/i&gt; or, if this is not sufficient,
            the operator &lt;i&gt;ExampleSource&lt;/i&gt;.&lt;/p&gt;</help>
    </operator>
    <operator>
        <name>k-Means (Kernel)</name>
        <synopsis>Clustering with kernel k-means</synopsis>
        <help>This operator is an implementation of kernel k means. Kernel K
            Means uses kernels to estimate distance between objects and clusters.
            Because of the nature of kernels it is necessary to sum over all
            elements of a cluster to calculate one distance. So this algorithm is
            quadratic in number of examples and returns NO CentroidClusterModel,
            as its older brother KMeans does. This operator will create a cluster
            attribute if not present yet.</help>
    <key>k_means_kernel</key>
    <shortName>Clustering</shortName>
  </operator>
    <operator>
        <name>Remove Document Parts</name>
        <synopsis>Removes parts of the text matching a given regular
            expression.</synopsis>
        <help>This operator allows to remove parts of each token of a sequence
            matching a given regular expression. Since this probably will work
            best when the tokens are long enough, this operator is especially
            useful before the actual tokenization is applied during word vector
            creation. This operator could for example be used to remove tags from
            a html fragment, which cannot be parsed normally. A regular
            expression like &lt;[^&gt;]*&gt; would match all tags.</help>
    <key>remove_document_parts</key>
  </operator>
    <operator>
        <name>Split Document</name>
        <synopsis>Splits a Input TextObject into segments using regular
            expressions specifiying start and end of segments.</synopsis>
        <help>This operator segments a text based on a starting and ending
            regular expression.</help>
    <key>split_document</key>
  </operator>
    <operator>
        <name>Numeric2Polynominal</name>
        <synopsis>Maps all numeric values simply to the corresponding nominal
            values. Please use one of the discretization operators if you need
            more sophisticated nominalization methods.</synopsis>
        <help>Converts all numerical attributes to nominal ones. Each
            numerical value is simply used as nominal value of the new attribute.
            If the value is missing, the new value will be missing. Please note
            that this operator might drastically increase memory usage if many
            different numerical values are used. Please use the available
            discretization operators then.</help>
    </operator>
    <operator>
        <name>Nominal to Text</name>
        <synopsis>Replaces all nominal attributes by corresponding string
            attributes.</synopsis>
        <help>Converts all nominal attributes to string attributes. Each
            nominal value is simply used as string value of the new attribute. If
            the value is missing, the new value will be missing.</help>
    <key>nominal_to_text</key>
    <tags>
         <tag>Categorical</tag>
         <tag>Ordinal</tag>
         <tag>Strings</tag>
         <tag>Clob</tag>
         <tag>Types</tag>
    </tags>
  </operator>
        <operator>
        <name>Filter Examples</name>
        <synopsis>This operator only allows examples which fulfill a specified
            condition.</synopsis>
        <help>&lt;p&gt;This operator takes an &lt;i&gt;ExampleSet&lt;/i&gt; as
            input and returns a new &lt;i&gt;ExampleSet&lt;/i&gt; including only
            the &lt;i&gt;Example&lt;/i&gt;s that fulfill a condition.&lt;/p&gt;
            &lt;p&gt;By specifying an implementation of
            &lt;i&gt;Condition&lt;/i&gt; and a parameter string, arbitrary
            filters can be applied. Users can implement their own conditions by
            writing a subclass of the above class and implementing a two argument
            constructor taking an &lt;i&gt;ExampleSet&lt;/i&gt; and a parameter
            string. This parameter string is specified by the parameter
            &lt;code&gt;parameter_string&lt;/code&gt;. Instead of using one of
            the predefined conditions users can define their own implementation
            with the fully qualified class name.&lt;/p&gt; &lt;p&gt;For
            &amp;quot;attribute_value_condition&amp;quot; the parameter string
            must have the form &lt;code&gt;attribute op value&lt;/code&gt;, where
            attribute is a name of an attribute, value is a value the attribute
            can take and op is one of the binary logical operators similar to the
            ones known from Java, e.g. greater than or equals. Please note your
            can define a logical OR of several conditions with || and a logical
            AND of two conditions with two ampers and - or simply by applying
            several ExampleFilter operators in a row. Please note also that for
            nominal attributes you can define a regular expression for value of
            the possible equal and not equal checks.&lt;/p&gt; &lt;p&gt;For
            &amp;quot;unknown_attributes&amp;quot; the parameter string must be
            empty. This filter removes all examples containing attributes that
            have missing or illegal values. For &amp;quot;unknown_label&amp;quot;
            the parameter string must also be empty. This filter removes all
            examples with an unknown label value.&lt;/p&gt;</help>
    <key>filter_examples</key>
    <tags>
         <tag>Select</tag>
         <tag>Keep</tag>
         <tag>Remove</tag>
         <tag>Drop</tag>
         <tag>Delete</tag>
         <tag>Rows</tag>
         <tag>Cases</tag>
         <tag>Instances</tag>
         <tag>Lines</tag>
         <tag>Observations</tag>
         <tag>Filter Missing</tag>
    </tags>
  </operator>
    <operator>
        <name>Linear Regression</name>
        <synopsis>Linear regression.</synopsis>
        <help>&lt;p&gt;This operator calculates a linear regression model. It
            uses the Akaike criterion for model selection.&lt;/p&gt;</help>
    <key>linear_regression</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Regression</tag>
         <tag>Model</tag>
         <tag>Least squares</tag>
         <tag>Ordinary</tag>
         <tag>Ridge</tag>
         <tag>Ols</tag>
         <tag>Glm</tag>
         <tag>Generalized</tag>
    </tags>
  </operator>
    <operator>
        <name>Seemingly Unrelated Regression</name>
        <shortName>SUR</shortName>
        <synopsis>This will perform regressions on different datasets with different labels and will take into account the 
        correlation of residuals.</synopsis>
        <help>
        &lt;p&gt;The regression might be performed on different attribute sets, but all delivered ExampleSets must have the same number of examples. A main set must be 
        delivered containing the union of all attributes in all subsets. This must be connected to the first input port of this operator.
        On all other ports subsets might be attached.&lt;/p&gt; 
        &lt;p&gt;To compute the residuals a linear regression is performed on each single set using the parameter settings of
        this operator. The covariance of the residuals is used for improve the quality of predictions that are influenced by effects not captured by the attributes.&lt;/p&gt;
        </help>
    <key>seemingly_unrelated_regression</key>
  </operator>
    <operator>
        <name>Generate Gaussians</name>
        <synopsis>Creates a gaussian function based on a given attribute and a
            specified mean and standard deviation sigma.</synopsis>
        <help>Creates a gaussian function based on a given attribute and a
            specified mean and standard deviation sigma.</help>
    <key>generate_gaussians</key>
  </operator>
        <operator>
        <name>Visualize Model by SOM</name>
        <synopsis>Generates a SOM plot (transforming arbitrary number of
            dimensions to two) of the given data set and colorizes the landscape
            with the predictions of the given model.</synopsis>
        <help>This class provides an operator for the visualization of
            arbitrary models with help of the dimensionality reduction via a SOM
            of both the data set and the given model.</help>
    <key>visualize_model_by_som</key>
  </operator>
    <operator>
        <name>Apply Threshold</name>
        <synopsis>Applies a threshold on soft classified data.</synopsis>
        <help>This operator applies the given threshold to an example set and
            maps a soft prediction to crisp values. If the confidence for the
            second class (usually positive for RapidMiner) is greater than the
            given threshold the prediction is set to this class.</help>
    <key>apply_threshold</key>
    <tags>
         <tag>Platt</tag>
         <tag>Scoring</tag>
         <tag>Scores</tag>
         <tag>Confidences</tag>
         <tag>Thresholds</tag>
    </tags>
  </operator>
    <operator>
        <name>Split Data</name>
        <synopsis>Partitions an example set into subsets according to the
            specified relative sizes.</synopsis>
        <help>Divides a data set into the defined partitions and deliver the
            subsets.</help>
    <key>split_data</key>
    <tags>
         <tag>Divide</tag>
         <tag>Separate</tag>
         <tag>Part</tag>
         <tag>Training</tag>
         <tag>Testing</tag>
         <tag>Samples</tag>
         <tag>Subsets</tag>
         <tag>Partitions</tag>
         <tag>Sampling</tag>
    </tags>
  </operator>
    <operator>
        <name>Unset Macro</name>
        <synopsis>This operator can be used to remove one or more previously defined macros from
        the process context.</synopsis>
        <help>&lt;p&gt;This operator offers two parameters for entering one or a number of macro names that
        might have been defined. If they have been defined during exection of the process or in the 
        process context, they will be removed and are not accessible after this. Any reference to these macros
        will only return their name. If they haven't, nothing will happen.&lt;/p&gt;
        &lt;p&gt;This might be especially useful in scenarios, where one wants to branch by testing on existence of a macro.&lt;/p&gt;</help>
        <key>unset_macro</key>
    </operator>
    <operator>
        <name>Set Macros</name>
        <synopsis>This operator can be used to define arbitrary macros which
            can be used by %{my_macro} in parameter values.</synopsis>
        <help>&lt;p&gt;(Re-)Define macros for the current process. Macros will
            be replaced in the value strings of parameters by the macro values
            defined in the parameter list of this operator. &lt;/p&gt;
            &lt;p&gt;In the parameter list of this operator, you have to define
            the macro name (without the enclosing brackets) and the macro value.
            The defined macro can then be used in all succeeding operators as
            parameter value for string type parameters. A macro must then be
            enclosed by &amp;quot;MACRO_START&amp;quot; and
            &amp;quot;MACRO_END&amp;quot;.&lt;/p&gt; &lt;p&gt;There are several
            predefined macros:&lt;/p&gt; &lt;ul&gt;
            &lt;li&gt;MACRO_STARTprocess_nameMACRO_END: will be replaced by the
            name of the process (without path and extension)&lt;/li&gt;
            &lt;li&gt;MACRO_STARTprocess_fileMACRO_END: will be replaced by the
            file name of the process (with extension)&lt;/li&gt;
            &lt;li&gt;MACRO_STARTprocess_pathMACRO_END: will be replaced by the
            complete absolute path of the process file&lt;/li&gt; &lt;/ul&gt;
            &lt;p&gt;In addition to those the user might define arbitrary other
            macros which will be replaced by arbitrary string during the process
            run. Please note also that several other short macros exist, e.g.
            MACRO_STARTaMACRO_END for the number of times the current operator
            was applied. Please refer to the section about macros in the
            RapidMiner tutorial. Please note also that other operators like the
            &lt;i&gt;FeatureIterator&lt;/i&gt; also add specific
            macros.&lt;/p&gt;</help>
    <key>set_macros</key>
  </operator>
    <operator>
        <name>Vote</name>
        <synopsis>Uses a majority vote (for classification) or the average
            (for regression) on top of the predictions of the other inner
            learners.</synopsis>
        <help>This class uses n+1 inner learners and generates n different
            models by using the last n learners. The predictions of these n
            models are taken to create n new features for the example set, which
            is finally used to serve as an input of the first inner learner.
        </help>
    <key>vote</key>
  </operator>
    <operator>
        <name>Remap Binominals</name>
        <synopsis>Corrects the internal value mapping of binominal attributes
            according to the specified negative and positive values.</synopsis>
        <help>Correct internal mapping of binominal attributes according to
            the specified positive and negative values. If the internal mapping
            differs from the specifications, the mapping is switched. If the
            mapping contains other values than the specified ones, the mapping is
            not corrected and the attribute is simply skipped.</help>
    <key>remap_binominals</key>
    <tags>
         <tag>Mapping</tag>
         <tag>Positive</tag>
         <tag>Negative</tag>
    </tags>
  </operator>
    <operator>
        <name>Generate Extract</name>
        <synopsis>Extracts values from structured and unstructured sources
            using XPath expressions, regular expressions or simple string
            matching.</synopsis>
        <help>This operator allows to extract additional attributes from
            structured or unstructured text using regular expression, XPath or
            simple string matching. The input texts are take from the specified
            source attribute, which must be nominal. The query type can be either
            XPath for XML documents, or regular expressions for less structured
            texts. The XPath expression specifies directly which part of the XML
            document is retrieved and this is used as value for the new
            attribute. If you use regular expressions, the first matching group
            is used as value. For example an expression like "Name:\s*(.*)\n" on
            a text "Name: Paul" followed by a line break will yield "Paul" as new
            value in the attribute. String matching is a fast and easy to use
            replacement for regular expressions, but less powerful. You just have
            to specify a start and an end string. Everything between the two
            strings is extracted. For example if the start string would be
            "Name:" and the end string a linebreak, then the result of the above
            text would be " Paul". The response might contain a separated list of
            results, for example a XML tag like this:
            &lt;languages&gt;en,de,fr,sp&lt;/languages&gt; Then it is possible to
            enter the a query yielding "en,de,fr,sp" multiple times, using
            different attribute names. If the separator parameter contains the
            ",", then the first attribute will be filled with "en" the second
            with "de" and so on. This might be used to get only the first
            enumerated value, too. But be careful with this feature, since other
            results might be splitted, too, even if you don't enter a query
            twice. You might avoid this, by inserting a second operator, where
            you don't specify a separator.</help>
    <key>generate_extract</key>
  </operator>
    <operator>
        <name>Recall</name>
        <synopsis>
    This operators retrieves an objects stored by the &lt;i&gt;Remember&lt;/i&gt; operator.
  </synopsis>
        <help>
    This operator can be used to retrieve the object which was previously 
    stored under the specified name by a &lt;i&gt;Remember&lt;/i&gt; operator. The 
    combination of those two operator can be used to build complex processes 
    where an input object is used in completely different parts or loops of 
    processes.
  </help>
    <key>recall</key>
    <tags>
         <tag>Remember</tag>
         <tag>Cache</tag>
         <tag>Load</tag>
    </tags>
  </operator>
  <operator>
        <name>Recall from App</name>
        <synopsis>
    Retrieves objects from an app.
  </synopsis>
        <help>
    This operator can be used to retrieve the object which was previously 
    stored under the specified name by a &lt;i&gt;Publish to App&lt;/i&gt; operator. The 
    combination of those two operator can be used to build complex apps where an input object is used multiple times in 
    completely different parts of the app.
  </help>
    <key>recall_from_app</key>
  </operator>
    <operator>
        <name>Extract Log Value</name>
        <synopsis>Reads the specified value from the input example set and
            provides the value for logging purposes.</synopsis>
        <help>This operator can be used to log a specific value of a given
            example set into the provided log value
            &amp;quot;data_value&amp;quot; which can then be logged by the
            operator &lt;i&gt;ProcessLogOperator&lt;/i&gt;.</help>
    <key>extract_log_value</key>
  </operator>
    <operator>
        <name>Data to Similarity Data</name>
        <synopsis>Calculates distance or similarity between all given examples.</synopsis>
        <help>This operator creates a new data set from the given one based on
            the specified similarity. The created data set is merely a view so
            that no memory problems should occur.</help>
    <key>data_to_similarity_data</key>
  </operator>
    <operator>
        <name>Cross Distances</name>
        <synopsis>Calculates the distance between each example of a request to each example of a reference set.</synopsis>
        <help>
      &lt;p&gt;
      This operator creates an exampleSet containing the distances between each example of the request
  exampleSet and the k nearest of the reference exampleSet.
  &lt;/p&gt;
  &lt;p&gt;
  This operator needs ID attributes in both example sets in order to work. If not present, new ones are created.
  &lt;/p&gt;
      
      </help>
    <key>cross_distances</key>
  </operator>
    <operator>
        <name>Detect Outlier (Densities)</name>
        <synopsis>Identifies outliers in the given ExampleSet based on the
            data density.</synopsis>
        <help>&lt;p&gt;This operator is a DB outlier detection algorithm which
            calculates the DB(p,D)-outliers for an ExampleSet passed to the
            operator. DB(p,D)-outliers are Distance based outliers according to
            Knorr and Ng. A DB(p,D)-outlier is an object to which at least a
            proportion of p of all objects are farer away than distance D. It
            implements a global homogenous outlier search.&lt;/p&gt;
            &lt;p&gt;Currently, the operator supports cosine, sine or squared
            distances in addition to the usual euclidian distance which can be
            specified by the corresponding parameter. The operator takes two
            other real-valued parameters p and D. Depending on these parameters,
            search objects will be created from the examples in the ExampleSet
            passed to the operator. These search objects will be added to a
            search space which will perform the outlier search according to the
            DB(p,D) scheme.&lt;/p&gt; &lt;p&gt;The Outlier status (boolean in its
            nature) is written to a new special attribute
            &amp;quot;Outlier&amp;quot; and is passed on with the example
            set.&lt;/p&gt;</help>
    <key>detect_outlier_densities</key>
  </operator>
<operator>
  <name>Detect Outlier (COF)</name>
  <synopsis>Identifies outliers in the given ExampleSet based on Class
            outlier factors.</synopsis>
  <help>
         &lt;p&gt;This operator performs a Class Outlier Factor (COF) search. COF outliers or Class Outliers method 
        search for observations (objects) those that arouse suspicions, taking into account the class labels
        according to the definition of Class Outlier by Hewaihi and Saad in "A comparative Study of Outlier Mining and Class Outlier Mining", CS Letters, Vol 1, No 1 (2009)", 
        and "Class Outliers Mining: Distance-Based Approach", International Journal of Intelligent Systems and Technologies, Vol. 2, No. 1, pp 55-68, 2007".
        &lt;/p&gt;
        &lt;p&gt;
        It detects rare / exceptional / suspicious cases with respect to a group of similar cases.
        &lt;/p&gt;
        &lt;p&gt;
        The main concept of ECODB (Enhanced Class Outlier - Distance Based) algorithm is to rank each 
        instance in the dataset D given the parameters N (top N class outliers), and K (the number of nearest neighbors. 
        The Rank finds out the rank of each instance using the formula (COF = PCL(T,K) - norm(deviation(T)) + norm(kDist(T))). where PCL(T,K) 
        is the Probability of the class label of the instance T with respect to the class labels of its K Nearest Neighbors. and 
        norm(Deviation(T)) and norm(KDist(T)) are the normalized value of Deviation(T) and KDist(T) respectively and their value fall into the 
        range [0 - 1]. Deviation(T) is how much the instance T deviates from instances of the same class, and computed by summing the distances 
        between the instance T and every instance belong to the same class of the instance. KDist(T) is the summation of distance between the 
        instance T and its K nearest neighbors.
        &lt;/p&gt;   
        &lt;p&gt;
        The ECODB algorithm maintains a list of only the instances of the top N class outliers. The less is the value of COF of an instance, 
        the higher is the priority of the instance to be a class outlier.
        &lt;/p&gt;   
        &lt;p&gt;The operator supports mixed euclidian distance. The Operator takes an example set
        and passes it on with an boolean top-n COF outlier status in a new boolean-valued
        special outlier attribute indicating true (outlier) and false (no outlier),
        and another special attribute "COF Factor" which measures the degree of being Class Outlier for an object.
        &lt;/p&gt;
        </help>
      <key>detect_outlier_cof</key>
      </operator>
    <operator>
        <name>T-Test</name>
        <synopsis>Performs a t-test to determine the probability for the null
            hypothesis 'the actual means are the same'.</synopsis>
        <help>Determines if the null hypothesis (all actual mean values are
            the same) holds for the input performance vectors. This operator uses
            a simple (pairwise) t-test to determine the probability that the null
            hypothesis is wrong. Since a t-test can only be applied on two
            performance vectors this test will be applied to all possible pairs.
            The result is a significance matrix. However, pairwise t-test may
            introduce a larger type I error. It is recommended to apply an
            additional ANOVA test to determine if the null hypothesis is wrong at
            all.</help>
    <key>t_test</key>
  </operator>
    <operator>
        <name>Generate ID</name>
        <synopsis>Adds a new id attribute to the example set, each example is
            tagged with an incremented number.</synopsis>
        <help>This operator adds an ID attribute to the given example set.
            Each example is tagged with an incremental integer number. If the
            example set already contains an id attribute, the old attribute will
            be removed before the new one is added.</help>
    <key>generate_id</key>
    <tags>
         <tag>Create</tag>
         <tag>Id</tag>
         <tag>Identifier</tag>
         <tag>Index</tag>
         <tag>Row no</tag>
         <tag>Row number</tag>
         <tag>unique</tag>
         <tag>key</tag>
         <tag>primary</tag>
    </tags>
  </operator>
    <operator>
        <name>Compare ROCs</name>
        <synopsis>Generates a ROC chart for the models created by each of the
            inner learners and plot all charts in the same plotter.</synopsis>
        <help>This operator uses its inner operators (each of those must
            produce a model) and calculates the ROC curve for each of them. All
            ROC curves together are plotted in the same plotter. The comparison
            is based on the average values of a k-fold cross validation.
            Alternatively, this operator can use an internal split into a test
            and a training set from the given data set. Please note that a former
            predicted label of the given example set will be removed during the
            application of this operator.</help>
    <key>compare_rocs</key>
    <tags>
         <tag>Roc</tag>
         <tag>Curves</tag>
         <tag>Comparing</tag>
         <tag>Comparisons</tag>
         <tag>Validations</tag>
         <tag>Evaluations</tag>
         <tag>Performances</tag>
         <tag>Sensitivity</tag>
    </tags>
  </operator>
    <operator>
        <name>Split</name>
        <synopsis>Creates new attributes from a nominal attribute by dividing
            the nominal values into parts according to a split criterion.
        </synopsis>
        <help>&lt;p&gt;This operator creates new attributes from a nominal
            attribute by dividing the nominal values into parts according to a
            split criterion (regular expression). This operator provides two
            different modes, depending on the setting of the parameter
            &amp;quot;splitting_mode&amp;quot;.&lt;/p&gt; &lt;h3&gt;Ordered
            Splits&lt;/h3&gt; &lt;p&gt;In the first split mode, called
            ordered_split, the resulting attributes get the name of the original
            attribute together with a number indicating the order. For example,
            if the original data contained the values&lt;br/&gt;&lt;br/&gt;
            attribute-name &lt;br/&gt; -------------- &lt;br/&gt; value1
            &lt;br/&gt; value2, value3 &lt;br/&gt; value3 &lt;br/&gt; &lt;br/&gt;
            and should be divided by the separating commas, the resulting
            attributes would be attribute-name1, attribute-name2, attribute-name3
            with the tuples (value1, ?, ?), (value2, value3, ?), and (value3, ?,
            ?), respectively. This mode is useful if the original values
            indicated some order like, for example, a preference. &lt;/p&gt;
            &lt;h3&gt;Unordered Splits&lt;/h3&gt; &lt;p&gt;In the second split
            mode, called unordered_split, the resulting attributes get the name
            of the original attribute together with the value for each of the
            occurring values. For example, if the original data contained the
            values&lt;br/&gt;&lt;br/&gt; attribute-name &lt;br/&gt;
            -------------- &lt;br/&gt; value1 &lt;br/&gt; value2, value3
            &lt;br/&gt; value3 &lt;br/&gt; &lt;br/&gt; and again should be
            divided by the separating commas, the resulting attributes would be
            attribute-name-value1, attribute-name-value2, and
            attribute-name-value3 with the tuples (true, false, false), (false,
            true, true), and (false, false, true), respectively. This mode is
            useful if the order is not important but the goal is a basket like
            data set containing all occurring values. &lt;/p&gt;</help>
    <key>split</key>
  </operator>
    <operator>
        <name>NeuralNetSimple</name>
        <synopsis>Learns a neural net from the input data.</synopsis>
        <help>&lt;p&gt;This operator learns a model by means of a feed-forward
            neural network. The learning is done via backpropagation. The user
            can define the structure of the neural network in two different ways
            according to the setting of the parameter
            define_different_hidden_layers. If different hidden layers are
            defined, the parameter hidden_layer_sizes must be set to a comma
            separated list of the sizes of all hidden layers, e.g. 3,7,5. If no
            different hidden layers are defined, the parameters for the default
            hidden layers are used. A size value of -1 or 0 indicates that the
            layer size should be calculated from the number of attributes of the
            input example set. In this case, the layer size will be set to
            (number of attributes + number of classes) / 2 + 1. All layers have a
            sigmoid activation function.&lt;/p&gt; &lt;p&gt;If the user does not
            specify any hidden layers, a default hidden layer with size (number
            of attributes + number of classes) / 2 + 1 will be created and added
            to the net.&lt;/p&gt;</help>
    <key>neuralnetsimple</key>
  </operator>

    <operator>
        <name>Performance (Binominal Classification)</name>
        <synopsis>This operator delivers as output a list of performance
            values according to a list of selected performance criteria (for
            binominal classification tasks).</synopsis>
        <help>&lt;p&gt;This performance evaluator operator should be used for
            classification tasks, i.e. in cases where the label attribute has a
            binominal value type. Other polynominal classification tasks, i.e.
            tasks with more than two classes can be handled by the
            &lt;i&gt;PolynominalClassificationPerformanceEvaluator&lt;/i&gt;
            operator. This operator expects a test &lt;i&gt;ExampleSet&lt;/i&gt;
            as input, whose elements have both true and predicted labels, and
            delivers as output a list of performance values according to a list
            of performance criteria that it calculates. If an input performance
            vector was already given, this is used for keeping the performance
            values.&lt;/p&gt; &lt;p&gt;All of the performance criteria can be
            switched on using boolean parameters. Their values can be queried by
            a ProcessLogOperator using the same names. The main criterion is used
            for comparisons and need to be specified only for processes where
            performance vectors are compared, e.g. feature selection or other
            meta optimization process setups. If no other main criterion was
            selected, the first criterion in the resulting performance vector
            will be assumed to be the main criterion.&lt;/p&gt; &lt;p&gt;The
            resulting performance vectors are usually compared with a standard
            performance comparator which only compares the fitness values of the
            main criterion. Other implementations than this simple comparator can
            be specified using the parameter
            &lt;var&gt;comparator_class&lt;/var&gt;. This may for instance be
            useful if you want to compare performance vectors according to the
            weighted sum of the individual criteria. In order to implement your
            own comparator, simply subclass
            &lt;i&gt;PerformanceComparator&lt;/i&gt;. Please note that for true
            multi-objective optimization usually another selection scheme is used
            instead of simply replacing the performance comparator.&lt;/p&gt;
        </help>
    <key>performance_binominal_classification</key>
    <tags>
         <tag>Roc</tag>
         <tag>Accuracy</tag>
         <tag>Errors</tag>
         <tag>Precision</tag>
         <tag>Recall</tag>
         <tag>Auc</tag>
         <tag>Lift</tag>
         <tag>True</tag>
         <tag>False</tag>
         <tag>Positives</tag>
         <tag>Negatives</tag>
         <tag>Metrics</tag>
    </tags>
    <shortName>Performance</shortName>
  </operator>
    
    <operator>
        <name>Log to Weights</name>
        <synopsis>This operator creates new attribute weights from a attribute
            names column logged with the ProcessLog operator.</synopsis>
        <help>&lt;p&gt;This operator creates attribute weights from an
            attribute column in the statistics created by the ProcessLog
            operator. In order to use this operator, one has first to add a
            ProcessLog operator inside of a feature selection operator and log
            the currently selected attribute names. This operator will then
            calculate attribute weights from such a statistics table by checking
            how often each attribute was selected and use the relative
            frequencies as attribute weights.&lt;/p&gt; &lt;p&gt;If the
            performance is also logged with the ProcessLog operator, these values
            can also be used to calculate the relative frequencies. In this case,
            the sorting type can be set to only use the top k or the bottom k
            attribute sets for frequency calculation according to the specified
            performance column.&lt;/p&gt;</help>
    <key>log_to_weights</key>
  </operator>
    <operator>
        <name>Pivot</name>
        <synopsis>Transforms an example set by grouping multiple examples of
            single units to single examples.</synopsis>
        <help>&lt;p&gt;Transforms an example set by grouping multiple examples
            of single groups into single examples. The parameter
            &lt;em&gt;group_attribute&lt;/em&gt; specifies an attribute which
            identifies examples belonging to the groups. The parameter
            &lt;em&gt;index_attribute&lt;/em&gt; specifies an attribute whose
            values are used to identify the examples inside the groups. The
            values of this attributes are used to name the group attributes which
            are created during the pivoting. Typically the values of such an
            attribute capture subgroups or dates. If the source example set
            contains example weights, these weights may be aggregated in each
            group to maintain the weightings among groups.&lt;/p&gt;</help>
    <key>pivot</key>
    <tags>
         <tag>Rotate</tag>
         <tag>Rotations</tag>
         <tag>Pivoting</tag>
         <tag>Group</tag>
         <tag>Grouping</tag>
         <tag>Group by</tag>
         <tag>Aggregate</tag>
         <tag>Cross-table</tag>
         <tag>Crosstable</tag>
         <tag>Long</tag>
         <tag>Wide</tag>
         <tag>Transpose</tag>
         <tag>Sum</tag>
         <tag>Count</tag>
    </tags>
  </operator>
    <operator>
        <name>MetaCost</name>
        <synopsis>Builds a classification model using cost values from a given
            matrix.</synopsis>
        <help>This operator uses a given cost matrix to compute label
            predictions according to classification costs. The method used by
            this operator is similar to MetaCost as described by Pedro Domingos.
        </help>
    <key>metacost</key>
  </operator>
    <operator>
        <name>Single Rule Induction</name>
        <synopsis>Returns a best conjunctive rule with respect to the WRAcc
            metric for boolean prediction problems and polynominal attributes.
        </synopsis>
        <help>This operator returns the best rule regarding WRAcc using
            exhaustive search. Features like the incorporation of other metrics
            and the search for more than a single rule are prepared. The search
            strategy is BFS, with save pruning whenever applicable. This operator
            can easily be extended to support other search strategies.</help>
    <key>single_rule_induction</key>
  </operator>

    <operator>
        <name>Numerical to Real</name>
        <synopsis>Replaces all numerical attributes (especially integer
            attributes) by corresponding real valued attributes.</synopsis>
        <help>Converts all numerical attributes (especially integer
            attributes) to real valued attributes. Each integer value is simply
            used as real value of the new attribute. If the value is missing, the
            new value will be missing.</help>
    <key>numerical_to_real</key>
    <tags>
         <tag>Continous</tag>
         <tag>Numbers</tag>
         <tag>Integers</tag>
         <tag>Reals</tag>
         <tag>Types</tag>
         <tag>Quantitative</tag>
    </tags>
  </operator>
    <operator>
        <name>Guess Types</name>
        <synopsis>(Re-)guesses all value types and changes them accordingly.
        </synopsis>
        <help>This operator can be used to (re-)guess the value types of all
            attributes. This might be useful after some preprocessing
            transformations and &amp;quot;purifying&amp;quot; some of the
            columns, especially if columns which were nominal before can be
            handled as numerical columns. With this operator, the value types of
            all attributes do not have to be transformed manually with operators
            like &lt;i&gt;Parse Numbers&lt;/i&gt;.</help>
    <key>guess_types</key>
    <tags>
         <tag>Automatic</tag>
         <tag>Continous</tag>
         <tag>Categorical</tag>
         <tag>Nominal</tag>
         <tag>Ordinary</tag>
         <tag>Discrete</tag>
         <tag>Binominal</tag>
         <tag>Polynominal</tag>
         <tag>Numerical</tag>
         <tag>Real</tag>
         <tag>Integer</tag>
         <tag>Binary</tag>
         <tag>Dual</tag>
         <tag>Types</tag>
    </tags>
  </operator>
    <operator>
        <name>Loop Attributes (Deprecated)</name>
        <synopsis>Iterates over the given features and applies the inner
            operators for each feature where the inner operators can access the
            current feature name by a macro.</synopsis>
        <help>&lt;p&gt;This operator takes an input data set and applies its
            inner operators as often as the number of features of the input data
            is. Inner operators can access the current feature name by a macro,
            whose name can be specified via the parameter
            &lt;code&gt;iteration_macro&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The user
            can specify with a parameter if this loop should iterate over all
            features or only over features with a specific value type, i.e. only
            over numerical or over nominal features. A regular expression can
            also be specified which is used as a filter, i.e. the inner operators
            are only applied for feature names matching the filter
            expression.&lt;/p&gt;</help>
    <key>loop_attributes</key>
    <tags>
         <tag>Iterate</tag>
         <tag>Iteration</tag>
    </tags>
  </operator>
    <operator>
        <name>Loop Examples</name>
        <synopsis>Iterates over the given example set and applies the inner
            operators for each example where the inner operators can access the
            current example name by a macro.</synopsis>
        <help>
    &lt;p&gt;
      This operator takes an input data set and applies its inner operators as 
      often as the number of examples of the input data is. Inner operators 
      can access the current example number (begins with 0) by a macro, whose 
      name can be specified via the parameter &lt;code&gt;iteration_macro&lt;/code&gt;.
    &lt;/p&gt;
    &lt;p&gt;
      As input example will be used either the cloned &lt;i&gt;ExampleSet&lt;/i&gt; 
      delivered on the &lt;i&gt;OutputPort&lt;/i&gt; of this operator or if not connected 
      the initial unmodified &lt;i&gt;ExampleSet&lt;/i&gt;.
    &lt;/p&gt;
  </help>
    <key>loop_examples</key>
  </operator>
    <operator>
        <name>Clear Log</name>
        <synopsis>Clears a table generated by a ProcessLog operator.
        </synopsis>
        <help>This operator can be used to clear a data table generated by a
            &lt;i&gt;ProcessLogOperator&lt;/i&gt;.</help>
    <key>clear_log</key>
  </operator>
        <operator>
        <name>Generalized Hebbian Algorithm</name>
        <synopsis>Generalized Hebbian Algorithm (GHA). Performs an iterative
            principal components analysis.</synopsis>
        <help>Generalized Hebbian Algorithm (GHA) is an iterative method to
            compute principal components. From a computational point of view, it
            can be advantageous to solve the eigenvalue problem by iterative
            methods which do not need to compute the covariance matrix directly.
            This is useful when the ExampleSet contains many Attributes
            (hundreds, thousands). The operator outputs a
            &lt;code&gt;GHAModel&lt;/code&gt;. With the
            &lt;code&gt;ModelApplier&lt;/code&gt; you can transform the features.
        </help>
    <key>generalized_hebbian_algorithm</key>
    <shortName>GHA</shortName>
  </operator>
    <operator>
        <name>Update Model</name>
        <synopsis>Updates a model according to an example set. Please note
            that this operator can only be used for updatable models, otherwise
            an error will be shown.</synopsis>
        <help>This operator updates a &lt;i&gt;Model&lt;/i&gt; with an
            &lt;i&gt;ExampleSet&lt;/i&gt;. Please note that the model must return
            true for &lt;i&gt;Model#isUpdatable()&lt;/i&gt; in order to be usable
            with this operator.</help>
    <key>update_model</key>
  </operator>
    <operator>
        <name>Weight by Relief</name>
        <synopsis>Relief measures the relevance of features by sampling
            examples and comparing the value of the current feature for the
            nearest example of the same and of a different class.</synopsis>
        <help>&lt;p&gt;Relief measures the relevance of features by sampling
            examples and comparing the value of the current feature for the
            nearest example of the same and of a different class. This version
            also works for multiple classes and regression data sets. The
            resulting weights are normalized into the interval between 0 and
            1.&lt;/p&gt;</help>
    <key>weight_by_relief</key>
  </operator>
    <operator>
        <name>Normalize</name>
        <synopsis>Normalizes the attribute values for a specified range.
        </synopsis>
        <help>This operator performs a normalization. This can be done between
            a user defined minimum and maximum value or by a z-transformation,
            i.e. on mean 0 and variance 1. or by a proportional transformation as
            proportion of the total sum of the respective attribute.</help>
    <key>normalize</key>
    <tags>
         <tag>Normalization</tag>
         <tag>Standardize</tag>
         <tag>Standardization</tag>
         <tag>Z-transformation</tag>
         <tag>Z-Transformation</tag>
         <tag>Scaling</tag>
         <tag>Features</tag>
         <tag>Attributes</tag>
         <tag>Variables</tag>
         <tag>Columns</tag>
    </tags>
  </operator>
    <operator>
        <name>De-Normalize</name>
        <synopsis>This operator will return an Normalization Model that inverts a given
        Normalization Model. 
        </synopsis>
        <help>With help of this operator any Normalization method can be reversed.</help>
    <key>denormalize</key>
  </operator>

    <operator>
        <name>Rename</name>
        <synopsis>This operator can be used to rename an attribute.</synopsis>
        <help>&lt;p&gt; This operator can be used to rename an attribute of
         the input &lt;i&gt;ExampleSet&lt;/i&gt;. Please keep in mind, that attribute names have to be unique.&lt;br /&gt;
         Although renamed, an attribute keeps it's role. For example if you rename an attribute &quot;label&quot; of the role &lt;b&gt;label&lt;/b&gt; to &quot;color&quot;, the resulting attribute &quot;color&quot; will still have the role &lt;b&gt;label&lt;/b&gt;. For changing a role, see &lt;a href="rm://opdoc/set_role"&gt;Set Role&lt;/a&gt;.
         &lt;/p&gt;</help>
    <key>rename</key>
    <tags>
         <tag>Names</tag>
         <tag>Columns</tag>
         <tag>Attributes</tag>
         <tag>Features</tag>
         <tag>Variables</tag>
    </tags>
  </operator>
    <operator>
        <name>LinearCombination</name>
        <synopsis>This operator created a new example set containing only one
            feature: the linear combination of all input attributes.</synopsis>
        <help>&lt;p&gt;This operator applies a linear combination for each
            vector of the input ExampleSet, i.e. it creates a new feature
            containing the sum of all values of each row.&lt;/p&gt;</help>
    <key>linearcombination</key>
  </operator>
    <operator>
        <name>Generate Weight (Stratification)</name>
        <synopsis>Distributes weight over examples, so that per label weights
            sum up equally.</synopsis>
        <help>This operator distributes example weights so that all example
            weights of labels sum up equally.</help>
    <key>generate_weight_stratification</key>
  </operator>
     <operator>
        <name>Generate Weight (LPR)</name>
        <synopsis>This operator uses the distance between an example's label value and the result of a local polynomial regression to determine 
        the weight of this example.</synopsis>
        <help>&lt;p&gt;
            This operator performs a weighting of the examples and hence the resulting exampleset will contain a new weight
  attribute. If a weight attribute was already included in the exampleSet, its values will be used as initial values
  for this algorithm. If not, each example is assigned a weight of 1.&lt;/p&gt;
  
  &lt;p&gt;For calculating the weights, this operator will perform a local polynomial regression for each example. For more
  information about local polynomial regression, take a look at the operator description of the local polynomial
  regression operator Local Polynomial Regression.&lt;/p&gt;
  
  &lt;p&gt;After the predicted result has been calculated, the residuals are computed and rescaled using their median.&lt;/p&gt;
  
  &lt;p&gt;This result will be transformed by a smooth function, which cuts of values greater than a threshold. This means, that
  examples without prediction error will gain a weight of 1, while examples with an error greater than the threshold
  will be down weighted to 0.&lt;/p&gt;
  
  &lt;p&gt;This procedure is iterated as often as specified by the user and will result in weights, which will penalize outliers
  heavily. This is especially useful for algorithms using the least squares optimization such as Linear Regression, Polynomial Regression
  or Local Polynomial Regression, since least square is very sensitive to outliers.&lt;/p&gt; 
            </help>
    <key>generate_weight_lpr</key>
  </operator>
  
  
    <operator>
        <name>Read Excel</name>
        <synopsis>This operator reads an example set from Excel spreadsheet
            files.</synopsis>
        <help>&lt;p&gt;This operator can be used to load data from Microsoft
            Excel spreadsheets. This operator is able to reads data from Excel
            95, 97, 2000, XP, and 2003. The user has to define which of the
            spreadsheets in the workbook should be used as data table. The table
            must have a format so that each line is an example and each column
            represents an attribute. Please note that the first line might be
            used for attribute names which can be indicated by a
            parameter.&lt;/p&gt; &lt;p&gt;The data table can be placed anywhere
            on the sheet and is allowed to contain arbitrary formatting
            instructions, empty rows, and empty columns. Missing data values are
            indicated by empty cells or by cells containing only
            &amp;quot;?&amp;quot;.&lt;/p&gt;</help>
    <key>read_excel</key>
    <tags>
         <tag>Load</tag>
         <tag>Import</tag>
         <tag>Read</tag>
         <tag>Data</tag>
         <tag>Files</tag>
         <tag>Xls</tag>
         <tag>Xlsx</tag>
         <tag>Microsoft</tag>
         <tag>Spreadsheets</tag>
         <tag>Datasets</tag>
    </tags>
  </operator>
  
  <operator>
        <name>Read Excel with Format</name>
        <synopsis>This operator reads an example set from Excel spreadsheet
            file together with the format information about the cells.</synopsis>
        <help>&lt;p&gt;This operator can be used to load data from Microsoft
            Excel spreadsheets. This operator is able to reads data from Excel
            95, 97, 2000, XP, and 2003. The user has to define which of the
            spreadsheets in the workbook should be used as data table. The table
            must have a format so that each line is an example and each column
            represents an attribute. In contrast to the normal operator for
            reading Excel files, this operator creates a second attribute for 
            each original data column storing the formatting information from 
            the Excel sheet. Missing data values are
            indicated by empty cells or by cells containing only
            &amp;quot;?&amp;quot;.&lt;/p&gt;</help>
    <key>read_excel_format</key>
  </operator>
  
    <operator>
        <name>Local Polynomial Regression</name>
        <synopsis>Generates word vectors from a single text.</synopsis>
        <key>local_polynomial_regression</key>
        <help>&lt;p&gt;
        This operator provides functionality to perform a local regression. That means, that if the label value for a
  point in the data space is requested, the local neighborhood of this point is searched. For this search the distance
  measure specified in the distance measure parameter is used. After the neighborhood has been determined, its
  datapoints are used for fitting a polynomial of the specified degree using the weighted least squares optimization.
  The value of this polynom at the requested point in data space is then returned as result. During the fitting of the
  polynom, the neighborhoods data points are weighted by their distance to the requested point. Here again the distance
  function specified in the parameters is used. The weight is calculated from the distance using the kernel smoother,
  specified in the parameters. The resulting weight is then included into the least squares optimization. If the
  training example set contains a weight attribute, the distance based weight is multiplied by the example's weight. If
  the parameter use_robust_estimation is checked, a Generate Weight (LPR) is performed with the same
  parameters as the following Local Polynomial Regression. For different settings the operator
  Generate Weight (LPR) might be used as a preprocessing step instead of checking the parameter. The effect
  is, that outlier will be downweighted so that the least squares fitting will not be affected by them anymore.&lt;/p&gt;
  
  
  &lt;p&gt;Since it is a local method, the computational need for training is minimal: In fact, each example is only stored in a
  way which provides a fast neighborhood search during application time. Since all calculations are performed during
  application time, it is slower than for example SVM, LinearRegression or NaiveBayes. In fact it really much depends
  on the number of training examples and the number of attributes. If a higher degree than 1 is used, the calculations
  take much longer, because implicitly the polynomial expansion must be calculated.&lt;/p&gt;
      </help>
    </operator>
    <operator>
        <name>Optimize Selection</name>
        <synopsis>This operator realizes feature selection by forward
            selection and backward elimination, respectively.</synopsis>
        <help>&lt;p&gt; This operator realizes the two deterministic greedy
            feature selection algorithms forward selection and backward
            elimination. However, we added some enhancements to the standard
            algorithms which are described below: &lt;/p&gt; &lt;h4&gt;Forward
            Selection&lt;/h4&gt; &lt;ol&gt; &lt;li&gt;Create an initial
            population with &lt;i&gt;n&lt;/i&gt; individuals where
            &lt;i&gt;n&lt;/i&gt; is the input example set's number of attributes.
            Each individual will use exactly one of the features.&lt;/li&gt;
            &lt;li&gt;Evaluate the attribute sets and select only the best
            &lt;i&gt;k&lt;/i&gt;.&lt;/li&gt; &lt;li&gt;For each of the
            &lt;i&gt;k&lt;/i&gt; attribute sets do: If there are
            &lt;i&gt;j&lt;/i&gt; unused attributes, make &lt;i&gt;j&lt;/i&gt;
            copies of the attribute set and add exactly one of the previously
            unused attributes to the attribute set.&lt;/li&gt; &lt;li&gt;As long
            as the performance improved in the last &lt;i&gt;p&lt;/i&gt;
            iterations go to 2&lt;/li&gt; &lt;/ol&gt; &lt;h4&gt;Backward
            Elimination&lt;/h4&gt; &lt;ol&gt; &lt;li&gt;Start with an attribute
            set which uses all features.&lt;/li&gt; &lt;li&gt;Evaluate all
            attribute sets and select the best &lt;i&gt;k&lt;/i&gt;.&lt;/li&gt;
            &lt;li&gt;For each of the &lt;i&gt;k&lt;/i&gt; attribute sets do: If
            there are &lt;i&gt;j&lt;/i&gt; attributes used, make
            &lt;i&gt;j&lt;/i&gt; copies of the attribute set and remove exactly
            one of the previously used attributes from the attribute
            set.&lt;/li&gt; &lt;li&gt;As long as the performance improved in the
            last &lt;i&gt;p&lt;/i&gt; iterations go to 2&lt;/li&gt; &lt;/ol&gt;
            &lt;p&gt; The parameter &lt;i&gt;k&lt;/i&gt; can be specified by the
            parameter &lt;code&gt;keep_best&lt;/code&gt;, the parameter
            &lt;i&gt;p&lt;/i&gt; can be specified by the parameter
            &lt;code&gt;generations_without_improval&lt;/code&gt;. These
            parameters have default values 1 which means that the standard
            selection algorithms are used. Using other values increase the
            runtime but might help to avoid local extrema in the search for the
            global optimum. &lt;/p&gt; &lt;p&gt; Another unusual parameter is
            &lt;code&gt;maximum_number_of_generations&lt;/code&gt;. This
            parameter bounds the number of iterations to this maximum of feature
            selections / deselections. In combination with
            &lt;code&gt;generations_without_improval&lt;/code&gt; this allows
            several different selection schemes (which are described for forward
            selection, backward elimination works analogous): &lt;ul&gt;
            &lt;li&gt;&lt;code&gt;maximum_number_of_generations&lt;/code&gt; =
            &lt;i&gt;m&lt;/i&gt; and
            &lt;code&gt;generations_without_improval&lt;/code&gt; =
            &lt;i&gt;p&lt;/i&gt;: Selects maximal &lt;i&gt;m&lt;/i&gt; features.
            The selection stops if not performance improvement was measured in
            the last &lt;i&gt;p&lt;/i&gt; generations.&lt;/li&gt;
            &lt;li&gt;&lt;code&gt;maximum_number_of_generations&lt;/code&gt; =
            &lt;i&gt;-1&lt;/i&gt; and
            &lt;code&gt;generations_without_improval&lt;/code&gt; =
            &lt;i&gt;p&lt;/i&gt;: Tries to selects new features until no
            performance improvement was measured in the last &lt;i&gt;p&lt;/i&gt;
            generations.&lt;/li&gt;
            &lt;li&gt;&lt;code&gt;maximum_number_of_generations&lt;/code&gt; =
            &lt;i&gt;m&lt;/i&gt; and
            &lt;code&gt;generations_without_improval&lt;/code&gt; =
            &lt;i&gt;-1&lt;/i&gt;: Selects maximal &lt;i&gt;m&lt;/i&gt; features.
            The selection stops is not stopped until all combinations with
            maximal &lt;i&gt;m&lt;/i&gt; were tried. However, the result might
            contain less features than these.&lt;/li&gt;
            &lt;li&gt;&lt;code&gt;maximum_number_of_generations&lt;/code&gt; =
            &lt;i&gt;-1&lt;/i&gt; and
            &lt;code&gt;generations_without_improval&lt;/code&gt; =
            &lt;i&gt;-1&lt;/i&gt;: Test all combinations of attributes (brute
            force, this might take a very long time and should only be applied to
            small attribute sets).&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt;</help>
    <key>optimize_selection</key>
  </operator>
   <operator>
      <key>optimize_selection_forward</key>
      <tags>
         <tag>Iterate</tag>
         <tag>Iteration</tag>
         <tag>Weighting</tag>
         <tag>Importance</tag>
         <tag>Influence</tag>
         <tag>Significance</tag>
         <tag>Factors</tag>
         <tag>Relevance</tag>
      </tags>
      <name>Forward Selection</name>
      <synopsis>A highly efficient implementation of the forward selection scheme.</synopsis>
      <help>&lt;p&gt;
         This operator starts with an empty selection of attributes and, in each round, it adds each unused attribute of the
         given set of examples. For each added attribute, the performance is estimated using inner operators, e.g. a
         cross-validation. Only the attribute giving the highest increase of performance is added to the selection. Then a new
         round is started with the modified selection. This implementation will avoid any additional memory consumption beside
         the memory used originally for storing the data and the memory which might be needed for applying the inner
         operators.&lt;/p&gt;
         &lt;p&gt;&lt;br /&gt;A parameter specifies when the iteration will be aborted. There are three different behaviors possible:
         &lt;ul&gt;
         &lt;li&gt;&lt;b&gt;without increase&lt;/b&gt;&lt;br /&gt; runs as long as there is any increase in performance&lt;/li&gt;
         &lt;li&gt;&lt;b&gt;without increase of at least&lt;/b&gt;&lt;br /&gt; runs as long as the increase is at least as high as specified, either relative or absolute.&lt;/li&gt;
         &lt;li&gt;&lt;b&gt;without significant increase&lt;/b&gt;&lt;br /&gt; stops as soon as the increase isn't significant to the specified level.&lt;/li&gt;
         &lt;/ul&gt;
         &lt;/p&gt;
         &lt;p&gt;The parameter speculative_rounds defines how many rounds will be performed in a row, after a first time the stopping
         criterion was fulfilled. If the performance increases again during the speculative rounds, the selection will be continued.
         Otherwise all additionally selected attributes will be removed, as if no speculative rounds would have been executed.
         This might help to avoid getting stuck in local optima. A following backward elimination operator might remove unneeded
         attributes again.&lt;/p&gt;
         &lt;p&gt;The operator provides a value for logging the performance in each round using a Log.&lt;/p&gt;
        </help>
   </operator>
   <operator>
      <key>optimize_selection_backward</key>
      <name>Backward Elimination</name>
      <synopsis>A highly efficient implementation of the backward elimination scheme.</synopsis>
      <help>
        This operator starts with the full set of attributes and, in each round, it removes each remaining attribute of the
         given set of examples. For each removed attribute, the performance is estimated using inner operators, e.g. a
         cross-validation. Only the attribute giving the least decrease of performance is finally removed from the selection. Then a new
         round is started with the modified selection. This implementation will avoid any additional memory consumption beside
         the memory used originally for storing the data and the memory which might be needed for applying the inner
         operators.&lt;br&gt;
         A parameter specifies when the iteration will be aborted. There are three different behaviors possible:
         &lt;ul&gt;
         &lt;li&gt;&lt;b&gt;with decrease&lt;/b&gt;&lt;br /&gt;runs as long as there is any increase in performance&lt;/li&gt;
         &lt;li&gt;&lt;b&gt;with decrease of more than&lt;/b&gt;&lt;br /&gt;runs as long as the decrease is less than the specified threshold, either relative or absolute.&lt;/li&gt;
         &lt;li&gt;&lt;b&gt;with significant decrease&lt;/b&gt;&lt;br /&gt; stops as soon as the decrease is significant to the specified level.&lt;/li&gt;
         &lt;/ul&gt;
         &lt;p&gt;
         The parameter speculative_rounds defines how many rounds will be performed in a row, after a first time the stopping
         criterion was fulfilled. If the performance increases again during the speculative rounds, the elimination will be continued.
         Otherwise all additionally eliminated attributes will be restored, as if no speculative rounds would have been executed.
         This might help to avoid getting stuck in local optima.
         &lt;/p&gt;
         &lt;p&gt;The operator provides a value for logging the performance in each round using a Log.&lt;/p&gt;
        </help>
   </operator>    
  
    <operator>
        <name>X-Prediction</name>
        <synopsis>Predicts the examples in a cross-validation-like fashion.
        </synopsis>
        <help>Operator chain that splits an &lt;i&gt;ExampleSet&lt;/i&gt; into
            a training and test sets similar to XValidation, but returns the test
            set predictions instead of a performance vector. The inner two
            operators must be a learner returning a &lt;i&gt;Model&lt;/i&gt; and
            an operator or operator chain that can apply this model (usually a
            model applier)</help>
    <key>x_prediction</key>
    <tags>
         <tag>Cross-validations</tag>
         <tag>Crossvalidations</tag>
         <tag>Predictions</tag>
         <tag>Validations</tag>
         <tag>Scoring</tag>
         <tag>Scores</tag>
         <tag>Training</tag>
         <tag>Testing</tag>
    </tags>
  </operator>
    <operator>
        <name>Expectation Maximization Clustering</name>
        <synopsis>Clustering with EM</synopsis>
        <help>This operator represents an implementation of the EM-algorithm.
        </help>
    <key>expectation_maximization_clustering</key>
    <shortName>Clustering</shortName>
  </operator>
    <operator>
        <name>Remove Correlated Attributes</name>
        <synopsis>Removes correlated features.</synopsis>
        <help>&lt;p&gt;Removes (un-) correlated features due to the selected
            filter relation. The procedure is quadratic in number of attributes.
            In order to get more stable results, the original, random, and
            reverse order of attributes is available.&lt;/p&gt; &lt;p&gt;Please
            note that this operator might fail in some cases when the attributes
            should be filtered out, e.g. it might not be able to remove for
            example all negative correlated features. The reason for this
            behaviour seems to be that for the complete m x m - matrix of
            correlations (for m attributes) the correlations will not be
            recalculated and hence not checked if one of the attributes of the
            current pair was already marked for removal. That means for three
            attributes a1, a2, and a3 that it might be that a2 was already ruled
            out by the negative correlation with a1 and is now not able to rule
            out a3 any longer.&lt;/p&gt; &lt;p&gt; The used correlation function
            is the Pearson correlation. &lt;/p&gt;</help>
    <key>remove_correlated_attributes</key>
    <tags>
         <tag>Filter</tag>
         <tag>Keep</tag>
         <tag>Remove</tag>
         <tag>Drop</tag>
         <tag>Delete</tag>
         <tag>Correlations</tag>
    </tags>
  </operator>
    <operator>
        <name>ValueSubgroupIterator</name>
        <synopsis>Iterates over the values of the specified attributes and
            applies the inner operators on the subgroups which exhibit the
            current attribute value.</synopsis>
        <help>&lt;p&gt; In each iteration step, this meta operator applies its
            inner operators to a subset of the input example set. The subsets
            represent subgroups which are defined by the values of the specified
            attributes. If an attribute is specified which has 'male' or 'female'
            as possible values the first iteration subset will consist of all
            males, the second of all females, respectively. Please note that no
            attribute value combinations are supported and hence only subgroups
            defined by exactly one attribute are considered at a time.&lt;/p&gt;
            &lt;p&gt;A subset is build (and an inner operator application is
            executed) for each possible attribute value of the specified
            attributes if &lt;code&gt;all&lt;/code&gt; is selected for the
            &lt;code&gt;values&lt;/code&gt; parameter. If &lt;code&gt;above
            p&lt;/code&gt; is selected, a subset is only build for that values
            which exhibit an occurrence ratio of at least p. This may be helpful,
            if only large subgroups should be considered.&lt;/p&gt; &lt;p&gt; The
            parameter &lt;code&gt;filter_attribute&lt;/code&gt; specifies, if the
            subgroup defining attribute should be filtered from the
            subsets.&lt;/p&gt; &lt;p&gt; The parameter
            &lt;code&gt;apply_on_complete_set&lt;/code&gt; specifies, if the
            inner operators should be applied on the completed example set in
            addition to the subset iterations.&lt;/p&gt; &lt;p&gt;The current
            value of the loop can be accessed with the specified macro
            name.&lt;/p&gt;</help>
    <key>valuesubgroupiterator</key>
  </operator>
    <operator>
        <name>Bootstrapping Validation</name>
        <synopsis>This operator encapsulates an iterated bootstrapping
            sampling with performance evaluation on the remaining examples.
        </synopsis>
        <help>&lt;p&gt;This validation operator performs several bootstrapped
            samplings (sampling with replacement) on the input set and trains a
            model on these samples. The remaining samples, i.e. those which were
            not sampled, build a test set on which the model is evaluated. This
            process is repeated for the specified number of iterations after
            which the average performance is calculated.&lt;/p&gt; &lt;p&gt;The
            basic setup is the same as for the usual cross validation operator.
            The first inner operator must provide a model and the second a
            performance vector. Please note that this operator does not regard
            example weights, i.e. weights specified in a weight column.&lt;/p&gt;
            &lt;p&gt;This validation operator provides several values which can
            be logged by means of a &lt;i&gt;ProcessLogOperator&lt;/i&gt;. All
            performance estimation operators of RapidMiner provide access to the
            average values calculated during the estimation. Since the operator
            cannot ensure the names of the delivered criteria, the ProcessLog
            operator can access the values via the generic value names:&lt;/p&gt;
            &lt;ul&gt; &lt;li&gt;performance: the value for the main criterion
            calculated by this validation operator&lt;/li&gt;
            &lt;li&gt;performance1: the value of the first criterion of the
            performance vector calculated&lt;/li&gt; &lt;li&gt;performance2: the
            value of the second criterion of the performance vector
            calculated&lt;/li&gt; &lt;li&gt;performance3: the value of the third
            criterion of the performance vector calculated&lt;/li&gt;
            &lt;li&gt;for the main criterion, also the variance and the standard
            deviation can be accessed where applicable.&lt;/li&gt; &lt;/ul&gt;
        </help>
    <key>bootstrapping_validation</key>
    <shortName>Validation</shortName>
  </operator>
    <operator>
        <name>AttributeBasedVote</name>
        <synopsis>Actually no learning scheme since the prediction is the
            average of all attribute values.</synopsis>
        <help>AttributeBasedVotingLearner is very lazy. Actually it does not
            learn at all but creates an
            &lt;i&gt;AttributeBasedVotingModel&lt;/i&gt;. This model simply
            calculates the average of the attributes as prediction (for
            regression) or the mode of all attribute values (for classification).
            AttributeBasedVotingLearner is especially useful if it is used on an
            example set created by a meta learning scheme, e.g. by
            &lt;i&gt;Vote&lt;/i&gt;.</help>
    <key>attributebasedvote</key>
  </operator>
        <operator>
        <name>Write as Text</name>
        <synopsis>This operator can be used at each point in an operator chain
            and and writes current results to the console or to a file.
        </synopsis>
        <help>This operator can be used at each point in an operator chain. It
            returns all input it receives without any modification. Every input
            object which implements the &lt;i&gt;ResultObject&lt;/i&gt; interface
            (which is the case for almost all objects generated by the core
            RapidMiner operators) will write its results to the file specified by
            the parameter &lt;var&gt;result_file&lt;/var&gt;. If the definition
            of this parameter is ommited then the global result file parameter
            with the same name of the ProcessRootOperator (the root of the
            process) will be used. If this file is also not specified the results
            are simply written to the console (standard out).</help>
    <key>write_as_text</key>
  </operator>
    <operator>
        <name>Generate Empty Attribute</name>
        <synopsis>Adds a new attribute to the data set with the given name and
            type.</synopsis>
        <help>This operator creates a new attribute for the data set. The new
            attribute will have the specified name and value type (e.g. nominal
            or real). Please note that all values are missing right after
            creation and therefore operators like SetData must be used to change
            this.</help>
    <key>generate_empty_attribute</key>
  </operator>
    <operator>
        <name>Generate Function Set</name>
        <synopsis>The feature generation operator generates new features via
            applying a set of functions on all features.</synopsis>
        <help>This operator applies a set of functions on all features of the
            input example set. Applicable functions include +, -, *, /, norm,
            sin, cos, tan, atan, exp, log, min, max, floor, ceil, round, sqrt,
            abs, and pow. Features with two arguments will be applied on all
            pairs. Non commutative functions will also be applied on all
            permutations.</help>
    <key>generate_function_set</key>
  </operator>
    <operator>
        <name>Process Documents from Data</name>
        <synopsis>Generates word vectors from string attributes.</synopsis>
        <help/>
    <key>process_document_from_data</key>
  </operator>
    
    
    <operator>
        <name>Cut</name>
        <synopsis>Creates new attributes from nominal attributes which only
            contain substrings of the original attributes.</synopsis>
        <help>This operator creates new attributes from nominal attributes
            where the new attributes contain only substrings of the original
            values. Please note that the counting starts with 1 and that the
            first and the last character will be included in the resulting
            substring. For example, the value is &amp;quot;RapidMiner&amp;quot;
            and the first index is set to 6 and the last index is set to 9 the
            result will be &amp;quot;Mine&amp;quot;. If the last index is larger
            than the length of the word, the resulting substrings will end with
            the last character.</help>
    <key>cut</key>
  </operator>
    <operator>
        <name>Generate Concatenation</name>
        <synopsis>Merges two attributes into a single new attribute by
            concatenating the values.</synopsis>
        <help>This operator merges two attributes by simply concatenating the
            values and store those new values in a new attribute which will be
            nominal. If the resulting values are actually numerical, you could
            simply change the value type afterwards with the corresponding
            operators.</help>
    <key>generate_concatenation</key>
  </operator>
    <operator>
        <name>Write AML</name>
        <synopsis>Writes the values of all examples to a file.</synopsis>
        <help>Writes values of all examples in an
            &lt;i&gt;ExampleSet&lt;/i&gt; to a file. Both dense and sparse formats 
            can be generate. These formats can be read using the
            &lt;i&gt;Read AML&lt;/i&gt; and
            &lt;i&gt;Read Sparse&lt;/i&gt; operators, respectively. &lt;dl&gt;
            &lt;dt&gt;dense:&lt;/dt&gt; &lt;dd&gt; Each line of the generated
            data file is of the form&lt;br/&gt; &lt;center&gt; &lt;pre&gt;
            regular attributes &amp;lt;special attributes&amp;gt; &lt;/pre&gt;
            &lt;/center&gt; For example, each line could have the form
            &lt;center&gt; &lt;pre&gt; value1 value2 ... valueN
            &amp;lt;id&amp;gt; &amp;lt;label&amp;gt; &amp;lt;prediction&amp;gt;
            ... &amp;lt;confidences&amp;gt; &lt;/pre&gt; &lt;/center&gt; Values
            in parenthesis are optional and are only printed if they are
            available. The confidences are only given for nominal predictions.
            Other special attributes might be the example weight or the cluster
            number. &lt;/dd&gt; &lt;dt&gt;sparse:&lt;/dt&gt; &lt;dd&gt;Only non 0
            values are written to the file, prefixed by a column index. See the
            description of &lt;i&gt;SparseFormatExampleSource&lt;/i&gt; for
            details. &lt;/dd&gt; &lt;dt&gt;special:&lt;/dt&gt;          
            &lt;/dl&gt;</help>
    <key>write_aml</key>
  </operator>
    <operator>
        <name>Generate Absolutes</name>
        <synopsis>Replaces all numerical values by their absolute values.
        </synopsis>
        <help>This operator simply replaces all values by their absolute
            respective value.</help>
    <key>generate_absolutes</key>
  </operator>

    <operator>
        <name>Optimize Weights (Backward)</name>
        <synopsis>Assumes that features are independent and optimizes the
            weights of the attributes with a linear search.</synopsis>
        <help>Uses the backward selection idea for the weighting of features.
        </help>
    <key>optimize_weights_backward</key>
  </operator>

    
    <operator>
        <name>ID3Numerical</name>
        <synopsis>Learns an unpruned decision tree from nominal and numerical
            data.</synopsis>
        <help>This operator learns decision trees without pruning using both
            nominal and numerical attributes. Decision trees are powerful
            classification methods which often can also easily be understood.
            This decision tree learner works similar to Quinlan's ID3.</help>
    <key>id3numerical</key>
  </operator>
    <operator>
        <name>Merge</name>
        <synopsis>Merges two nominal values of a specified attribute.
        </synopsis>
        <help>Merges two nominal values of a given regular attribute. To
            process special attributes like labels, wrap this operator by an
            AttributeSubsetPreprocessing operator with the parameter
            process_special_attributes enabled.</help>
    <key>merge</key>
  </operator>
    <operator>
        <name>Weight by Uncertainty</name>
        <synopsis>This operator calculates the relevance of an attribute by
            measuring the symmetrical uncertainty with respect to the class.
        </synopsis>
        <help>&lt;p&gt; This operator calculates the relevance of an attribute
            by measuring the symmetrical uncertainty with respect to the class.
            The formulaization for this is: &lt;/p&gt; &lt;code&gt;relevance = 2
            * (P(Class) - P(Class | Attribute)) / P(Class) +
            P(Attribute)&lt;/code&gt;</help>
    <key>weight_by_uncertainty</key>
  </operator>
    
    <operator>
        <name>Text to Nominal</name>
        <synopsis>Replaces all string attributes by corresponding nominal
            attributes.</synopsis>
        <help>Converts all string attributes to nominal attributes. Each
            string value is simply used as nominal value of the new attribute. If
            the value is missing, the new value will be missing.</help>
    <key>text_to_nominal</key>
    <tags>
         <tag>Categorical</tag>
         <tag>Ordinal</tag>
         <tag>Strings</tag>
         <tag>Clob</tag>
         <tag>Types</tag>
    </tags>
  </operator>
    <operator>
        <name>Subgroup Discovery (Meta)</name>
        <synopsis>A Subgroup Discovery meta learning scheme</synopsis>
        <help>Subgroup discovery learner.</help>
    <key>subgroup_discovery_meta</key>
  </operator>
    <operator>
        <name>Loop and Average</name>
        <synopsis>Iterates the inner operators and builds the average of the
            results.</synopsis>
        <help>This operator chain performs the inner operators the given
            number of times. The inner operators must provide a
            PerformanceVector. These are averaged and returned as result.</help>
    <key>loop_and_average</key>
  </operator>
    <operator>
        <name>Ungroup Models</name>
        <synopsis>Ungroups a previously grouped model into the single models
            which might then be handled on their own.</synopsis>
        <help>&lt;p&gt;This operator ungroups a previously grouped model
            (&lt;i&gt;ModelGrouper&lt;/i&gt;) and delivers the grouped input
            models.&lt;/p&gt; &lt;p&gt;This operator replaces the automatic model
            grouping known from previous versions of RapidMiner. The explicit
            usage of this ungrouping operator gives the user more control about
            the ungrouping procedure. Single models can be grouped with the
            &lt;i&gt;ModelGrouper&lt;/i&gt; operator.&lt;/p&gt;</help>
    <key>ungroup_models</key>
  </operator>
    <operator>
        <name>Set Data</name>
        <synopsis>Sets the data of a specified example and attribute to the
            specified value.</synopsis>
        <help>This operator simply sets the value for the specified example
            and attribute to the given value.</help>
    <key>set_data</key>
  </operator>
  <operator>
        <name>Declare Missing Value</name>
        <synopsis>Declares a missing numeric or nominal value on a selected subset,
        	which will be converted to Double.NaN.</synopsis>
        <help>The given value will be replaced with Double.NaN throughout the specified subset,
        	so it will be treated as a missing value by subsequent operators.</help>
    <key>declare_missing_value</key>
    <tags>
         <tag>Nulls</tag>
         <tag>Empty</tag>
         <tag>Cleansing</tag>
         <tag>Quality</tag>
         <tag>Missings</tag>
         <tag>Handle</tag>
         <tag>Impute</tag>
         <tag>Na</tag>
         <tag>Nan</tag>
         <tag>Fill na</tag>
         <tag>Declare na</tag>
    </tags>
  </operator>
    <operator>
        <name>Stem (Snowball)</name>
        <synopsis>The Snowball stemmer for different languages.</synopsis>
        <help>The Snowball stemmer uses the snowball library for stemming in
            the selected language. Arbitrary languages are supported and can be
            chosen using the language Parameter.</help>
    <key>stem_snowball</key>
  </operator>

    <operator>
        <name>FeatureNameFilter</name>
        <synopsis>This operator switches off those features whose name matches
            the given one (regular expressions are also allowed).</synopsis>
        <help>This operator switches off all features whose name matches the
            one given in the parameter
            &lt;code&gt;skip_features_with_name&lt;/code&gt;. The name can be
            defined as a regular expression.</help>
    <key>featurenamefilter</key>
  </operator>
    <operator>
        <name>Covariance Matrix</name>
        <synopsis>Determines the covariance between all attributes.</synopsis>
        <help>This operator calculates the covariances between all attributes
            of the input example set and returns a covariance matrix object which
            can be visualized.</help>
    <key>covariance_matrix</key>
  </operator>
    <operator>
        <name>Item Distribution Performance</name>
        <synopsis>Delivers a performance of a cluster model based on the
            distribution of examples.</synopsis>
        <help>Evaluates flat cluster models on how well the examples are
            distributed over the clusters.</help>
    <key>item_distribution_performance</key>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>k-Means</name>
        <synopsis>Clustering with k-means</synopsis>
        <help>This operator represents an implementation of k-means. This
            operator will create a cluster attribute if not present yet.</help>
    <key>k_means</key>
    <tags>
         <tag>Unsupervised</tag>
         <tag>Clustering</tag>
         <tag>Segmentation</tag>
         <tag>Grouping</tag>
         <tag>Similarity</tag>
         <tag>Similarities</tag>
         <tag>Euclidean</tag>
         <tag>Distances</tag>
         <tag>Centroids</tag>
         <tag>K Means</tag>
         <tag>K means</tag>
         <tag>Kmeans</tag>
    </tags>
    <shortName>Clustering</shortName>
  </operator>
    <operator>
        <name>Generate Transfer Data</name>
        <synopsis>Generates data for testing purposes based on a transfers
            data set.</synopsis>
        <help>Generates a random example set for testing purposes. The data
            represents a team profit example set.</help>
    <key>generate_transfer_data</key>
  </operator>
    <operator>
        <name>Generate Transaction Data</name>
        <synopsis>Generates data for testing purposes based on a transaction 
            data set.</synopsis>
        <help>Generates a random example set for testing purposes. The data
            represents a transaction example set where customers behave like 
            being in clusters.</help>
    <key>generate_transaction_data</key>
  </operator>  
    <operator>
        <name>Join Paths</name>
        <synopsis>This operators delivers the first non-null input to its
            output.</synopsis>
        <help>This operator returns the first non-null input it receives.
        </help>
    <key>join_paths</key>
  </operator>
    <operator>
        <name>TextObjectWriter</name>
        <synopsis>Writes a textobject into a file.</synopsis>
        <help>This operator writes a given textObject into a file. It might be
            specified if an existing file with the same name should be
            overwritten.</help>
    </operator>
    <operator>
        <name>k-Medoids</name>
        <synopsis>Clustering with k-medoids</synopsis>
        <help>This operator represents an implementation of k-medoids. This
            operator will create a cluster attribute if not present yet.</help>
    <key>k_medoids</key>
    <tags>
         <tag>Unsupervised</tag>
         <tag>Clustering</tag>
         <tag>Segmentation</tag>
         <tag>Grouping</tag>
         <tag>Similarity</tag>
         <tag>Similarities</tag>
         <tag>Euclidean</tag>
         <tag>Distances</tag>
         <tag>Centroids</tag>
         <tag>K Medoids</tag>
         <tag>K medoids</tag>
         <tag>Kmedoids</tag>
    </tags>
    <shortName>Clustering</shortName>
  </operator>
    <operator>
        <name>Remove Duplicates</name>
        <synopsis>This operator removed duplicates from an example set by
            comparing all examples with each other on basis of the specified
            attributes.</synopsis>
        <help>
    This operator removed duplicates from an example set by comparing all 
    examples with each other on basis of the specified selection of 
    attributes. Hence two examples are equal if all values of all selected 
    attributes are equal.
  </help>
    <key>remove_duplicates</key>
    <tags>
         <tag>Deduplication</tag>
         <tag>Matches</tag>
         <tag>Matching</tag>
         <tag>Replicates</tag>
         <tag>Copies</tag>
         <tag>Cleansing</tag>
         <tag>Quality</tag>
         <tag>Distinct</tag>
         <tag>Equal</tag>
         <tag>Filter</tag>
    </tags>
  </operator>

    <operator>
        <name>Data to Weights</name>
        <synopsis>This operator simply creates new attribute weights of 1 for
            each input attribute.</synopsis>
        <help>This operator creates a new attribute weights IOObject from a
            given example set. The result is a vector of attribute weights
            containing the weight 1.0 for each of the input attributes.</help>
    <key>data_to_weights</key>
  </operator>

    <operator>
        <name>Loop and Deliver Best</name>
        <synopsis>Performs its inner operators k times and returns the best
            results.</synopsis>
        <help>This operator iterates several times through the inner operators
            and in each cycle evaluates a performance measure. The IOObjects that
            are produced as output of the inner operators in the best cycle are
            then returned. The target of this operator are methods that involve
            some non-deterministic elements such that the performance in each
            cycle may vary. An example is k-means with random initialization.
        </help>
    <key>loop_and_deliver_best</key>
  </operator>
    <operator>
        <name>Multiply</name>
        <synopsis>This operators multiplies its input object.</synopsis>
        <help>&lt;p&gt;This operator copies its input object to all connected output ports. As     more ports are connected, more copies are generated. Note that objects are     copied by reference, hence the underlying data of &lt;i&gt;ExampleSets&lt;/i&gt; is     never copied (unless using a &lt;i&gt;Materialize Data&lt;/i&gt; operator). Therefore,     copying objects is cheap. When copying &lt;i&gt;ExampleSets&lt;/i&gt; only references     to attributes are copied. When attributes are changed or added to one     example set, this change is invisible to the other copies. However, if &lt;i&gt;data&lt;/i&gt;     is modified in one thread of the process flow, it is also modified in the     other copies.&lt;/p&gt;</help>
    <key>multiply</key>
    <tags>
         <tag>Copy</tag>
         <tag>Branch</tag>
         <tag>Duplicate</tag>
    </tags>
  </operator>
    <operator>
        <name>Additive Regression</name>
        <synopsis>Additive regression operator allowing all learners (not
            restricted to Weka learners).</synopsis>
        <help>&lt;p&gt;This operator uses regression learner as a base
            learner. The learner starts with a default model (mean or mode) as a
            first prediction model. In each iteration it learns a new base model
            and applies it to the example set. Then, the residuals of the labels
            are calculated and the next base model is learned. The learned meta
            model predicts the label by adding all base model
            predictions.&lt;/p&gt;</help>
    <key>additive_regression</key>
  </operator>
    <operator>
        <name>Performance (User-Based)</name>
        <synopsis>This operator delivers as output a list of performance
            values according to a list of user defined performance criteria.
        </synopsis>
        <help>&lt;p&gt;This performance evaluator operator should be used for
            regression tasks, i.e. in cases where the label attribute has a
            numerical value type. The operator expects a test
            &lt;i&gt;ExampleSet&lt;/i&gt; as input, whose elements have both true
            and predicted labels, and delivers as output a list of performance
            values according to a list of performance criteria that it
            calculates. If an input performance vector was already given, this is
            used for keeping the performance values.&lt;/p&gt;
            &lt;p&gt;Additional user-defined implementations of
            &lt;i&gt;PerformanceCriterion&lt;/i&gt; can be specified by using the
            parameter list
            &lt;var&gt;additional_performance_criteria&lt;/var&gt;. Each
            key/value pair in this list must specify a fully qualified classname
            (as the key), and a string parameter (as value) that is passed to the
            constructor. Please make sure that the class files are in the
            classpath (this is the case if the implementations are supplied by a
            plugin) and that they implement a one-argument constructor taking a
            string parameter. It must also be ensured that these classes extend
            &lt;i&gt;MeasuredPerformance&lt;/i&gt; since the PerformanceEvaluator
            operator will only support these criteria. Please note that only the
            first three user defined criteria can be used as logging value with
            names &amp;quot;user1&amp;quot;, ... ,
            &amp;quot;user3&amp;quot;.&lt;/p&gt; &lt;p&gt;The resulting
            performance vectors are usually compared with a standard performance
            comparator which only compares the fitness values of the main
            criterion. Other implementations than this simple comparator can be
            specified using the parameter
            &lt;var&gt;comparator_class&lt;/var&gt;. This may for instance be
            useful if you want to compare performance vectors according to the
            weighted sum of the individual criteria. In order to implement your
            own comparator, simply subclass
            &lt;i&gt;PerformanceComparator&lt;/i&gt;. Please note that for true
            multi-objective optimization usually another selection scheme is used
            instead of simply replacing the performance comparator.&lt;/p&gt;
        </help>
    <key>performance_user_based</key>
    <shortName>Performance</shortName>
  </operator>

    <operator>
        <name>Write Excel</name>
        <synopsis>This operator writes an example set to Excel spreadsheet
            files.</synopsis>
        <help>&lt;p&gt;This operator can be used to write data into Microsoft
            Excel spreadsheets. This operator creates Excel files readable by
            Excel 95, 97, 2000, XP, 2003 and newer. Missing data values are
            indicated by empty cells.&lt;/p&gt;</help>
    <key>write_excel</key>
    <tags>
         <tag>Save</tag>
         <tag>Export</tag>
         <tag>Write</tag>
         <tag>Datasets</tag>
         <tag>Files</tag>
         <tag>Xls</tag>
         <tag>Xlsx</tag>
         <tag>Microsoft</tag>
         <tag>Spreadsheets</tag>
    </tags>
  </operator>
    <operator>
        <name>ROCChart</name>
        <synopsis>Generates a ROC chart for the given binominal model and
            input data set.</synopsis>
        <help>This operator creates a ROC chart for the given example set and
            model. The model will be applied on the example set and a ROC chart
            will be produced afterwards. If you are interested in finding an
            optimal threshold, the operator &lt;i&gt;ThresholdFinder&lt;/i&gt;
            should be used. If you are interested in the performance criterion
            Area-Under-Curve (AUC) the usual
            &lt;i&gt;PerformanceEvaluator&lt;/i&gt; can be used. This operator
            just presents a ROC plot for a given model and data set. Please note
            that a predicted label of the given example set will be removed
            during the application of this operator.</help>
    <key>rocchart</key>
  </operator>
    <operator>
        <name>Rule Induction</name>
        <synopsis>Learns a pruned set of rules with respect to the information
            gain.</synopsis>
        <help>&lt;p&gt;This operator works similar to the propositional rule
            learner named Repeated Incremental Pruning to Produce Error Reduction
            (RIPPER, Cohen 1995). Starting with the less prevalent classes, the
            algorithm iteratively grows and prunes rules until there are no
            positive examples left or the error rate is greater than
            50%.&lt;/p&gt; &lt;p&gt;In the growing phase, for each rule greedily
            conditions are added to the rule until the rule is perfect (i.e. 100%
            accurate). The procedure tries every possible value of each attribute
            and selects the condition with highest information gain.&lt;/p&gt;
            &lt;p&gt;In the prune phase, for each rule any final sequences of the
            antecedents is pruned with the pruning metric p/(p+n).&lt;/p&gt;
        </help>
    <key>rule_induction</key>
    <tags>
         <tag>Ripper</tag>
         <tag>Version space</tag>
         <tag>Cn2</tag>
         <tag>Part</tag>
         <tag>Ridor</tag>
         <tag>Oner</tag>
         <tag>Prism</tag>
         <tag>Jrip</tag>
    </tags>
  </operator>
    
    <operator>
        <name>Perceptron</name>
        <synopsis>Single Perceptron finding seperating hyperplane if one
            exists</synopsis>
        <help>The perceptron is a type of artificial neural network invented
            in 1957 by Frank Rosenblatt. It can be seen as the simplest kind of
            feedforward neural network: a linear classifier. Beside all
            biological analogies, the single layer perceptron is simply a linear
            classifier which is efficiently trained by a simple update rule: for
            all wrongly classified data points, the weight vector is either
            increased or decreased by the corresponding example values.</help>
    <key>perceptron</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Regression</tag>
         <tag>Deep</tag>
         <tag>Backpropagation</tag>
         <tag>Back-propagation</tag>
         <tag>Perceptron</tag>
         <tag>Network</tag>
         <tag>Artificial</tag>
         <tag>Anns</tag>
    </tags>
  </operator>
       <operator>
        <name>Performance (Ranking)</name>
        <synopsis>This operator delivers as a performance value representing costs
           for the cofidence rank of the true label.</synopsis>
        <help>&lt;p&gt;This performance evaluator operator should be used for
            tasks, where it is not only important that the real class is assigned the 
            highest but receives a comparably high confidence.&lt;/p&gt;
            &lt;p&gt;This operator will sort the confidences for each label and depending of the
            rank position of the real label, costs are generated. You can define these costs
            by the parameter ranking_costs. The costs are entered for hole intervalls, so you 
            don't have to enter a cost value for each rank. These intervalls are defined
            by their start rank and range either until the start of the next interval or infinite.
            Everything before the first mentioned rank will receive costs of 0. The counting of 
            rank starts by 0, so the most confident label is rank 0.&lt;/p&gt;
            &lt;p&gt;The costs are entered on the right side of the table.&lt;/p&gt;
            &lt;p&gt;For example, if you want to assing costs of zero if the true label is predicted with 
            the highest confidence, 1 for the second place, 2 for the third and 10 for each following, you 
            have to enter:&lt;/p&gt;
            &lt;table border="1px"&gt; 
            &lt;tr&gt;
            &lt;td&gt;
            1
            &lt;/td&gt;
            &lt;td&gt;
            1
            &lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
            &lt;td&gt;
            2
            &lt;/td&gt;
            &lt;td&gt;
            2
            &lt;/td&gt;
            
            &lt;/tr&gt;
            &lt;tr&gt;
            &lt;td&gt;
            3
            &lt;/td&gt;
            &lt;td&gt;
            10
            &lt;/td&gt;
            &lt;/tr&gt;
            &lt;/table&gt;
        </help>
    <key>performance_ranking</key>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>Performance (Regression)</name>
        <synopsis>This operator delivers as output a list of performance
            values according to a list of selected performance criteria (for
            regression tasks).</synopsis>
        <help>&lt;p&gt;This performance evaluator operator should be used for
            regression tasks, i.e. in cases where the label attribute has a
            numerical value type. The operator expects a test
            &lt;i&gt;ExampleSet&lt;/i&gt; as input, whose elements have both true
            and predicted labels, and delivers as output a list of performance
            values according to a list of performance criteria that it
            calculates. If an input performance vector was already given, this is
            used for keeping the performance values.&lt;/p&gt; &lt;p&gt;All of
            the performance criteria can be switched on using boolean parameters.
            Their values can be queried by a ProcessLogOperator using the same
            names. The main criterion is used for comparisons and need to be
            specified only for processes where performance vectors are compared,
            e.g. feature selection or other meta optimization process setups. If
            no other main criterion was selected, the first criterion in the
            resulting performance vector will be assumed to be the main
            criterion.&lt;/p&gt; &lt;p&gt;The resulting performance vectors are
            usually compared with a standard performance comparator which only
            compares the fitness values of the main criterion. Other
            implementations than this simple comparator can be specified using
            the parameter &lt;var&gt;comparator_class&lt;/var&gt;. This may for
            instance be useful if you want to compare performance vectors
            according to the weighted sum of the individual criteria. In order to
            implement your own comparator, simply subclass
            &lt;i&gt;PerformanceComparator&lt;/i&gt;. Please note that for true
            multi-objective optimization usually another selection scheme is used
            instead of simply replacing the performance comparator.&lt;/p&gt;
        </help>
    <key>performance_regression</key>
    <tags>
         <tag>RMSE</tag>
         <tag>Errors</tag>
         <tag>Absolute</tag>
         <tag>Relative</tag>
         <tag>Squared</tag>
    </tags>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>LogisticRegression</name>
        <synopsis>A logistic regression learner for binary classification
            tasks.</synopsis>
        <help>This operator determines a logistic regression model.</help>
    <key>logisticregression</key>
  </operator>
    <operator>
        <name>Discretize by Binning</name>
        <synopsis>Discretize numerical attributes into a user defined number
            of bins.</synopsis>
        <help>This operator discretizes all numeric attributes in the dataset
            into nominal attributes. This discretization is performed by simple
            binning, i.e. the specified number of equally sized bins is created
            and the numerical values are simply sorted into those bins. Skips all
            special attributes including the label.</help>
    <key>discretize_by_bins</key>
    <tags>
         <tag>Continous</tag>
         <tag>Categorical</tag>
         <tag>Nominal</tag>
         <tag>Polynominal</tag>
         <tag>Ordinary</tag>
         <tag>Discrete</tag>
         <tag>Discretization</tag>
         <tag>Dichotomization</tag>
         <tag>Dichotomy</tag>
         <tag>Binning</tag>
         <tag>Histogram</tag>
         <tag>Types</tag>
         <tag>Qualitative</tag>
         <tag>Quantitative</tag>
         <tag>Groups</tag>
         <tag>Intervals</tag>
    </tags>
    <shortName>Discretize</shortName>
  </operator>
    <operator>
        <name>Fill Data Gaps</name>
        <synopsis>This operator fills gaps in the data based on the ID
            attribute of the data set.</synopsis>
        <help>&lt;p&gt;This operator fills gaps in the data based on the ID
            attribute of the data set. The ID attribute must either have the
            value type &amp;quot;integer&amp;quot; or one of the data value
            types.&lt;/p&gt; &lt;p&gt;The operator performs the following
            steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The data is sorted according to
            the ID attribute&lt;/li&gt; &lt;li&gt;All occurring distances between
            consecutive ID values are calculated&lt;/li&gt; &lt;li&gt;The
            greatest common divisor (GCD) of all distances is
            calculated&lt;/li&gt; &lt;li&gt;All rows which would have an ID value
            which is a multiple of the GCD but are missing are added to the data
            set&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Please note that all values of
            attributes beside the ID attribute will have a missing value which
            often must be replaced as a next step.&lt;/p&gt;</help>
    <key>fill_data_gaps</key>
  </operator>
    <operator>
        <name>Combine Performances</name>
        <synopsis>Returns a performance vector containing the weighted fitness
            value of the input criteria.</synopsis>
        <help>Returns a performance vector containing the weighted fitness
            value of the input criteria.</help>
    <key>combine_performances</key>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>ANOVA</name>
        <synopsis>Performs ANalysis Of VAriances to determine the probability
            for the null hypothesis 'the actual means are the same'.</synopsis>
        <help>Determines if the null hypothesis (all actual mean values are
            the same) holds for the input performance vectors. This operator uses
            an ANalysis Of VAriances approach to determine probability that the
            null hypothesis is wrong.</help>
    <key>anova</key>
  </operator>
    <operator>
        <name>Cluster Distance Performance</name>
        <synopsis>Delivers a performance based on cluster centroids.
        </synopsis>
        <help>An evaluator for centroid based clustering methods. The average
            within cluster distance is calculated by averaging the distance
            between the centroid and all examples of a cluster.</help>
    <key>cluster_distance_performance</key>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>Generate Nominal Data</name>
        <synopsis>Generates an example set based on nominal attributes.
        </synopsis>
        <help>Generates a random example set for testing purposes. All
            attributes have only (random) nominal values and a classification
            label.</help>
    <key>generate_nominal_data</key>
  </operator>
    <operator>
        <name>Optimize Selection (Brute Force)</name>
        <synopsis>Selects the best features for an example set by trying all
            possible combinations of attribute selections.</synopsis>
        <help>This feature selection operator selects the best attribute set
            by trying all possible combinations of attribute selections. It
            returns the example set containing the subset of attributes which
            produced the best performance. As this operator works on the powerset
            of the attributes set it has exponential runtime.</help>
    <key>optimize_selection_brute_force</key>
  </operator>

    <operator>
        <name>AbsoluteStratifiedSampling</name>
        <synopsis>Creates a stratified sample from an example set by drawing a
            given number of examples.</synopsis>
        <help>Stratified sampling operator. This operator performs a random
            sampling of a given size. In contrast to the simple sampling
            operator, this operator performs a stratified sampling for data sets
            with nominal label attributes, i.e. the class distributions remains
            (almost) the same after sampling. Hence, this operator cannot be
            applied on data sets without a label or with a numerical label. In
            these cases a simple sampling without stratification is performed. In
            some cases it might happen that not the exact desired number of
            examples is sampled, e.g. if the desired number is 100 from three
            qually distributed classes the resulting number will be 99 (33 of
            each class).</help>
    <key>absolutestratifiedsampling</key>
  </operator>
    <operator>
        <name>Read CSV</name>
        <synopsis>This operator can read csv files.</synopsis>
        <help>&lt;p&gt;This operator can read csv files, where all values of an example are written into one line and separated by an constant separator.
         The separator might be specified in the &lt;b&gt;column separators&lt;/b&gt; parameter. The default will split the line on each comma, semicolon and blank. Arbitrary regular expressions are usable as separator.
         Empty values and the question mark will be read as missing values. You can quote the values (including
         the column separators) with a double quote (&quot;). You can
         escape the quoting character with a backslash, i.e.
         \&quot;.&lt;/p&gt; 
         &lt;p&gt;The first line is used for the attribute names as default, controlled by the &lt;b&gt;use first row as attribute names&lt;/b&gt; parameter.&lt;br /&gt;
         This operator tries to determine an appropriate type of the attributes by reading the first few lines and checking the occuring values. If all values are integers, the attribute will become integer, if 
         real numbers occur, it will be of type real. Columns containing values which can't be interpreted as numbers will be nominal, as long as they don't match the date and time pattern of the &lt;b&gt;date format&lt;/b&gt; parameter. If they do, this column of the csv file will be automatically parsed as date and the according attribute will be of type date.
         &lt;/p&gt;</help>
    <key>read_csv</key>
    <tags>
         <tag>Load</tag>
         <tag>Import</tag>
         <tag>Read</tag>
         <tag>Data</tag>
         <tag>Files</tag>
         <tag>Text</tag>
         <tag>Commas</tag>
         <tag>Spreadsheet</tag>
         <tag>Excel</tag>
         <tag>Datasets</tag>
         <tag>Tsv</tag>
    </tags>
  </operator>
     
   <operator>
        <name>Read URL</name>
        <synopsis>This operator reads an example set from a URL. It allows
            only a fixed data format but on the other hand is able to read data
            from arbitrary sources.</synopsis>
        <help>&lt;p&gt; This operator reads an example set from an URL. The
            format has to be a CSV format with ';' as column separator and
            nominal values have to be quoted with a double quote (&amp;quot;). A
            quote inside of a nominal value has to be escaped by a backslash like
            in \&amp;quot;. The first row is allowed to contain the column names
            which has to be indicated by the corresponding parameter. Comments
            are not allowed, unknown attribute values can be marked with empty
            strings or a question mark. &lt;/p&gt; &lt;p&gt; This operator is not
            nearly as powerful as the operators ExampleSource or
            SimpleExampleSource but is on the other hand able to read data from
            arbitrary places as long as the format fits the specification above.
            Please note also that the usage of this operator hardly allows for a
            correct meta data description which might lead to problems if the
            meta data between training and test set differ in a learning
            scenario. &lt;/p&gt; &lt;p&gt; Attribute roles can not be directly
            set during loading but the operator ChangeAttributeRole has to be
            used after loading in order to change the roles. &lt;/p&gt;</help>
    <key>read_url</key>
  </operator>
    <operator>
        <name>Add</name>
        <synopsis>This operator adds an additional value to a specified
            nominal attribute which is then mapped to a specific index.
        </synopsis>
        <help>Adds a value to a nominal attribute definition.</help>
    <key>add</key>
  </operator>
    <operator>
        <name>Weight by User Specification</name>
        <synopsis>This operator defines the weights for all features based on
            a list of regular expressions for the feature names which can be used
            to set a specified weight to features with a name fulfilling these
            expressions.</synopsis>
        <help>&lt;p&gt;This operator is able to create feature weights based
            on regular expressions defined for the feature names. For example,
            the user can map all features with a name starting with "Att" to the
            weight 0.5 by using the regular expression "Att.*". Alternatively,
            the specified weight may be considered as weight sum for all
            attributes matching the corresponding regular expression and may be
            equally distributed among these attributes. All other feature weights
            whose feature names are not covered by one of the regular expressions
            are set to the default weight.&lt;/p&gt; &lt;p&gt;Please note that
            the weights defined in the regular expression list are set in the
            order as they are defined in the list, i.e. weights can overwrite
            weights set before.&lt;/p&gt;</help>
    <key>weight_by_user_specification</key>
  </operator>

    <operator>
        <name>FeatureValueTypeFilter</name>
        <synopsis>This operator switches off those features whose value type
            matches the given one.</synopsis>
        <help>This operator switches off all features whose value type matches
            the one given in the parameter
            &lt;code&gt;skip_features_of_type&lt;/code&gt;. This can be useful
            e.g. for learning schemes that can handle only nominal attributes.
        </help>
    <key>featurevaluetypefilter</key>
  </operator>
    <operator>
        <name>Join</name>
        <synopsis>Build the join of two example sets using the id attributes
            of the sets in order to identify the same examples.</synopsis>
        <help>&lt;p&gt; Build the join of two example sets using the id
            attributes of the sets, i.e. both example sets must have an id
            attribute where the same id indicate the same examples. If examples
            are missing an exception will be thrown. The result example set will
            consist of the same number of examples but the union set or the union
            list (depending on parameter setting double attributes will be
            removed or renamed) of both feature sets. In case of removing double
            attribute the attribute values must be the same for the examples of
            both example set, otherwise an exception will be thrown. &lt;/p&gt;
            &lt;p&gt; Please note that this check for double attributes will only
            be applied for regular attributes. Special attributes of the second
            input example set which do not exist in the first example set will
            simply be added. If they already exist they are simply skipped.
            &lt;/p&gt;</help>
    <key>join</key>
    <tags>
         <tag>Combine</tag>
         <tag>Merge</tag>
         <tag>Match</tag>
         <tag>Left</tag>
         <tag>Right</tag>
         <tag>Outer</tag>
         <tag>Inner</tag>
         <tag>Cartesian</tag>
    </tags>
  </operator>

    <operator>
        <name>MinMaxBinDiscretization</name>
        <synopsis>Discretize numerical attributes into a user defined number
            of bins lying in a user defined value range.</synopsis>
        <help>This operator discretizes all numeric attributes in the dataset
            into nominal attributes. This discretization is performed by simple
            binning, i.e. the specified number of equally sized bins is created
            and the numerical values are simply sorted into those bins. Skips all
            special attributes including the label. In contrast to the usual
            simple binning performed by the &lt;i&gt;BinDiscretization&lt;/i&gt;,
            this operator bins the values into a predefined range (and not into
            the range defined by the minimum and maximum values taken from the
            data).</help>
    <key>minmaxbindiscretization</key>
  </operator>

    <operator>
        <name>Bayesian Boosting</name>
        <synopsis>Boosting operator based on Bayes' theorem.</synopsis>
        <help>&lt;p&gt;This operator trains an ensemble of classifiers for
            boolean target attributes. In each iteration the training set is
            reweighted, so that previously discovered patterns and other kinds of
            prior knowledge are &amp;quot;sampled out&amp;quot; {@rapidminer.cite
            Scholz/2005b}. An inner classifier, typically a rule or decision tree
            induction algorithm, is sequentially applied several times, and the
            models are combined to a single global model. The number of models to
            be trained maximally are specified by the parameter
            &lt;code&gt;iterations&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;If the
            parameter &lt;code&gt;rescale_label_priors&lt;/code&gt; is set, then
            the example set is reweighted, so that all classes are equally
            probable (or frequent). For two-class problems this turns the problem
            of fitting models to maximize weighted relative accuracy into the
            more common task of classifier induction {@rapidminer.cite
            Scholz/2005a}. Applying a rule induction algorithm as an inner
            learner allows to do subgroup discovery. This option is also
            recommended for data sets with class skew, if a &amp;quot;very weak
            learner&amp;quot; like a decision stump is used. If
            &lt;code&gt;rescale_label_priors&lt;/code&gt; is not set, then the
            operator performs boosting based on probability estimates.&lt;/p&gt;
            &lt;p&gt;The estimates used by this operator may either be computed
            using the same set as for training, or in each iteration the training
            set may be split randomly, so that a model is fitted based on the
            first subset, and the probabilities are estimated based on the
            second. The first solution may be advantageous in situations where
            data is rare. Set the parameter
            &lt;code&gt;ratio_internal_bootstrap&lt;/code&gt; to 1 to use the
            same set for training as for estimation. Set this parameter to a
            value of lower than 1 to use the specified subset of data for
            training, and the remaining examples for probability
            estimation.&lt;/p&gt; &lt;p&gt;If the parameter
            &lt;code&gt;allow_marginal_skews&lt;/code&gt; is
            &lt;em&gt;not&lt;/em&gt; set, then the support of each subset defined
            in terms of common base model predictions does not change from one
            iteration to the next. Analogously the class priors do not change.
            This is the procedure originally described in {@rapidminer.cite
            Scholz/2005b} in the context of subgroup discovery.&lt;/p&gt;
            &lt;p&gt;Setting the &lt;code&gt;allow_marginal_skews&lt;/code&gt;
            option to &lt;code&gt;true&lt;/code&gt; leads to a procedure that
            changes the marginal weights/probabilities of subsets, if this is
            beneficial in a boosting context, and stratifies the two classes to
            be equally likely. As for AdaBoost, the total weight upper-bounds the
            training error in this case. This bound is reduced more quickly by
            the BayesianBoosting operator, however.&lt;/p&gt; &lt;p&gt;In sum, to
            reproduce the sequential sampling, or knowledge-based sampling, from
            {@rapidminer.cite Scholz/2005b} for subgroup discovery, two of the
            default parameter settings of this operator have to be changed:
            &lt;code&gt;rescale_label_priors&lt;/code&gt; must be set to
            &lt;code&gt;true&lt;/code&gt;, and
            &lt;code&gt;allow_marginal_skews&lt;/code&gt; must be set to
            &lt;code&gt;false&lt;/code&gt;. In addition, a boolean (binomial)
            label has to be used.&lt;/p&gt; &lt;p&gt;The operator requires an
            example set as its input. To sample out prior knowledge of a
            different form it is possible to provide another model as an optional
            additional input. The predictions of this model are used to weight
            produce an initial weighting of the training set. The ouput of the
            operator is a classification model applicable for estimating
            conditional class probabilities or for plain crisp classification. It
            contains up to the specified number of inner base models. In the case
            of an optional initial model, this model will also be stored in the
            output model, in order to produce the same initial weighting during
            model application.&lt;/p&gt;</help>
    <key>bayesian_boosting</key>
  </operator>
    <operator>
        <name>Cluster Count Performance</name>
        <synopsis>Delivers a performance based on the number of clusters.
        </synopsis>
        <help>This operator does actually not compute a performance criterion
            but simply provides the number of clusters as a value.</help>
    <key>cluster_count_performance</key>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>Sort</name>
        <synopsis>This operator sorts the given &lt;i&gt;ExampleSet&lt;/i&gt; according to a
            single attribute.</synopsis>
        <help>
        &lt;p&gt; This operator sorts the given  &lt;i&gt;ExampleSet&lt;/i&gt; according to
            a single attribute specified by the &lt;b&gt;attribute_name&lt;/b&gt; parameter. The examples are sorted according to the
            natural order of the values of this attribute either in increasing or
            in decreasing direction, depending on the setting of &lt;b&gt;sorting direction&lt;/b&gt;. &lt;/p&gt;
        </help>
    <key>sort</key>
    <tags>
         <tag>Rank</tag>
         <tag>Order</tag>
         <tag>Ascending</tag>
         <tag>Descending</tag>
    </tags>
  </operator>

    <operator>
        <name>Generate Direct Mailing Data</name>
        <synopsis>Generates data for testing purposes based on a direct
            mailing data set.</synopsis>
        <help>Generates a random example set for testing purposes. The data
            represents a direct mailing example set.</help>
    <key>generate_direct_mailing_data</key>
  </operator>
    <operator>
        <name>Numerical to Binominal</name>
        <synopsis>Maps all numeric values to 'false' if they are in the
            specified range (typical: equal 0.0) and to 'true' otherwise.
        </synopsis>
        <help>Converts all numerical attributes to binary ones. If the value
            of an attribute is between the specified minimal and maximal value,
            it becomes &lt;em&gt;false&lt;/em&gt;, otherwise
            &lt;em&gt;true&lt;/em&gt;. If the value is missing, the new value
            will be missing. The default boundaries are both set to 0, thus only
            0.0 is mapped to false and all other values are mapped to true.
        </help>
    <key>numerical_to_binominal</key>
    <tags>
         <tag>Binary</tag>
         <tag>Binarizer</tag>
         <tag>Dual</tag>
         <tag>Categorical</tag>
         <tag>Continous</tag>
         <tag>Types</tag>
    </tags>
  </operator>

    <operator>
        <name>ExperimentLog</name>
        <synopsis>Clears a table generated by a ProcessLog operator.
        </synopsis>
        <help>This operator can be used to clear a data table generated by a
            &lt;i&gt;ProcessLogOperator&lt;/i&gt;.</help>
    </operator>
    <operator>
        <name>Flatten Clustering</name>
        <synopsis>Creates a flat cluster model from a hierarchical one.
        </synopsis>
        <help>Creates a flat cluster model from a hierarchical one by
            expanding nodes in the order of their distance until the desired
            number of clusters is reached.</help>
    <key>flatten_clustering</key>
  </operator>
    <operator>
        <name>Split Validation</name>
        <synopsis>A SimpleValidation randomly splits up the example set into a
            training and test set and evaluates the model.</synopsis>
        <help>&lt;p&gt; A &lt;code&gt;RandomSplitValidationChain&lt;/code&gt;
            splits up the example set into a training and test set and evaluates
            the model. The first inner operator must accept an
            &lt;i&gt;ExampleSet&lt;/i&gt; while the second must accept an
            &lt;i&gt;ExampleSet&lt;/i&gt; and the output of the first (which is
            in most cases a &lt;i&gt;Model&lt;/i&gt;) and must produce a
            &lt;i&gt;PerformanceVector&lt;/i&gt;. &lt;/p&gt; &lt;p&gt;This
            validation operator provides several values which can be logged by
            means of a &lt;i&gt;ProcessLogOperator&lt;/i&gt;. All performance
            estimation operators of RapidMiner provide access to the average
            values calculated during the estimation. Since the operator cannot
            ensure the names of the delivered criteria, the ProcessLog operator
            can access the values via the generic value names:&lt;/p&gt;
            &lt;ul&gt; &lt;li&gt;performance: the value for the main criterion
            calculated by this validation operator&lt;/li&gt;
            &lt;li&gt;performance1: the value of the first criterion of the
            performance vector calculated&lt;/li&gt; &lt;li&gt;performance2: the
            value of the second criterion of the performance vector
            calculated&lt;/li&gt; &lt;li&gt;performance3: the value of the third
            criterion of the performance vector calculated&lt;/li&gt;
            &lt;li&gt;for the main criterion, also the variance and the standard
            deviation can be accessed where applicable.&lt;/li&gt; &lt;/ul&gt;
        </help>
    <key>split_validation</key>
    <tags>
         <tag>Divide</tag>
         <tag>Separate</tag>
         <tag>Part</tag>
         <tag>Training</tag>
         <tag>Testing</tag>
         <tag>Holdout</tag>
         <tag>Partitions</tag>
         <tag>Validations</tag>
         <tag>Evaluations</tag>
    </tags>
    <shortName>Validation</shortName>
  </operator>
    <operator>
        <name>Performance (Costs)</name>
        <synopsis>A cost evaluator delivers as output the costs for given
            classification results.</synopsis>
        <help>This operator provides the ability to evaluate classification
            costs. Therefore a cost matrix might be specified, denoting the costs
            for every possible classification outcome: predicted label x real
            label. Costs will be minimized during optimization.</help>
    <key>performance_costs</key>
    <shortName>Performance</shortName>
  </operator>
    
    <operator>
        <name>Transition Graph</name>
        <synopsis>Creates a graphical representation of transitions defined by
            source and target attributes.</synopsis>
        <help>&lt;p&gt;This operator creates a transition graph from the given
            example set. The example set must have a specific structure with (at
            least) two columns where one column specifies the source of the
            transition and the second column specifies the target of the
            transition. Optionally, a third column can be specified in order to
            define the strength of the transition (this column can for example
            store the number of times this transition occurred after an
            aggregation).&lt;/p&gt; &lt;p&gt;The parameter
            &amp;quot;node_description&amp;quot; will be used for displaying
            information about the nodes if the information is made available via
            an example visualization operator. The string might contain macros
            pointing to attribute names.&lt;/p&gt;</help>
    <key>transition_graph</key>
  </operator>
    <operator>
        <name>Nominal to Date</name>
        <synopsis>Parses the nominal values for the specified attribute with
            respect to the given date format string and transforms the values
            into date values.</synopsis>
        <help>&lt;p&gt;This operator parses given nominal attributes in order
            to create date and / or time attributes. The date format can be
            specified by the &lt;b&gt;date_format&lt;/b&gt; parameter. The old nominal attribute
            will be removed and replaced by a new date attribute if the
            corresponding parameter &lt;b&gt;keep_old_attribute&lt;/b&gt; is not set (default).&lt;/p&gt;
            &lt;br /&gt;
            &lt;h4&gt;Date and Time Patterns&lt;/h4&gt; &lt;p&gt; Date and time
            formats are specified by &lt;em&gt;date and time pattern&lt;/em&gt;
            strings in the &lt;b&gt;date_format&lt;/b&gt; parameter. Within date and time pattern
            strings, unquoted letters from &lt;code&gt;'A'&lt;/code&gt; to
            &lt;code&gt;'Z'&lt;/code&gt; and from &lt;code&gt;'a'&lt;/code&gt; to
            &lt;code&gt;'z'&lt;/code&gt; are interpreted as pattern letters
            representing the components of a date or time string. Text can be
            quoted using single quotes (&lt;code&gt;'&lt;/code&gt;) to avoid
            interpretation. &lt;code&gt;"''"&lt;/code&gt; represents a single
            quote. All other characters are not interpreted; they're simply
            copied into the output string during formatting or matched against
            the input string during parsing.&lt;/p&gt; &lt;p&gt; The following
            pattern letters are defined (all other characters from
            &lt;code&gt;'A'&lt;/code&gt; to &lt;code&gt;'Z'&lt;/code&gt; and from
            &lt;code&gt;'a'&lt;/code&gt; to &lt;code&gt;'z'&lt;/code&gt; are
            reserved):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;G&lt;/em&gt;: era
            designator; Text; example: AD&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;y&lt;/em&gt;: year; Year; example: 1996;
            96&lt;/li&gt; &lt;li&gt;&lt;em&gt;M&lt;/em&gt;: month in year; Month;
            example: July; Jul; 07&lt;/li&gt; &lt;li&gt;&lt;em&gt;w&lt;/em&gt;:
            week in year; Number; example: 27&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;W&lt;/em&gt;: week in month; Number; example:
            2&lt;/li&gt; &lt;li&gt;&lt;em&gt;D&lt;/em&gt;: day in year; Number;
            example: 189&lt;/li&gt; &lt;li&gt;&lt;em&gt;d&lt;/em&gt;: day in
            month; Number; example: 10&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;F&lt;/em&gt;: day of week in month; Number;
            example: 2&lt;/li&gt; &lt;li&gt;&lt;em&gt;E&lt;/em&gt;: day in week;
            Text; example: Tuesday; Tue&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;a&lt;/em&gt;: am/pm marker; Text; example:
            PM&lt;/li&gt; &lt;li&gt;&lt;em&gt;H&lt;/em&gt;: hour in day (0-23);
            Number; example: 0&lt;/li&gt; &lt;li&gt;&lt;em&gt;k&lt;/em&gt;: hour
            in day (1-24); Number; example: 24&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;K&lt;/em&gt;: hour in am / pm (0-11); Number;
            example: 0&lt;/li&gt; &lt;li&gt;&lt;em&gt;h&lt;/em&gt;: hour in am /
            pm (1-12); Number; example: 12&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;m&lt;/em&gt;: minute in hour; Number; example:
            30&lt;/li&gt; &lt;li&gt;&lt;em&gt;s&lt;/em&gt;: second in minute;
            Number; example: 55&lt;/li&gt; &lt;li&gt;&lt;em&gt;S&lt;/em&gt;:
            millisecond; Number; example: 978&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;z&lt;/em&gt;: time zone; General Time Zone;
            example: Pacific Standard Time; PST; GMT-08:00&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;Z&lt;/em&gt;: time zone; RFC 822 Time Zone;
            example: -0800&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Pattern letters are
            usually repeated, as their number determines the exact
            presentation:&lt;/p&gt; &lt;ul&gt;
            &lt;li&gt;&lt;em&gt;Text:&lt;/em&gt; For formatting, if the number of
            pattern letters is 4 or more, the full form is used; otherwise a
            short or abbreviated form is used if available. For parsing, both
            forms are accepted, independent of the number of pattern
            letters.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Number:&lt;/em&gt; For
            formatting, the number of pattern letters is the minimum number of
            digits, and shorter numbers are zero-padded to this amount. For
            parsing, the number of pattern letters is ignored unless it's needed
            to separate two adjacent fields.&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;Year:&lt;/em&gt; If the underlying calendar is
            the Gregorian calendar, the following rules are applied. &lt;ul&gt;
            &lt;li&gt;For formatting, if the number of pattern letters is 2, the
            year is truncated to 2 digits; otherwise it is interpreted as a
            &lt;em&gt;number&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;For parsing, if the
            number of pattern letters is more than 2, the year is interpreted
            literally, regardless of the number of digits. So using the pattern
            "MM/dd/yyyy", "01/11/12" parses to Jan 11, 12 A.D.&lt;/li&gt;
            &lt;li&gt;For parsing with the abbreviated year pattern ("y" or
            "yy"), this operator must interpret the abbreviated year relative to
            some century. It does this by adjusting dates to be within 80 years
            before and 20 years after the time the operator is created. For
            example, using a pattern of "MM/dd/yy" and the operator created on
            Jan 1, 1997, the string &quot;01/11/12&quot; would be
            interpreted as Jan 11, 2012 while the string
            &quot;05/04/64&quot; would be interpreted as May 4, 1964.
            During parsing, only strings consisting of exactly two digits will be
            parsed into the default century. Any other numeric string, such as a
            one digit string, a three or more digit string, or a two digit string
            that isn't all digits (for example, &quot;-1&quot;), is
            interpreted literally. So &quot;01/02/3&quot; or
            &quot;01/02/003&quot; are parsed, using the same pattern, as
            Jan 2, 3 AD. Likewise, &quot;01/02/-3&quot; is parsed as Jan
            2, 4 BC.&lt;/li&gt; &lt;/ul&gt; Otherwise, calendar system specific
            forms are applied. If the number of pattern letters is 4 or more, a
            calendar specific long form is used. Otherwise, a calendar short or
            abbreviated form is used.&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;Month:&lt;/em&gt; If the number of pattern
            letters is 3 or more, the month is interpreted as
            &lt;em&gt;text&lt;/em&gt;; otherwise, it is interpreted as a
            &lt;em&gt;number&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;em&gt;General
            time zone:&lt;/em&gt; Time zones are interpreted as
            &lt;em&gt;text&lt;/em&gt; if they have names. It is possible to
            define time zones by representing a GMT offset value. RFC 822 time
            zones are also accepted.&lt;/li&gt; &lt;li&gt;&lt;em&gt;RFC 822 time
            zone:&lt;/em&gt; For formatting, the RFC 822 4-digit time zone format
            is used. General time zones are also accepted.&lt;/li&gt; &lt;/ul&gt;
            &lt;p&gt;This operator also supports &lt;em&gt;localized date and
            time pattern&lt;/em&gt; strings by defining the locale parameter. In
            these strings, the pattern letters described above may be replaced
            with other, locale dependent, pattern letters.&lt;/p&gt;
            &lt;h4&gt;Examples&lt;/h4&gt; &lt;p&gt;&lt;br/&gt;The following examples show
            how date and time patterns are interpreted in the U.S. locale. The
            given date and time are 2001-07-04 12:08:56 local time in the U.S.
            Pacific Time time zone.&lt;/p&gt; &lt;ul&gt;
            &lt;li&gt;&lt;em&gt;&quot;yyyy.MM.dd G 'at' HH:mm:ss
            z&quot;&lt;/em&gt;: 2001.07.04 AD at 12:08:56 PDT&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&quot;EEE, MMM d, ''yy&quot;&lt;/em&gt;:
            Wed, Jul 4, '01&lt;/li&gt; &lt;li&gt;&lt;em&gt;&quot;h:mm
            a&quot;&lt;/em&gt;: 12:08 PM&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&quot;hh 'o''clock' a,
            zzzz&quot;&lt;/em&gt;: 12 o'clock PM, Pacific Daylight
            Time&lt;/li&gt; &lt;li&gt;&lt;em&gt;&quot;K:mm a,
            z&quot;&lt;/em&gt;: 0:08 PM, PDT&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&quot;yyyy.MMMMM.dd GGG hh:mm
            aaa&quot;&lt;/em&gt;: 02001.July.04 AD 12:08 PM&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&quot;EEE, d MMM yyyy HH:mm:ss
            Z&quot;&lt;/em&gt;: Wed, 4 Jul 2001 12:08:56 -0700&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&quot;yyMMddHHmmssZ&quot;&lt;/em&gt;:
            010704120856-0700&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&quot;yyyy-MM-dd'T'HH:mm:ss.SSSZ&quot;&lt;/em&gt;:
            2001-07-04T12:08:56.235-0700&lt;/li&gt; &lt;/ul&gt;</help>
    <key>nominal_to_date</key>
    <tags>
         <tag>Dates</tag>
         <tag>Times</tag>
         <tag>Datetimes</tag>
         <tag>Formats</tag>
         <tag>Nominal</tag>
         <tag>Categorical</tag>
         <tag>Polynominal</tag>
         <tag>Weeks</tag>
         <tag>Days</tag>
         <tag>Months</tag>
         <tag>Years</tag>
         <tag>Quarters</tag>
         <tag>Hours</tag>
         <tag>Minutes</tag>
         <tag>Seconds</tag>
         <tag>Types</tag>
    </tags>
  </operator>
        <operator>
        <name>Aggregate</name>
        <synopsis>Performs one of the aggregation functions (count, sum...)
            known from SQL (allows also grouping).</synopsis>
        <help>&lt;p&gt;This operator creates a new example set from the input
            example set showing the results of arbitrary aggregation functions
            (as SUM, COUNT etc. known from SQL). Before the values of different
            rows are aggregated into a new row the rows might be grouped by the
            values of a multiple attributes (similar to the group-by clause known
            from SQL). In this case a new line will be created for each
            group.&lt;/p&gt; &lt;p&gt;Please note that the known HAVING clause
            from SQL can be simulated by an additional
            &lt;i&gt;ExampleFilter&lt;/i&gt; operator following this
            one.&lt;/p&gt;</help>
    <key>aggregate</key>
    <tags>
         <tag>Groupby</tag>
         <tag>Group by</tag>
         <tag>Grouping</tag>
         <tag>Sum</tag>
         <tag>Count</tag>
         <tag>Min</tag>
         <tag>Max</tag>
         <tag>Average</tag>
         <tag>Avg</tag>
         <tag>Mean</tag>
         <tag>Pivot</tag>
         <tag>Cross-table</tag>
         <tag>Crosstable</tag>
         <tag>Distinct</tag>
    </tags>
  </operator>
    <operator>
        <name>IOConsumer</name>
        <synopsis>This operators simply consumes some unused outputs.
        </synopsis>
        <help>Most RapidMiner operators should define their desired input and
            delivered output in a senseful way. In some cases operators can
            produce additional output which is indicated with a boolean
            parameter. Other operators are able to deliver their input as output
            instead of consuming it (parameter keep_...). However, in some cases
            it might be usefull to delete unwanted output to ensure that
            following operators use the correct input object. Furthermore, some
            operators produce additional unneeded and therefore unconsumed
            output. In an iterating operator chain this unneeded output will grow
            with each iteration. Therefore, the IOConsumeOperator can be used to
            delete one (the n-th) object of a given type (indicated by
            delete_one), all input objects of a given type (indicated by
            delete_all), all input objects but those of a given type (indicated
            by delete_all_but), or all input objects of the given type except for
            the n-th object of the type.</help>
    <key>ioconsumer</key>
  </operator>
    
    <operator>
        <name>Read AML</name>
        <synopsis>&lt;p&gt;This operator reads an example set from file. The operator can be configured to read almost all line based file formats.&lt;/p&gt;</synopsis>
        <help>&lt;p&gt;This operator reads an example set from (a) file(s). Probably you can use the default parameter values for the most file formats (including the format produced by the ExampleSetWriter, CSV, ...). Please refer to section &lt;i&gt;First steps/File formats&lt;/i&gt; for details on the attribute description file set by the parameter &lt;var&gt;attributes&lt;/var&gt; used to specify attribute types. You can use the wizard of this operator or the tool Attribute Editor in order to create those meta data .aml files for your datasets.&lt;/p&gt;&lt;p&gt; This operator supports the reading of data from multiple source files. Each attribute (including special attributes like labels, weights, ...) might be read from another file. Please note that only the minimum number of lines of all files will be read, i.e. if one of the data source files has less lines than the others, only this number of examples will be read. &lt;/p&gt; &lt;p&gt;The split points can be defined with regular expressions (please refer to one of the plenty tutorials available on the web for an introduction). The default split parameter &amp;quot;,\s*|;\s*|\s+&amp;quot; should work for most file formats. This regular expression describes the following       column separators &lt;/p&gt; &lt;ul&gt;&lt;li&gt; the character &amp;quot;,&amp;quot; followed by a whitespace of arbitrary length (also no white space) &lt;/li&gt; &lt;li&gt; the character &amp;quot;;&amp;quot; followed by a whitespace of arbitrary length (also no white space) &lt;/li&gt; &lt;li&gt; a whitespace of arbitrary length (min. 1) &lt;/li&gt; &lt;/ul&gt; A logical XOR is defined by &amp;quot;|&amp;quot;. Other useful separators might be &amp;quot;\t&amp;quot; for tabulars, &amp;quot; &amp;quot; for a single whitespace, and &amp;quot;\s&amp;quot; for any whitespace.&lt;/p&gt;&lt;p&gt;&lt;br&gt; &lt;p&gt; Quoting is also possible with &amp;quot;. You can escape quotes with a backslash, i.e. \&amp;quot;. Please note that you can change these characters by adjusting the corresponding settings. &lt;/p&gt; &lt;p&gt; Additionally you can specify comment characters which can be used at arbitrary locations of the data lines. Any content after the comment character will be ignored. Unknown attribute values can be marked with empty strings (if this is possible for your column separators) or by a question mark (recommended). &lt;/p&gt;</help>
    <key>read_aml</key>
  </operator>
    <operator>
        <name>FeatureBlockTypeFilter</name>
        <synopsis>This operator switches off those features whose block type
            matches the given one.</synopsis>
        <help>This operator switches off all features whose block type matches
            the one given in the parameter
            &lt;code&gt;skip_features_of_type&lt;/code&gt;. This can be useful
            e.g. for preprocessing operators that can handle only series
            attributes.</help>
    <key>featureblocktypefilter</key>
  </operator>
    <operator>
        <name>Log</name>
        <synopsis>Saves almost arbitrary data to a log table (also possibly in
            a file) and create statistics for online plotting of
            values/parameters provided by operators.</synopsis>
        <help>This operator records almost arbitrary data. It can be written
            to a file which can then be read, e.g., by gnuplot. Alternatively,
            the collected data can be plotted by the GUI. This is even possible
            during process runtime (i.e. online plotting).&lt;br/&gt; Parameters
            in the list &lt;code&gt;log&lt;/code&gt; are interpreted as follows:
            The &lt;var&gt;key&lt;/var&gt; gives the name for the column name
            (e.g. for use in the plotter). The &lt;var&gt;value&lt;/var&gt;
            specifies where to retrieve the value from. This is best explained by
            an example: &lt;ul&gt; &lt;li&gt;If the value is
            &lt;code&gt;operator.Evaluator.value.absolute&lt;/code&gt;, the
            ProcessLogOperator looks up the operator with the name
            &lt;code&gt;Evaluator&lt;/code&gt;. If this operator is a
            &lt;i&gt;PerformanceEvaluator&lt;/i&gt;, it has a value named
            &lt;var&gt;absolute&lt;/var&gt; which gives the absolute error of the
            last evaluation. This value is queried by the
            ProcessLogOperator&lt;/li&gt; &lt;li&gt;If the value is
            &lt;code&gt;operator.SVMLearner.parameter.C&lt;/code&gt;, the
            ProcessLogOperator looks up the parameter &lt;var&gt;C&lt;/var&gt; of
            the operator named &lt;code&gt;SVMLearner&lt;/code&gt;.&lt;/li&gt;
            &lt;/ul&gt; Each time the ProcessLogOperator is applied, all the
            values and parameters specified by the list
            &lt;var&gt;log&lt;/var&gt; are collected and stored in a data row.
            When the process finishes, the operator writes the collected data
            rows to a file (if specified). In GUI mode, 2D or 3D plots are
            automatically generated and displayed in the result viewer.
            &lt;br/&gt; Please refer to section &lt;i&gt;Advanced
            Processes/Parameter and performance analysis&lt;/i&gt; for an example
            application.</help>
    <key>log</key>
    <tags>
         <tag>Record</tag>
         <tag>Measures</tag>
    </tags>
  </operator>
    <operator>
        <name>Loop Values (Deprecated)</name>
        <synopsis>Iterates over the values of the specified attributes and
            applies the inner operators on the input example set while the
            current value can be accessed via a macro. In contrast to the
            ValueSubgroupIterator operator the inner operators are applied on the
            complete example set.</synopsis>
        <help>&lt;p&gt; In each iteration step, this meta operator executes
            its inner process to the input example set. This will happen for each
            possible attribute value of the specified attributes if
            &lt;code&gt;all&lt;/code&gt; is selected for the
            &lt;code&gt;values&lt;/code&gt; parameter. If &lt;code&gt;above
            p&lt;/code&gt; is selected, an iteration is only performed for those
            values which exhibit an occurrence ratio of at least p. This may be
            helpful, if only large subgroups should be considered.&lt;/p&gt;
            &lt;p&gt;The current value of the loop can be accessed with the
            specified macro name.&lt;/p&gt;</help>
    <key>loop_values</key>
    <tags>
         <tag>Iterate</tag>
         <tag>Iteration</tag>
    </tags>
  </operator>
    <operator>
        <name>Numeric2Binominal</name>
        <synopsis>Maps all numeric values to 'false' if they are in the
            specified range (typical: equal 0.0) and to 'true' otherwise.
        </synopsis>
        <help>Converts all numerical attributes to binary ones. If the value
            of an attribute is between the specified minimal and maximal value,
            it becomes &lt;em&gt;false&lt;/em&gt;, otherwise
            &lt;em&gt;true&lt;/em&gt;. If the value is missing, the new value
            will be missing. The default boundaries are both set to 0, thus only
            0.0 is mapped to false and all other values are mapped to true.
        </help>
    </operator>
    <operator>
        <name>ForwardSelection</name>
        <synopsis>This operator realizes forward feature selection.</synopsis>
        <help/>
    </operator>
    <operator>
        <name>Stem (German)</name>
        <synopsis>A stemmer for German texts.</synopsis>
        <help>A fast german stemmer, based on the algorithm in the report "A
            Fast and Simple Stemming Algorithm for German Words" by Joerg
            Caumanns (joerg.caumanns@isst.fhg.de).</help>
    <key>stem_german</key>
  </operator>

    <operator>
        <name>Optimize Selection (Weight-Guided)</name>
        <synopsis>Adds iteratively features according to input attribute
            weights</synopsis>
        <help>&lt;p&gt; This operator uses input attribute weights to
            determine the order of features added to the feature set starting
            with the feature set containing only the feature with highest weight.
            The inner operators must provide a performance vector to determine
            the fitness of the current feature set, e.g. a cross validation of a
            learning scheme for a wrapper evaluation. Stops if adding the last
            &lt;code&gt;k&lt;/code&gt; features does not increase the performance
            or if all features were added. The value of
            &lt;code&gt;k&lt;/code&gt; can be set with the parameter
            &lt;code&gt;generations_without_improval&lt;/code&gt;. &lt;/p&gt;
        </help>
    <key>optimize_selection_weight_guided</key>
  </operator>

    <operator>
        <name>Select by Weights</name>
        <synopsis>Selects only attributes which weights fulfill a given
            relation with respect to the input attribute weights.</synopsis>
        <help>This operator selects all attributes which have a weight
            satisfying a given condition. For example, only attributes with a
            weight greater than &lt;code&gt;min_weight&lt;/code&gt; should be
            selected. This operator is also able to select the k attributes with
            the highest weight.</help>
    <key>select_by_weights</key>
    <tags>
         <tag>Weighting</tag>
         <tag>Importance</tag>
         <tag>Influence</tag>
         <tag>Significance</tag>
         <tag>Factors</tag>
         <tag>Relevance</tag>
         <tag>Thresholds</tag>
    </tags>
  </operator>
    
    <operator>
        <name>CSVWriter</name>
        <synopsis>This operator can write csv files.</synopsis>
        <help>&lt;p&gt;This operator can be used to write data into CSV files
            (Comma Separated Values). The values and columns are separated by
            &amp;quot;;&amp;quot;. Missing data values are indicated by empty
            cells.&lt;/p&gt;</help>
    </operator>
    <operator>
        <name>Set Minus</name>
        <synopsis>This operator returns these examples of an example set whose
            IDs are not contained within another example set.</synopsis>
        <help>This operator performs a set minus on two example sets, i.e. the
            resulting example set contains all the examples of the minuend
            example set whose IDs do not appear in the subtrahend example set.
            Please note, that the subtrahend example set must be the first on the
            ioobject stack, i.e. it must be the last example set which has been
            added. As compared to SQL, both example sets need have neither the
            same number of columns nor the same data types. The operation does
            only depend on the ID columns of the example sets.</help>
    <key>set_minus</key>
  </operator>
    <operator>
        <name>Retrieve</name>
        <synopsis>&lt;p&gt;Reads an object from the data repository.&lt;/p&gt;</synopsis>
        <help>&lt;p&gt;      This operator can be used to access the repositories introduced in       RapidMiner 5. It should replace all file access, since it provides full       meta data processing, which eases the usage of RapidMiner a lot. In       contrast to accessing a raw file, it will provide the complete meta data       of the data, so that all meta data transformations are possible.    &lt;/p&gt;    &lt;p&gt;      The single parameter &amp;quot;repository_entry&amp;quot; references an entry in the       repository which will be returned as the output of this operator.       Repository locations are resolved relative to the repository folder       containing the current process. Folders in the repository are separated       by a forward slash (/), a &amp;quot;..&amp;quot; references the parent folder. A leading       forward slash references the root folder of the repository containing       the current process. A leading double forward slash is interpreted as an       absolute path starting with the name of a repository.    &lt;/p&gt;    &lt;p&gt;      &lt;br&gt;          &lt;/p&gt;    &lt;ul&gt;      &lt;li&gt;        &amp;quot;MyData&amp;quot; looks up an entry &amp;quot;MyData&amp;quot; in the same folder as the current         process.      &lt;/li&gt;      &lt;li&gt;        &amp;quot;../input/MyData looks up an entry &amp;quot;MyData&amp;quot; located in a folder         &amp;quot;input&amp;quot; next to the folder containing the current process.      &lt;/li&gt;      &lt;li&gt;        &amp;quot;/data/Model&amp;quot; looks up an entry &amp;quot;Model&amp;quot; in a top-level folder &amp;quot;data&amp;quot;         in the repository holding the current process      &lt;/li&gt;      &lt;li&gt;        &amp;quot;//Samples/data/Iris&amp;quot; looks up the Iris data set in the &amp;quot;Samples&amp;quot;         repository.&lt;/p&gt;&lt;p&gt;&lt;br&gt;        &lt;p&gt;                  &lt;/p&gt;      &lt;/li&gt;    &lt;/ul&gt;</help>
    <key>retrieve</key>
    <tags>
         <tag>Load</tag>
         <tag>Import</tag>
         <tag>Read</tag>
         <tag>Datasets</tag>
         <tag>Examples</tag>
         <tag>Example Set</tag>
         <tag>Table</tag>
         <tag>Repository</tag>
    </tags>
  </operator>
    <operator>
        <name>Support Vector Machine</name>
        <synopsis>JMySVMLearner provides an internal Java implementation of
            the mySVM by Stefan Rueping.</synopsis>
        <help>This learner uses the Java implementation of the support vector
            machine &lt;em&gt;mySVM&lt;/em&gt; by Stefan R&amp;uuml;ping. This
            learning method can be used for both regression and classification
            and provides a fast algorithm and good results for many learning
            tasks.</help>
    <key>support_vector_machine</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Regression</tag>
         <tag>Model</tag>
         <tag>SVM</tag>
         <tag>Margin</tag>
    </tags>
    <shortName>SVM</shortName>
  </operator>
   <operator>
        <name>Support Vector Machine (Linear)</name>
        <synopsis>This operator provides a linear Support Vector Machine based upon the JMySVM.</synopsis>
        <help>This learner uses the Java implementation of the support vector
            machine &lt;em&gt;mySVM&lt;/em&gt; by Stefan R&amp;uuml;ping. It is restricted to
            the dot (linear) kernel, but outputs a high performance model that only contains the 
            linear coefficient for faster model application.</help>
    <key>support_vector_machine_linear</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Regression</tag>
         <tag>Model</tag>
         <tag>SVM</tag>
         <tag>Margin</tag>
    </tags>
    <shortName>SVM (Linear)</shortName>
  </operator>
    <operator>
        <name>Generate Copy</name>
        <synopsis>Copies a single attribute (only the view on the data column,
            not the data itself).</synopsis>
        <help>Adds a copy of a single attribute to the given example set.
        </help>
    <key>generate_copy</key>
    <tags>
         <tag>New</tag>
         <tag>Create</tag>
         <tag>Duplicate</tag>
    </tags>
  </operator>
    <operator>
        <name>Weight by Component Model</name>
        <synopsis>Creates the AttributeWeights of models containing components
            like PCA, GHA or FastICA.</synopsis>
        <help>For models creating components like
            &lt;code&gt;PCA&lt;/code&gt;, &lt;code&gt;GHA&lt;/code&gt; and
            &lt;code&gt;FastICA&lt;/code&gt; you can create the
            &lt;code&gt;AttributeWeights&lt;/code&gt; from a component.</help>
    <key>weight_by_component_model</key>
  </operator>
    <operator>
        <name>Select Attributes</name>
        <synopsis>&lt;p&gt;This operator allowes to select which attributes should be part of the     resulting &lt;i&gt;ExampleSet&lt;/i&gt;. Selection can be performed using several     conditions.&lt;/p&gt;</synopsis>
        <help>&lt;p&gt;      This operator selects which attributes of an &lt;i&gt;ExampleSet&lt;/i&gt; should be       kept and which are removed. Therefore, different filter types may be       selected in the parameter &lt;b&gt;attribute filter type&lt;/b&gt; and only       attributes fulfilling this condition type are selected. The rest will be       removed from the &lt;i&gt;ExampleSet.&lt;/i&gt; There's a global switch to invert       the outcome, so that all attributes which would have been originally       discarded will be kept and vice versa. To invert the decision, use the &lt;b&gt;invert       selection&lt;/b&gt; parameter.    &lt;/p&gt;    &lt;p&gt;          &lt;/p&gt;    &lt;p&gt;      These types are available    &lt;/p&gt;    &lt;ul&gt;      &lt;li&gt;        &lt;b&gt;all:&lt;/b&gt; Will simply select each attribute      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;single:&lt;/b&gt; This will allow you to select a single attribute name.         It might be selected from the drop down box of parameter &lt;b&gt;attribute&lt;/b&gt;         if the meta data is known      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;subset:&lt;/b&gt; Let's you choose a number of attributes from a list.         This will not work if no meta data is present. Each known attribute is         shown in the list and might be selected.      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;regular_expression:&lt;/b&gt; This let's you specify a regular         expression. Each attribute whose name matches this expression will be         selected. Regular expressions are a very powerful tool but need a         detailed explanation to beginners. Please refer to one of the several         tutorials available on the internet for a more detailed description.      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;value_type:&lt;/b&gt; Select only attributes of a certain type. Please         mention that the types are hierarchical: For example are binominal         attributes nomina as well as polynominal.      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;block_type:&lt;/b&gt; Similar to value_type this let's you select the         attributes depending on their block type.      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;no_missing_values: &lt;/b&gt;Will select all attributes which don't         contain a missing value in any example.      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;numeric_value_filter: &lt;/b&gt;This will select the attributes by         testing if all their values of all examples match this condition or if         they aren't not numerical. The numeric condition might be specified by         typing a numerical condition. For example the parameter string &amp;quot;&amp;gt; 6&amp;quot;         will keep all nominal attributes and all numeric attributes having a         value of greater 6 in every example. A combination of conditions is         possible: &amp;quot;&amp;gt; 6 &amp;amp;&amp;amp; &amp;lt; 11&amp;quot; or &amp;quot;&amp;lt;= 5 || &amp;lt; 0&amp;quot;. But &amp;amp;&amp;amp; and || must not be         mixed.      &lt;/li&gt;    &lt;/ul&gt;</help>
    <key>select_attributes</key>
    <tags>
         <tag>Filter</tag>
         <tag>Keep</tag>
         <tag>Remove</tag>
         <tag>Drop</tag>
         <tag>Delete</tag>
         <tag>Columns</tag>
         <tag>Variables</tag>
         <tag>Features</tag>
         <tag>Feature Set</tag>
    </tags>
  </operator>
      <operator>
        <name>Reorder Attributes</name>
        <synopsis></synopsis>
        <help></help>
    <key>order_attributes</key>
    <tags>
         <tag>Sort</tag>
         <tag>Arrange</tag>
         <tag>Ascending</tag>
         <tag>Descending</tag>
    </tags>
  </operator>
    <operator>
        <name>Copy File</name>
        <synopsis>Copies a file.</synopsis>
        <help>This operator copies the selected file to the chosen location.</help>
    <key>copy_file</key>
  </operator>
    <operator>
        <name>Move File</name>
        <synopsis>Moves a file.</synopsis>
        <help>This operator moves the selected file to the chosen location.</help>
    <key>move_file</key>
  </operator>
    <operator>
        <name>Delete File</name>
        <synopsis>Deletes a file.</synopsis>
        <help>This operator deletes the selected file.</help>
    <key>delete_file</key>
  </operator>
    <operator>
        <name>Create Directory</name>
        <synopsis>Creates a directory.</synopsis>
        <help>This operator creates a new directory a the chosen location.</help>
    <key>create_directory</key>
  </operator>
    <operator>
        <name>Rename File</name>
        <synopsis>Renames a file.</synopsis>
        <help>This operator entitles the selected file with the entered name.</help>
    <key>rename_file</key>
  </operator>
    <operator>
        <name>Quadratic Discriminant Analysis</name>
        <synopsis>A quadratic discriminant function for binominal labels and
            numerical attributes.</synopsis>
        <help>&lt;p&gt;This operator performs a quadratic discriminant
            analysis (QDA). QDA is closely related to linear discriminant
            analysis (LDA), where it is assumed that the measurements are
            normally distributed. Unlike LDA however, in QDA there is no
            assumption that the covariance of each of the classes is
            identical.&lt;/p&gt;</help>
    <key>quadratic_discriminant_analysis</key>
    <shortName>QDA</shortName>
  </operator>
    <operator>
        <name>Performance (Support Vector Count)</name>
        <synopsis>This operator created a performance vector containing the
            number of support vectors of the input kernel model.</synopsis>
        <help>Returns a performance vector just counting the number of support
            vectors of a given support vector based model (kernel model). Please
            note that this operator will try to derive the number of support
            vectors of the first delivered model and might fail on this task if
            no appropriate kernel based model is delivered. Currently, at least
            the models delivered by the operator JMySVM, MyKLR, LibSVM,
            GPLearner, KernelLogisticRegression, RVM, and the EvoSVM should be
            supported.</help>
    <key>performance_support_vector_count</key>
    <shortName>Performance</shortName>
  </operator>
  <operator>
 		 <name>Numerical to Date</name>
         <key>numerical_to_date</key>
  </operator>
  
    <operator>
        <name>Select Subprocess</name>
        <synopsis>This operator can be used to select a single inner operator
            which should be performed, e.g. by means of parameter iteration or
            optimization operators.</synopsis>
        <help>This operator can be used to employ a single inner operator or
            operator chain. Which operator should be used can be defined by the
            parameter &amp;quot;select_which&amp;quot;. Together with one of the
            parameter optimizing or iterating operators this operator can be used
            to dynamically change the process setup which might be useful in
            order to test different layouts, e.g. the gain by using different
            preprocessing steps or chains or the quality of a certain learner.
        </help>
    <key>select_subprocess</key>
  </operator>
    <operator>
        <name>GroupBy</name>
        <synopsis>Partitions an example set according to the values of a
            single nominal or integer attributes.</synopsis>
        <help>&lt;p&gt;This operator creates a SplittedExampleSet from an
            arbitrary example set. The partitions of the resulting example set
            are created according to the values of the specified attribute. This
            works similar to the &lt;code&gt;GROUP BY&lt;/code&gt; clause in
            SQL.&lt;/p&gt; &lt;p&gt;Please note that the resulting example set is
            simply a splitted example set where no subset is selected. Following
            operators might decide to select one or several of the subsets, e.g.
            one of the aggregation operators.&lt;/p&gt;</help>
    <key>groupby</key>
  </operator>
    <operator>
        <name>Remember</name>
        <synopsis>
    Stores an objects in the process' object store.
  </synopsis>
        <help>
    This operator can be used to store the input object into the process under 
    the specified name. It can later be restored by subsequent operators by 
    using the &lt;i&gt;Recall&lt;/i&gt; operator by referencing this name. There is no 
    scoping mechanism in RapidMiner processes. Objects can be retrieved from 
    operators in all nesting levels. The combination of those two operators 
    can be used to build complex processes where an input object is used in 
    completely different parts or loops of processes.
  </help>
    <key>remember</key>
    <tags>
         <tag>Store</tag>
         <tag>Memorize</tag>
         <tag>Recall</tag>
         <tag>Save</tag>
         <tag>Object</tag>
         <tag>Cache</tag>
    </tags>
  </operator>
  <operator>
        <name>Publish to App</name>
        <synopsis>
    Publishes objects in an app.
  </synopsis>
        <help>
    This operator can be used to publish the input object to the app under 
    the specified name. It can later be restored by subsequent operators by 
    using the &lt;i&gt;Recall from App&lt;/i&gt; operator by referencing this name. 
    The combination of those two operators 
    can be used to build complex apps where an input object is used multiple times in 
    completely different parts of the app.
  </help>
    <key>publish_to_app</key>
  </operator>
    <operator>
        <name>Date to Nominal</name>
        <synopsis>Parses the date values for the specified date attribute with
            respect to the given date format string and transforms the values
            into nominal values.</synopsis>
        <help>&lt;p&gt;This operator transforms the specified date attribute
            and writes a new nominal attribute in a user specified format. This
            might be useful for time base OLAP to change the granularity of the
            time stamps from day to week or month.&lt;/p&gt; &lt;p&gt;The date
            format can be specified by the date_format parameter like described
            in the following.&lt;/p&gt; &lt;h4&gt;Date and Time
            Patterns&lt;/h4&gt; &lt;p&gt; Date and time formats are specified by
            &lt;em&gt;date and time pattern&lt;/em&gt; strings in the date_format
            parameter. Within date and time pattern strings, unquoted letters
            from &lt;code&gt;'A'&lt;/code&gt; to &lt;code&gt;'Z'&lt;/code&gt; and
            from &lt;code&gt;'a'&lt;/code&gt; to &lt;code&gt;'z'&lt;/code&gt; are
            interpreted as pattern letters representing the components of a date
            or time string. Text can be quoted using single quotes
            (&lt;code&gt;'&lt;/code&gt;) to avoid interpretation.
            &lt;code&gt;"''"&lt;/code&gt; represents a single quote. All other
            characters are not interpreted; they're simply copied into the output
            string during formatting or matched against the input string during
            parsing.&lt;/p&gt; &lt;p&gt; The following pattern letters are
            defined (all other characters from &lt;code&gt;'A'&lt;/code&gt; to
            &lt;code&gt;'Z'&lt;/code&gt; and from &lt;code&gt;'a'&lt;/code&gt; to
            &lt;code&gt;'z'&lt;/code&gt; are reserved):&lt;/p&gt; &lt;ul&gt;
            &lt;li&gt;&lt;em&gt;G&lt;/em&gt;: era designator; Text; example:
            AD&lt;/li&gt; &lt;li&gt;&lt;em&gt;y&lt;/em&gt;: year; Year; example:
            1996; 96&lt;/li&gt; &lt;li&gt;&lt;em&gt;M&lt;/em&gt;: month in year;
            Month; example: July; Jul; 07&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;w&lt;/em&gt;: week in year; Number; example:
            27&lt;/li&gt; &lt;li&gt;&lt;em&gt;W&lt;/em&gt;: week in month;
            Number; example: 2&lt;/li&gt; &lt;li&gt;&lt;em&gt;D&lt;/em&gt;: day
            in year; Number; example: 189&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;d&lt;/em&gt;: day in month; Number; example:
            10&lt;/li&gt; &lt;li&gt;&lt;em&gt;F&lt;/em&gt;: day of week in month;
            Number; example: 2&lt;/li&gt; &lt;li&gt;&lt;em&gt;E&lt;/em&gt;: day
            in week; Text; example: Tuesday; Tue&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;a&lt;/em&gt;: am/pm marker; Text; example:
            PM&lt;/li&gt; &lt;li&gt;&lt;em&gt;H&lt;/em&gt;: hour in day (0-23);
            Number; example: 0&lt;/li&gt; &lt;li&gt;&lt;em&gt;k&lt;/em&gt;: hour
            in day (1-24); Number; example: 24&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;K&lt;/em&gt;: hour in am / pm (0-11); Number;
            example: 0&lt;/li&gt; &lt;li&gt;&lt;em&gt;h&lt;/em&gt;: hour in am /
            pm (1-12); Number; example: 12&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;m&lt;/em&gt;: minute in hour; Number; example:
            30&lt;/li&gt; &lt;li&gt;&lt;em&gt;s&lt;/em&gt;: second in minute;
            Number; example: 55&lt;/li&gt; &lt;li&gt;&lt;em&gt;S&lt;/em&gt;:
            millisecond; Number; example: 978&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;z&lt;/em&gt;: time zone; General Time Zone;
            example: Pacific Standard Time; PST; GMT-08:00&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;Z&lt;/em&gt;: time zone; RFC 822 Time Zone;
            example: -0800&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Pattern letters are
            usually repeated, as their number determines the exact
            presentation:&lt;/p&gt; &lt;ul&gt;
            &lt;li&gt;&lt;em&gt;Text:&lt;/em&gt; For formatting, if the number of
            pattern letters is 4 or more, the full form is used; otherwise a
            short or abbreviated form is used if available. For parsing, both
            forms are accepted, independent of the number of pattern
            letters.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Number:&lt;/em&gt; For
            formatting, the number of pattern letters is the minimum number of
            digits, and shorter numbers are zero-padded to this amount. For
            parsing, the number of pattern letters is ignored unless it's needed
            to separate two adjacent fields.&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;Year:&lt;/em&gt; If the underlying calendar is
            the Gregorian calendar, the following rules are applied. &lt;ul&gt;
            &lt;li&gt;For formatting, if the number of pattern letters is 2, the
            year is truncated to 2 digits; otherwise it is interpreted as a
            &lt;em&gt;number&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;For parsing, if the
            number of pattern letters is more than 2, the year is interpreted
            literally, regardless of the number of digits. So using the pattern
            "MM/dd/yyyy", "01/11/12" parses to Jan 11, 12 A.D.&lt;/li&gt;
            &lt;li&gt;For parsing with the abbreviated year pattern ("y" or
            "yy"), this operator must interpret the abbreviated year relative to
            some century. It does this by adjusting dates to be within 80 years
            before and 20 years after the time the operator is created. For
            example, using a pattern of "MM/dd/yy" and the operator created on
            Jan 1, 1997, the string &amp;quot;01/11/12&amp;quot; would be
            interpreted as Jan 11, 2012 while the string
            &amp;quot;05/04/64&amp;quot; would be interpreted as May 4, 1964.
            During parsing, only strings consisting of exactly two digits will be
            parsed into the default century. Any other numeric string, such as a
            one digit string, a three or more digit string, or a two digit string
            that isn't all digits (for example, &amp;quot;-1&amp;quot;), is
            interpreted literally. So &amp;quot;01/02/3&amp;quot; or
            &amp;quot;01/02/003&amp;quot; are parsed, using the same pattern, as
            Jan 2, 3 AD. Likewise, &amp;quot;01/02/-3&amp;quot; is parsed as Jan
            2, 4 BC.&lt;/li&gt; &lt;/ul&gt; Otherwise, calendar system specific
            forms are applied. If the number of pattern letters is 4 or more, a
            calendar specific long form is used. Otherwise, a calendar short or
            abbreviated form is used.&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;Month:&lt;/em&gt; If the number of pattern
            letters is 3 or more, the month is interpreted as
            &lt;em&gt;text&lt;/em&gt;; otherwise, it is interpreted as a
            &lt;em&gt;number&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;em&gt;General
            time zone:&lt;/em&gt; Time zones are interpreted as
            &lt;em&gt;text&lt;/em&gt; if they have names. It is possible to
            define time zones by representing a GMT offset value. RFC 822 time
            zones are also accepted.&lt;/li&gt; &lt;li&gt;&lt;em&gt;RFC 822 time
            zone:&lt;/em&gt; For formatting, the RFC 822 4-digit time zone format
            is used. General time zones are also accepted.&lt;/li&gt; &lt;/ul&gt;
            &lt;p&gt;This operator also supports &lt;em&gt;localized date and
            time pattern&lt;/em&gt; strings by defining the locale parameter. In
            these strings, the pattern letters described above may be replaced
            with other, locale dependent, pattern letters.&lt;/p&gt;
            &lt;h4&gt;Examples&lt;/h4&gt; &lt;p&gt;The following examples show
            how date and time patterns are interpreted in the U.S. locale. The
            given date and time are 2001-07-04 12:08:56 local time in the U.S.
            Pacific Time time zone.&lt;/p&gt; &lt;ul&gt;
            &lt;li&gt;&lt;em&gt;&amp;quot;yyyy.MM.dd G 'at' HH:mm:ss
            z&amp;quot;&lt;/em&gt;: 2001.07.04 AD at 12:08:56 PDT&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&amp;quot;EEE, MMM d, ''yy&amp;quot;&lt;/em&gt;:
            Wed, Jul 4, '01&lt;/li&gt; &lt;li&gt;&lt;em&gt;&amp;quot;h:mm
            a&amp;quot;&lt;/em&gt;: 12:08 PM&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&amp;quot;hh 'o''clock' a,
            zzzz&amp;quot;&lt;/em&gt;: 12 o'clock PM, Pacific Daylight
            Time&lt;/li&gt; &lt;li&gt;&lt;em&gt;&amp;quot;K:mm a,
            z&amp;quot;&lt;/em&gt;: 0:08 PM, PDT&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&amp;quot;yyyy.MMMMM.dd GGG hh:mm
            aaa&amp;quot;&lt;/em&gt;: 02001.July.04 AD 12:08 PM&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&amp;quot;EEE, d MMM yyyy HH:mm:ss
            Z&amp;quot;&lt;/em&gt;: Wed, 4 Jul 2001 12:08:56 -0700&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&amp;quot;yyMMddHHmmssZ&amp;quot;&lt;/em&gt;:
            010704120856-0700&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;&amp;quot;yyyy-MM-dd'T'HH:mm:ss.SSSZ&amp;quot;&lt;/em&gt;:
            2001-07-04T12:08:56.235-0700&lt;/li&gt; &lt;/ul&gt;</help>
    <key>date_to_nominal</key>
  </operator>
    <operator>
        <name>Process</name>
        <synopsis>
    The root operator which is the outer most operator of every process.
  </synopsis>
        <help>
    Each process must contain exactly one operator of this class, and it must 
    be the root operator of the process. This operator provides a set of 
    parameters that are of global relevance to the process like logging and 
    initialization parameters of the random number generator.
  </help>
    <key>process</key>
  </operator>
    <operator>
        <name>Optimize by Generation (GGA)</name>
        <synopsis>A genetic algorithm for feature selection and feature
            generation (GGA).</synopsis>
        <help>In contrast to the class &lt;i&gt;GeneticAlgorithm&lt;/i&gt;,
            the &lt;i&gt;GeneratingGeneticAlgorithm&lt;/i&gt; generates new
            attributes and thus can change the length of an individual. Therfore
            specialized mutation and crossover operators are being applied.
            Generators are chosen at random from a list of generators specified
            by boolean parameters. &lt;br/&gt; Since this operator does not
            contain algorithms to extract features from value series, it is
            restricted to example sets with only single attributes. For automatic
            feature extraction from values series the value series plugin for
            RapidMiner written by Ingo Mierswa should be used. It is available at
            &lt;a href="http://rapidminer.com"&gt;http://rapidminer.com&lt;/a&gt;
        </help>
    <key>optimize_by_generation_gga</key>
    <shortName>Generate</shortName>
  </operator>
    

    <operator>
        <name>Attributes2RealValues</name>
        <synopsis>Maps all values to real values.</synopsis>
        <help>This operator maps all non numeric attributes to real valued
            attributes. Nothing is done for numeric attributes, binary attributes
            are mapped to 0 and 1. For nominal attributes one of the following
            calculations will be done: &lt;ul&gt; &lt;li&gt;Dichotomization, i.e.
            one new attribute for each value of the nominal attribute. The new
            attribute which corresponds to the actual nominal value gets value 1
            and all other attributes gets value 0.&lt;/li&gt;
            &lt;li&gt;Alternatively the values of nominal attributes can be seen
            as equally ranked, therefore the nominal attribute will simply be
            turned into a real valued attribute, the old values results in
            equidistant real values.&lt;/li&gt; &lt;/ul&gt; At this moment the
            same applies for ordinal attributes, in a future release more
            appropriate values based on the ranking between the ordinal values
            may be included.</help>
    </operator>
    
    <operator>
        <name>Detect Outlier (Distances)</name>
        <synopsis>Identifies n outliers in the given ExampleSet based on the
            distance to their k nearest neighbors.</synopsis>
        <help>&lt;p&gt;This operator performs a D^k_n Outlier Search according
            to the outlier detection approach recommended by Ramaswamy, Rastogi
            and Shim in "Efficient Algorithms for Mining Outliers from Large Data
            Sets". It is primarily a statistical outlier search based on a
            distance measure similar to the DB(p,D)-Outlier Search from Knorr and
            Ng. But it utilizes a distance search through the k-th nearest
            neighbourhood, so it implements some sort of locality as
            well.&lt;/p&gt; &lt;p&gt;The method states, that those objects with
            the largest distance to their k-th nearest neighbours are likely to
            be outliers respective to the data set, because it can be assumed,
            that those objects have a more sparse neighbourhood than the average
            objects. As this effectively provides a simple ranking over all the
            objects in the data set according to the distance to their k-th
            nearest neighbours, the user can specify a number of n objects to be
            the top-n outliers in the data set.&lt;/p&gt; &lt;p&gt;The operator
            supports cosine, sine or squared distances in addition to the
            euclidian distance which can be specified by a distance parameter.
            The Operator takes an example set and passes it on with an boolean
            top-n D^k outlier status in a new boolean-valued special outlier
            attribute indicating true (outlier) and false (no outlier).&lt;/p&gt;
        </help>
    <key>detect_outlier_distances</key>
    <tags>
         <tag>Anomaly</tag>
         <tag>Anomalies</tag>
         <tag>Detection</tag>
         <tag>Removal</tag>
         <tag>Remove</tag>
         <tag>Cleansing</tag>
         <tag>Quality</tag>
    </tags>
  </operator>

    <operator>
        <name>Extract Performance</name>
        <synopsis>This operator can be used to directly derive a performance
            measure from a specific data or statistics value.</synopsis>
        <help>This operator can be used to derive a specific value of a given
            example set and provide it as a performance value which can be used
            for optimization purposes.</help>
    <key>extract_performance</key>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>Weight by Information Gain Ratio</name>
        <synopsis>This operator calculates the relevance of the attributes
            based on the information gain ratio.</synopsis>
        <help>This operator calculates the relevance of a feature by computing
            the information gain ratio for the class distribution (if exampleSet
            would have been splitted according to each of the given features).
        </help>
    <key>weight_by_information_gain_ratio</key>
    <tags>
         <tag>Weighting</tag>
         <tag>Importance</tag>
         <tag>Influence</tag>
         <tag>Significance</tag>
         <tag>Factors</tag>
         <tag>Relevance</tag>
    </tags>
  </operator>
    <operator>
        <name>Loop Labels</name>
        <synopsis>Performs its inner operators for each label found in input
            example set.</synopsis>
        <help>Performs the inner operator for all label attributes, i.e.
            special attributes whose role name starts with
            &amp;quot;label&amp;quot;. In each iteration one of the multiple
            labels is used as label. The results of the inner operators are
            collected and returned. The example set will be consumed during the
            iteration.</help>
    <key>loop_labels</key>
  </operator>

    <operator>
        <name>Grouped ANOVA</name>
        <synopsis>Performs an ANOVA significance test for a single numerical
            attribute based on the groups defined by another (nominal) attribute.
        </synopsis>
        <help>&lt;p&gt; This operator creates groups of the input example set
            based on the defined grouping attribute. For each of the groups the
            mean and variance of another attribute (the anova attribute) is
            calculated and an ANalysis Of VAriance (ANOVA) is performed. The
            result will be a significance test result for the specified
            significance level indicating if the values for the attribute are
            significantly different between the groups defined by the grouping
            attribute. &lt;/p&gt;</help>
    <key>grouped_anova</key>
  </operator>

    <operator>
        <name>Branch</name>
        <synopsis>
    This operator provides a conditional execution of subprocesses.
  </synopsis>
        <help>
    &lt;p&gt;
      This operator executes one of its two subprocesses based on a condition. 
      The first subprocess is executed if the specified condition is true, the 
      second one is executed if it is false (if-then-else). 
    &lt;/p&gt;
    &lt;p&gt;
      If the condition &amp;quot;attribute_value_filter&amp;quot; is used, the same attribute 
      value conditions already known from the &lt;i&gt;ExampleFilter&lt;/i&gt; operator 
      can be used. In addition to the known attribute value relation format 
      (e.g. &amp;quot;att1&amp;gt;=0.7&amp;quot;), this operator expects an additional definition for 
      the used example which cam be added in &amp;quot;[&amp;quot; and &amp;quot;]&amp;quot; after the attribute 
      value condition. The following values are possible:
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        a fixed number, e.g. &amp;quot;att1&amp;gt;0.7 [7]&amp;quot; meaning that the value for 
        attribute &amp;quot;att1&amp;quot; for the example 7 must be greater than 0.7
      &lt;/li&gt;
      &lt;li&gt;
        the wildcard &amp;quot;*&amp;quot; meaning that the attribute value condition must be 
        fulfilled for all examples, e.g. &amp;quot;att4&amp;lt;=5 [*]&amp;quot;
      &lt;/li&gt;
      &lt;li&gt;
        no example definition, meaning the same as the wildcard definition [*]
      &lt;/li&gt;
    &lt;/ul&gt;
  </help>
    <key>branch</key>
    <tags>
         <tag>If</tag>
         <tag>Then</tag>
         <tag>Else</tag>
         <tag>Conditional</tag>
         <tag>Conditions</tag>
         <tag>Fork</tag>
         <tag>If-then-else</tag>
         <tag>Case</tag>
    </tags>
  </operator>
    <operator>
        <name>Tokenize</name>
        <synopsis>Tokenizes a set of input tokens.</synopsis>
        <help>This class tokenizes all tokens in the input. All characters
            that are not defined as letters in the Java Characters class, are
            used as separators.</help>
    <key>tokenize</key>
  </operator>
    <operator>
        <name>Detect Outlier (LOF)</name>
        <synopsis>Identifies outliers in the given ExampleSet based on local
            outlier factors.</synopsis>
        <help>&lt;p&gt;This operator performs a LOF outlier search. LOF
            outliers or outliers with a local outlier factor per object are
            density based outliers according to Breuning, Kriegel, et
            al.&lt;/p&gt; &lt;p&gt;The approach to find those outliers is based
            on measuring the density of objects and its relation to each other
            (referred to as local reachability density). Based on the average
            ratio of the local reachability density of an object and its
            k-nearest neighbours (e.g. the objects in its k-distance
            neighbourhood), a local outlier factor (LOF) is computed. The
            approach takes a parameter MinPts (actually specifying the "k") and
            it uses the maximum LOFs for objects in a MinPts range (lower bound
            and upper bound to MinPts).&lt;/p&gt; &lt;p&gt;Currently, the
            operator supports cosine, sine or squared distances in addition to
            the usual euclidian distance which can be specified by the
            corresponding parameter. In the first step, the objects are grouped
            into containers. For each object, using a radius screening of all
            other objects, all the available distances between that object and
            another object (or group of objects) on the (same) radius given by
            the distance are associated with a container. That container than has
            the distance information as well as the list of objects within that
            distance (usually only a few) and the information, how many objects
            are in the container.&lt;/p&gt; &lt;p&gt;In the second step, three
            things are done: (1) The containers for each object are counted in
            acending order according to the cardinality of the object list within
            the container (= that distance) to find the k-distances for each
            object and the objects in that k-distance (all objects in all the
            subsequent containers with a smaller distance). (2) Using this
            information, the local reachability densities are computed by using
            the maximum of the actual distance and the k-distance for each object
            pair (object and objects in k-distance) and averaging it by the
            cardinality of the k-neighbourhood and than taking the reciprocal
            value. (3) The LOF is computed for each MinPts value in the range
            (actually for all up to upper bound) by averaging the ratio between
            the MinPts-local reachability-density of all objects in the
            k-neighbourhood and the object itself. The maximum LOF in the MinPts
            range is passed as final LOF to each object.&lt;/p&gt;
            &lt;p&gt;Afterwards LOFs are added as values for a special
            real-valued outlier attribute in the example set which the operator
            will return.&lt;/p&gt;</help>
    <key>detect_outlier_lof</key>
  </operator>
    <operator>
        <name>Split File by Content</name>
        <synopsis>Segments documents based on regular expressions or xpath.
        </synopsis>
        <help>Operator that allows to extract segments from a set of text
            documents in a directory based on regular expressions, XPath or
            simple string matching. This operator does support several formats as
            XML, HTML, Text and PDF, although XPath will work on XML and HTML
            documents only. The written files will be of the same ending as the
            input files type if possible. PDF for example will always be
            transformed into text files.</help>
    <key>split_file_by_content</key>
  </operator>
    <operator>
        <name>Cartesian Product</name>
        <synopsis>Build the cartesian product of two example sets. In contrast
            to the ExampleSetJoin operator Id attributes are not needes.
        </synopsis>
        <help>&lt;p&gt;Build the cartesian product of two example sets. In
            contrast to the &lt;i&gt;ExampleSetJoin&lt;/i&gt; operator, this
            operator does not depend on Id attributes. The result example set
            will consist of the union set or the union list (depending on
            parameter setting double attributes will be removed or renamed) of
            both feature sets. In case of removing double attribute the attribute
            values must be the same for the examples of both example set,
            otherwise an exception will be thrown.&lt;/p&gt; &lt;p&gt;Please note
            that this check for double attributes will only be applied for
            regular attributes. Special attributes of the second input example
            set which do not exist in the first example set will simply be added.
            If they already exist they are simply skipped.&lt;/p&gt;</help>
    <key>cartesian_product</key>
    <shortName>Cartesian</shortName>
  </operator>
    
    <operator>
        <name>NeuralNet</name>
        <synopsis>Learns a neural net from the input data.</synopsis>
        <help>&lt;p&gt;This operator learns a model by means of a feed-forward
            neural network. The learning is done via backpropagation. The user
            can define the structure of the neural network with the parameter
            list &amp;quot;hidden_layer_types&amp;quot;. Each list entry
            describes a new hidden layer. The key of each entry must correspond
            to the layer type which must be one out of&lt;/p&gt; &lt;ul&gt;
            &lt;li&gt;linear&lt;/li&gt; &lt;li&gt;sigmoid (default)&lt;/li&gt;
            &lt;li&gt;tanh&lt;/li&gt; &lt;li&gt;sine&lt;/li&gt;
            &lt;li&gt;logarithmic&lt;/li&gt; &lt;li&gt;gaussian&lt;/li&gt;
            &lt;/ul&gt; &lt;p&gt;The key of each entry must be a number defining
            the size of the hidden layer. A size value of -1 or 0 indicates that
            the layer size should be calculated from the number of attributes of
            the input example set. In this case, the layer size will be set to
            (number of attributes + number of classes) / 2 + 1.&lt;/p&gt;
            &lt;p&gt;If the user does not specify any hidden layers, a default
            hidden layer with sigmoid type and size (number of attributes +
            number of classes) / 2 + 1 will be created and added to the
            net.&lt;/p&gt; &lt;p&gt;The type of the input nodes is sigmoid. The
            type of the output node is sigmoid is the learning data describes a
            classification task and linear for numerical regression
            tasks.&lt;/p&gt;</help>
    <key>neuralnet</key>
  </operator>
    <operator>
        <name>PerformanceEvaluator</name>
        <synopsis>A performance evaluator delivers as output a list of
            performance values according to a list of performance criteria.
        </synopsis>
        <help>&lt;p&gt;A performance evaluator is an operator that expects a
            test &lt;i&gt;ExampleSet&lt;/i&gt; as input, whose elements have both
            true and predicted labels, and delivers as output a list of
            performance values according to a list of performance criteria that
            it calculates. If an input performance vector was already given, this
            is used for keeping the performance values.&lt;/p&gt; &lt;p&gt;All of
            the performance criteria can be switched on using boolean parameters.
            Their values can be queried by a ProcessLogOperator using the same
            names. The main criterion is used for comparisons and need to be
            specified only for processes where performance vectors are compared,
            e.g. feature selection processes. If no other main criterion was
            selected the first criterion in the resulting performance vector will
            be assumed to be the main criterion.&lt;/p&gt; &lt;p&gt;The resulting
            performance vectors are usually compared with a standard performance
            comparator which only compares the fitness values of the main
            criterion. Other implementations than this simple comparator can be
            specified using the parameter
            &lt;var&gt;comparator_class&lt;/var&gt;. This may for instance be
            useful if you want to compare performance vectors according to the
            weighted sum of the individual criteria. In order to implement your
            own comparator, simply subclass
            &lt;i&gt;PerformanceComparator&lt;/i&gt;. Please note that for true
            multi-objective optimization usually another selection scheme is used
            instead of simply replacing the performance comparator.&lt;/p&gt;
            &lt;p&gt;Additional user-defined implementations of
            &lt;i&gt;PerformanceCriterion&lt;/i&gt; can be specified by using the
            parameter list
            &lt;var&gt;additional_performance_criteria&lt;/var&gt;. Each
            key/value pair in this list must specify a fully qualified classname
            (as the key), and a string parameter (as value) that is passed to the
            constructor. Please make sure that the class files are in the
            classpath (this is the case if the implementations are supplied by a
            plugin) and that they implement a one-argument constructor taking a
            string parameter. It must also be ensured that these classes extend
            &lt;i&gt;MeasuredPerformance&lt;/i&gt; since the PerformanceEvaluator
            operator will only support these criteria. Please note that only the
            first three user defined criteria can be used as logging value with
            names &amp;quot;user1&amp;quot;, ... ,
            &amp;quot;user3&amp;quot;.&lt;/p&gt;</help>
    <key>performanceevaluator</key>
  </operator>
    <operator>
        <name>Store</name>
        <synopsis>Stores an IOObject in the data repository.</synopsis>
        <help>This operator stores IOObjects at a location in a repository.
        </help>
    <key>store</key>
    <tags>
         <tag>Save</tag>
         <tag>Export</tag>
         <tag>Write</tag>
         <tag>Datasets</tag>
         <tag>Repository</tag>
    </tags>
  </operator>
    <operator>
        <name>Weight by Gini Index</name>
        <synopsis>This operator calculates the relevance of the attributes
            based on the Gini impurity index.</synopsis>
        <help>This operator calculates the relevance of an attribute by computing
            the Gini index of the class distribution, if the given example set
            would have been splitted according to the feature.</help>
    <key>weight_by_gini_index</key>
    <tags>
         <tag>Weighting</tag>
         <tag>Importance</tag>
         <tag>Influence</tag>
         <tag>Significance</tag>
         <tag>Factors</tag>
         <tag>Relevance</tag>
    </tags>
  </operator>
    <operator>
        <name>Weight by Tree Importance</name>
        <synopsis>This operator calculates the importance of the attributes
            by analysing the split points of a Random Forest model.</synopsis>
        <help>
            &lt;p&gt;This weighting schema will use a given random forest to extract the implicit
 importance of the used attributes. Therefore each node of each tree is visited and the benefit created
 by the respective split is retrieved. This benefit is summed per attribute, that had been used for the split.
 The mean benefit over all trees is used as importance.&lt;/p&gt;
 &lt;p&gt;
 This algorithm is implemented following the idea from "A comparison of random forest and its Gini importance with standard chemometric methods for the feature selection and classification of spectral data"
 by Menze, Bjoen H et all (2009). It has been extended by additional criterias for computing the benefit created from a 
 certain split. The original paper only mentioned Gini Index, this operator additionally supports the more reliable criterions Information Gain
  and Information Gain Ratio.
 &lt;/p&gt;
 
 </help>
    <key>weight_by_forest</key>
  </operator>

    <operator>
        <name>SplitChain</name>
        <synopsis>Splits an example set in two parts based on a user defined
            ratio and uses the output of the first child and the second part as
            input for the second child.</synopsis>
        <help>&lt;p&gt;An operator chain that split an
            &lt;i&gt;ExampleSet&lt;/i&gt; into two disjunct parts and applies the
            first child operator on the first part and applies the second child
            on the second part and the result of the first child. The total
            result is the result of the second operator.&lt;/p&gt; &lt;p&gt;The
            input example set will be splitted based on a defined ratio between 0
            and 1.&lt;/p&gt;</help>
    <key>splitchain</key>
  </operator>
    <operator>
        <name>Adjust Date</name>
        <synopsis>Adjusts the date in a specified column by adding or
            subtracting the specified amount of time.</synopsis>
        <help>This operator allows to adjust a date attribute by adding a constant value
            in arbitrary units as days, hours or seconds to the attributes value.</help>
    <key>adjust_date</key>
  </operator>
    <operator>
        <name>Generate Up-Selling Data</name>
        <synopsis>Generates data for testing purposes based on an up-selling
            data set.</synopsis>
        <help>Generates a random example set for testing purposes. The data
            represents an up-selling example set.</help>
    <key>generate_up_selling_data</key>
  </operator>
    <operator>
        <name>ErrorNeglector</name>
        <synopsis>This operator performs the inner operator and neglects any
            errors. In this case, no inner output will be returned.</synopsis>
        <help>&lt;p&gt;This operator performs the inner operators and delivers
            the result of the inner operators. If any error occurs during this
            subprocess, this error will be neglected and this operator simply
            will return no additional input.&lt;/p&gt; &lt;p&gt;Please use this
            operator with care since it will also cover errors which are not
            expected by the analyst. In combination with a process branch,
            however, it can be used to handle exceptions in the analysis process
            (i.e. expected errors). &lt;/p&gt;</help>
    </operator>
    <operator>
        <name>Date to Numerical</name>
        <synopsis>Transforms date values into numerical ones capturing the
            milliseconds since 01/01/1970 00:00:00 GMT.</synopsis>
        <help>This operator changes a date attribute into a numerical one. It
            allows to specify exactly which entity should be extracted and to
            which unit or date it should relate. As an example, it is possible to
            extract seconds within a minute. Analogously, it is also possible to
            extract the day within a month. But it is also possible to extract
            the day within a week or within a year. For all time units, it is
            also possible to extract the number which has passed by since
            1970-01-01 00:00.</help>
    <key>date_to_numerical</key>
    <tags>
         <tag>Dates</tag>
         <tag>Times</tag>
         <tag>Datetimes</tag>
         <tag>Continous</tag>
         <tag>Numbers</tag>
         <tag>Weeks</tag>
         <tag>Days</tag>
         <tag>Months</tag>
         <tag>Years</tag>
         <tag>Quarters</tag>
         <tag>Hours</tag>
         <tag>Minutes</tag>
         <tag>Seconds</tag>
         <tag>Types</tag>
    </tags>
  </operator>
    <operator>
        <name>Generate Team Profit Data</name>
        <synopsis>Generates data for testing purposes based on a team profit
            data set.</synopsis>
        <help>Generates a random example set for testing purposes. The data
            represents a team profit example set.</help>
    <key>generate_team_profit_data</key>
  </operator>
    <operator>
        <name>Decision Stump</name>
        <synopsis>Learns only a root node of a decision tree. Can be very
            efficient when boosted.</synopsis>
        <help>This operator learns decision stumps, i.e. a small decision tree
            with only one single split. This decision stump works on both
            numerical and nominal attributes.</help>
    <key>decision_stump</key>
  </operator>
    <operator>
        <name>Rename by Generic Names</name>
        <synopsis>This operator can be used to rename all attributes of the
            input example set to a set of generic names like att1, att2, att3
            etc.</synopsis>
        <help>&lt;p&gt;This operator replaces the attribute names of the input
            example set by generic names like att1, att2, att3 etc.&lt;/p&gt;
        </help>
    <key>rename_by_generic_names</key>
  </operator>
    <operator>
        <name>Stem (Porter)</name>
        <synopsis>The Porter stemmer for English texts.</synopsis>
        <help>The porter stemmer for English texts.</help>
    <key>stem_porter</key>
  </operator>
    <operator>
        <name>Performance (Classification)</name>
        <synopsis>This operator delivers as a &lt;i&gt;PerformanceVector&lt;/i&gt; containing performance
            values according to a list of selected performance criteria applicable for multi-class
         classification tasks.</synopsis>
        <help>&lt;p&gt;This performance evaluator operator should be used for
            classification tasks, i.e. in cases where the label attribute has a
            (poly-)nominal value type. 
            
            &lt;p&gt;&lt;br/&gt;This operator expects a test &lt;i&gt;ExampleSet&lt;/i&gt; as
                        input, containing one attribute with the role &lt;i&gt;label&lt;/i&gt; and one with the role &lt;i&gt;prediction&lt;/i&gt;. See the &lt;a href="rm://opdoc/set_role"&gt;Set Role&lt;/a&gt; operator for more details. 
                        On the basis of this two attributes a &lt;i&gt;PerformanceVector&lt;/i&gt; is calculated, containing the values of the performance criteria. If a &lt;i&gt;PerformanceVector&lt;/i&gt; was fed into &lt;i&gt;performance&lt;/i&gt; input, it's values are kept if it does not already contain the new criteria. Otherwise the values are averaged over the old and the new values.
                        &lt;/p&gt;
                        
                        &lt;p&gt;All of
            the performance criteria can be switched on using boolean parameters.
            Their values can be queried by a &lt;a href="rm://opdoc/log"&gt;Log&lt;/a&gt; operator using the same
            names. The main criterion is used for comparisons and need to be
            specified only for processes where performance vectors are compared,
            e.g. attribute selection or other meta optimization process setups. If
            no main criterion was selected, the first criterion in the
            resulting performance vector will be assumed to be the main
            criterion.&lt;/p&gt; 

        </help>
    <key>performance_classification</key>
    <tags>
         <tag>Accuracy</tag>
         <tag>Errors</tag>
         <tag>Precision</tag>
         <tag>Recall</tag>
         <tag>Kappa</tag>
         <tag>Squared</tag>
         <tag>Relative</tag>
         <tag>Validations</tag>
         <tag>Evaluations</tag>
         <tag>Metrics</tag>
    </tags>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>Sample</name>
        <synopsis>Creates a sample from an example set by drawing a fraction.
        </synopsis>
        <help>Simple sampling operator. This operator performs a random
            sampling of a given fraction. For example, if the input example set
            contains 5000 examples and the sample ratio is set to 0.1, the result
            will have approximately 500 examples.</help>
    <key>sample</key>
    <tags>
         <tag>Subsets</tag>
         <tag>Random</tag>
         <tag>Ratio</tag>
         <tag>Stratified</tag>
         <tag>Stratification</tag>
         <tag>Bootstrap</tag>
         <tag>Population</tag>
         <tag>Downsample</tag>
    </tags>
  </operator>
    <operator>
        <name>Rename by Example Values</name>
        <synopsis>Uses the specified row as new attibute names and deletes the
            row from the data set.</synopsis>
        <help>&lt;p&gt;This operators uses the values of the specified row of
            the data set as new attribute names (including both regular and
            special columns). This might be useful for example after a transpose
            operation. The row will be deleted from the data set. Please note,
            however, that an internally used nominal mapping will not be removed
            and following operators like
            &lt;i&gt;NominalNumbers2Numerical&lt;/i&gt; could possibly not work
            as expected. In order to correct the value types and nominal value
            mappings, one could use the operator
            &lt;i&gt;GuessValueTypes&lt;/i&gt; after this operator.&lt;/p&gt;
        </help>
    <key>rename_by_example_values</key>
  </operator>
    <operator>
        <name>Random Clustering</name>
        <synopsis>Flat random clustering</synopsis>
        <help>Returns a random clustering. Note that this algorithm does not
            garantuee that all clusters are non-empty. This operator will create
            a cluster attribute if not present yet.</help>
    <key>random_clustering</key>
    <shortName>Clustering</shortName>
  </operator>
    <operator>
        <name>Replace</name>
        <synopsis>This operator replaces parts of the values of nominal attributes.</synopsis>
        <help>
        &lt;p&gt;This operator replaces parts of the string values of all nominal attributes it is applied on. The &lt;b&gt;attribute filter type&lt;/b&gt; gives the possibility to restrict them. 
           For each value of each attribute it is checked if the regular expression of &lt;b&gt;replace what&lt;/b&gt; matches the string. Each matching part of the string will be replaced by the value of the &lt;b&gt;replace_what&lt;/b&gt; parameter.
           The replacement might be empty and can contain capturing groups.&lt;br/&gt;
            Please keep in mind that although regular expressions are much more powerful than simple strings, you might simply enter characters to search for.&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
            &lt;h4&gt;Examples&lt;/h4&gt;
          The attribute contains the values "color red", "color green" and "color blue".
          &lt;ul&gt;
            &lt;li&gt;replacing "color" by "" yields: " red", " green", " blue"&lt;/li&gt;
            &lt;li&gt;replacing "color" by "colour" yields: "colour red", "colour green", "colour blue"&lt;/li&gt;
            &lt;li&gt;replacing "color\s" by "" yields: "red", "green", "blue"&lt;/li&gt;
            &lt;li&gt;replacing "\s+" by "_" yields: "color_red", "color_green", "color_blue"&lt;/li&gt;
            &lt;li&gt;replacing "color\s(.*)" by "$1" yields: "red", "green", "blue"&lt;/li&gt;
            &lt;li&gt;replacing ".*\s(.*)" by "$1" yields: "red", "green", "blue"&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/p&gt;
        </help>
    <key>replace</key>
    <tags>
         <tag>Map</tag>
         <tag>Change</tag>
         <tag>Regex</tag>
         <tag>Regular expressions</tag>
    </tags>
  </operator>
    <operator>
        <name>Rainflow Matrix</name>
        <synopsis>Determines the rainflow matrix for a specified attribute.
        </synopsis>
        <help>&lt;p&gt;This operator calculates the rainflow matrix for a
            series attribute. Please note that the attribute does have to be
            discretized before. Since this operator relies on the fact that the
            names of the bins follow the same natural order than the values they
            represent, we recommend to use the option &amp;quot;interval
            names&amp;quot; for the discretized bins.&lt;/p&gt;</help>
    <key>rainflow_matrix</key>
  </operator>
    
   <operator>
        <name>Weight by SVM</name>
        <synopsis>This operator uses the coefficients of a hyperplance
            calculated by an SVM as feature weights.</synopsis>
        <help>Uses the coefficients of the normal vector of a linear SVM as
            feature weights. In contrast to most of the SVM based operators
            available in RapidMiner, this one works for multiple classes, too.
            Please note that the attribute values, however, still have to be
            numerical. Please use appropriate preprocessing operators in order to
            ensure this.</help>
    <key>weight_by_svm</key>
    <tags>
         <tag>Weighting</tag>
         <tag>Importance</tag>
         <tag>Influence</tag>
         <tag>Significance</tag>
         <tag>Factors</tag>
         <tag>Relevance</tag>
         <tag>Support vector machine</tag>
    </tags>
  </operator>
    <operator>
        <name>Support Vector Clustering</name>
        <synopsis>Clustering with support vectors</synopsis>
        <help>An implementation of Support Vector Clustering based on
            {@rapidminer.cite BenHur/etal/2001a}. This operator will create a
            cluster attribute if not present yet.</help>
    <key>support_vector_clustering</key>
    <shortName>Clustering</shortName>
  </operator>
    <operator>
        <name>ID3</name>
        <synopsis>Learns an unpruned decision tree from nominal attributes
            only.</synopsis>
        <help>This operator learns decision trees without pruning using
            nominal attributes only. Decision trees are powerful classification
            methods which often can also easily be understood. This decision tree
            learner works similar to Quinlan's ID3.</help>
    <key>id3</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>J48</tag>
         <tag>J4.8</tag>
         <tag>C45</tag>
         <tag>C4.5</tag>
         <tag>C50</tag>
         <tag>C5.0</tag>
         <tag>Cart</tag>
         <tag>Chaid</tag>
         <tag>Decisiontree</tag>
         <tag>Decision tree</tag>
    </tags>
  </operator>
    <operator>
        <name>ANOVA Matrix</name>
        <synopsis>Performs an ANOVA significance test for a all numerical
            attribute based on the groups defined by all other nominal
            attributes.</synopsis>
        <help>&lt;p&gt;This operator calculates the significance of difference
            for the values for all numerical attributes depending on the groups
            defined by all nominal attributes. Please refer to the operator
            &lt;i&gt;GroupedANOVAOperator&lt;/i&gt; for details of the
            calculation.&lt;/p&gt;</help>
    <key>anova_matrix</key>
  </operator>
    <operator>
        <name>Weight by Value Average</name>
        <synopsis>This operator uses a corpus of examples to characterize a
            single class by setting feature weights.</synopsis>
        <help>This operator uses a corpus of examples to characterize a single
            class by setting feature weights. Characteristic features receive
            higher weights than less characteristic features. The weight for a
            feature is determined by calculating the average value of this
            feature for all examples of the target class. This operator assumes
            that the feature values characterize the importance of this feature
            for an example (e.g. TFIDF or others). Therefore, this operator is
            mainly used on textual data based on TFIDF weighting schemes. To
            extract such feature values from text collections you can use the
            Text plugin.</help>
    <key>weight_by_value_average</key>
  </operator>
    <operator>
        <name>Log to Data</name>
        <synopsis>Transforms the data generated by the ProcessLog operator
            into an example set which can be used by other operators.</synopsis>
        <help>This operator transforms the data generated by a ProcessLog
            operator into an ExampleSet which can then be used by other
            operators.</help>
    <key>log_to_data</key>
    <tags>
         <tag>Record</tag>
         <tag>Measures</tag>
    </tags>
  </operator>
    <operator>
        <name>Numeric2Binary</name>
        <synopsis>Maps all numeric values to 'false' if they are in the
            specified range (typical: equal 0.0) and to 'true' otherwise.
        </synopsis>
        <help>Converts all numerical attributes to binary ones. If the value
            of an attribute is between the specified minimal and maximal value,
            it becomes &lt;em&gt;false&lt;/em&gt;, otherwise
            &lt;em&gt;true&lt;/em&gt;. If the value is missing, the new value
            will be missing. The default boundaries are both set to 0, thus only
            0.0 is mapped to false and all other values are mapped to true.
        </help>
    </operator>
    <operator>
        <name>Intersect</name>
        <synopsis>This operator returns these examples of an example set whose
            IDs are contained within another example set.</synopsis>
        <help>This operator performs a set intersection on two example sets,
            i.e., the resulting example set contains all the examples of the
            first example set whose IDs appear also in the second example set. As
            compared to SQL, both example sets neither need to have neither the
            same number of columns nor the same data types. The operation does
            only depend on the ID columns of the example sets.</help>
    <key>intersect</key>
  </operator>
    <operator>
        <name>Transition Matrix</name>
        <synopsis>Determines the transition probabilities of nominal values.
        </synopsis>
        <help>This operator calculates the transition matrix of a specified
            attribute, i.e. the operator counts how often each possible nominal
            value follows after each other.</help>
    <key>transition_matrix</key>
  </operator>
    <operator>
        <name>Principal Component Analysis (Kernel)</name>
        <synopsis>Performs a kernel principal component analysis (PCA).
        </synopsis>
        <help>This operator performs a kernel-based principal components
            analysis (PCA). Hence, the result will be the set of data points in a
            non-linearly transformed space. Please note that in contrast to the
            usual linear PCA the kernel variant does also works for large numbers
            of attributes but will become slow for large number of examples.
        </help>
    <key>principal_component_analysis_kernel</key>
    <shortName>PCA (Kernel)</shortName>
  </operator>
    <operator>
        <name>Materialize Data</name>
        <synopsis>Creates a fresh and clean copy of the data. Might be useful
            after large preprocessing chains with a lot of views or even data
            copies, especially in combination with a memory clean up operation
            followed afterwards.</synopsis>
        <help>Creates a fresh and clean copy of the data in memory. Might be
            very useful in combination with the &lt;i&gt;MemoryCleanUp&lt;/i&gt;
            operator after large preprocessing trees using lot of views or data
            copies.</help>
    <key>materialize_data</key>
  </operator>
    <operator>
        <name>ArffWriter</name>
        <synopsis>Writes the values of all examples into an ARFF-file.
        </synopsis>
        <help>Writes values of all examples into an ARFF file which can be
            used by the machine learning library Weka. The ARFF format is
            described in the &lt;i&gt;ArffExampleSource&lt;/i&gt; operator which
            is able to read ARFF files to make them usable with RapidMiner.
        </help>
    </operator>
    <operator>
        <name>Vector Linear Regression</name>
        <synopsis>Vector linear regression.</synopsis>
        <help>This operator performs a vector linear regression. It regresses
            all regular attributes upon a vector of labels. The attributes
            forming the vector have to be marked as special, the special role
            names of all label attributes have to start with
            &lt;code&gt;label&lt;/code&gt;. TODO: Adapt meta data of model, but
            needs change of complete construction...</help>
    <key>vector_linear_regression</key>
  </operator>
    <operator>
        <name>Replace Missing Values</name>
        <synopsis>Replaces missing values in examples.</synopsis>
        <help>Replaces missing values in examples. If a value is missing, it
            is replaced by one of the functions &amp;quot;minimum&amp;quot;,
            &amp;quot;maximum&amp;quot;, &amp;quot;average&amp;quot;, and
            &amp;quot;none&amp;quot;, which is applied to the non missing
            attribute values of the example set. &amp;quot;none&amp;quot; means,
            that the value is not replaced. The function can be selected using
            the parameter list &lt;code&gt;columns&lt;/code&gt;. If an
            attribute's name appears in this list as a key, the value is used as
            the function name. If the attribute's name is not in the list, the
            function specified by the &lt;code&gt;default&lt;/code&gt; parameter
            is used. For nominal attributes the mode is used for the average,
            i.e. the nominal value which occurs most often in the data. For
            nominal attributes and replacement type zero the first nominal value
            defined for this attribute is used. The replenishment
            &amp;quot;value&amp;quot; indicates that the user defined parameter
            should be used for the replacement.</help>
    <key>replace_missing_values</key>
    <tags>
         <tag>Nulls</tag>
         <tag>Empty</tag>
         <tag>Cleansing</tag>
         <tag>Quality</tag>
         <tag>Missings</tag>
         <tag>Handle</tag>
         <tag>Impute</tag>
         <tag>Na</tag>
         <tag>Nan</tag>
         <tag>Fill na</tag>
    </tags>
  </operator>
    <operator>
        <name>AddNominalValue</name>
        <synopsis>This operator adds an additional value to a specified
            nominal attribute which is then mapped to a specific index.
        </synopsis>
        <help>Adds a value to a nominal attribute definition.</help>
    </operator>
    <operator>
        <name>Logistic Regression (Evolutionary)</name>
        <synopsis>A kernel logistic regression learner for binary
            classification tasks.</synopsis>
        <help>This operator determines a logistic regression model.</help>
    <key>logistic_regression_evolutionary</key>
  </operator>
    <operator>
        <name>Optimize Parameters (Grid)</name>
        <synopsis>This operator finds the optimal values for parameters.
        </synopsis>
        <help>
    &lt;p&gt;
      This operator finds the optimal values for a set of parameters using a 
      grid search. The parameter &lt;var&gt;parameters&lt;/var&gt; is a list of key value 
      pairs where the keys are of the form &lt;code&gt;operator_name.parameter_name&lt;/code&gt; 
      and the value is either a comma separated list of values (e.g. 
      10,15,20,25) or an interval definition in the format 
      [start;end;stepsize] (e.g. [10;25;5]). Alternatively a value grid 
      pattern may be used by [e.g. [start;end;no_steps;scale], where scale 
      identifies the type of the pattern.
    &lt;/p&gt;
    &lt;p&gt;
      The operator returns an optimal &lt;i&gt;ParameterSet&lt;/i&gt; which can as well be 
      written to a file with a &lt;i&gt;ParameterSetWriter&lt;/i&gt;. This parameter set 
      can be read in another process using a &lt;i&gt;ParameterSetLoader&lt;/i&gt;.
    &lt;/p&gt;
    &lt;p&gt;
      The file format of the parameter set file is straightforward and can 
      easily be generated by external applications. Each line is of the form
    &lt;/p&gt;
    &lt;center&gt;
      &lt;code&gt;operator_name.parameter_name = value&lt;/code&gt;
    &lt;/center&gt;
    &lt;p&gt;
      Additionally to the parameter set, it returns all inner results 
      generated during the execution, which delivered the best performance.
    &lt;/p&gt;
    &lt;p&gt;
      
    &lt;/p&gt;
    &lt;p&gt;
      Please refer to section &lt;i&gt;Advanced Processes/Parameter and performance 
      analysis&lt;/i&gt; for an example application. Another parameter optimization 
      schems like the &lt;i&gt;EvolutionaryParameterOptimizationOperator&lt;/i&gt; might 
      also be useful if the best ranges and dependencies are not known at all. 
      Another operator which works similar to this parameter optimization 
      operator is the operator &lt;i&gt;ParameterIteration&lt;/i&gt;. In contrast to the 
      optimization operator, this operator simply iterates through all 
      parameter combinations. This might be especially useful for plotting 
      purposes.
    &lt;/p&gt;
  </help>
    <key>optimize_parameters_grid</key>
    <tags>
         <tag>Iterate</tag>
         <tag>Settings</tag>
         <tag>Grid</tag>
         <tag>Search</tag>
         <tag>Tune</tag>
         <tag>Optimal</tag>
    </tags>
  </operator>
    <operator>
        <name>Nominal to Binominal</name>
        <synopsis>Maps all nominal values to binominal (binary) attributes.
        </synopsis>
        <help>This operator maps the values of all nominal values to binary
            attributes. For example, if a nominal attribute with name
            &amp;quot;costs&amp;quot; and possible nominal values
            &amp;quot;low&amp;quot;, &amp;quot;moderate&amp;quot;, and
            &amp;quot;high&amp;quot; is transformed, the result is a set of three
            binominal attributes &amp;quot;costs = low&amp;quot;, &amp;quot;costs
            = moderate&amp;quot;, and &amp;quot;costs = high&amp;quot;. Only one
            of the values of each attribute is true for a specific example, the
            other values are false.</help>
    <key>nominal_to_binominal</key>
    <tags>
         <tag>Categorical</tag>
         <tag>Polynominal</tag>
         <tag>Ordinary</tag>
         <tag>Dual</tag>
         <tag>Dichotomization</tag>
         <tag>Dichotomy</tag>
         <tag>Dummy</tag>
         <tag>Types</tag>
    </tags>
  </operator>
    <operator>
        <name>Replace Infinite Values</name>
        <synopsis>Replaces infinite values in examples.</synopsis>
        <help>Replaces positive and negative infinite values in examples by
            one of the functions &amp;quot;none&amp;quot;,
            &amp;quot;zero&amp;quot;, &amp;quot;max_byte&amp;quot;,
            &amp;quot;max_int&amp;quot;, &amp;quot;max_double&amp;quot;, and
            &amp;quot;missing&amp;quot;. &amp;quot;none&amp;quot; means, that the
            value is not replaced. The max_xxx functions replace plus infinity by
            the upper bound and minus infinity by the lower bound of the range of
            the Java type xxx. &amp;quot;missing&amp;quot; means, that the value
            is replaced by nan (not a number), which is internally used to
            represent missing values. A
            &lt;i&gt;MissingValueReplenishment&lt;/i&gt; operator can be used to
            replace missing values by average (or the mode for nominal
            attributes), maximum, minimum etc. afterwards.&lt;br/&gt; For each
            attribute, the function can be selected using the parameter list
            &lt;code&gt;columns&lt;/code&gt;. If an attribute's name appears in
            this list as a key, the value is used as the function name. If the
            attribute's name is not in the list, the function specified by the
            &lt;code&gt;default&lt;/code&gt; parameter is used.</help>
    <key>replace_infinite_values</key>
  </operator>

    <operator>
        <name>Optimize Weights (Evolutionary)</name>
        <synopsis>Weight the features with an evolutionary approach.
        </synopsis>
        <help>This operator performs the weighting of features with an
            evolutionary strategies approach. The variance of the gaussian
            additive mutation can be adapted by a 1/5-rule.</help>
    <key>optimize_weights_evolutionary</key>
  </operator>
    <operator>
        <name>Generate n-Grams (Terms)</name>
        <synopsis>Creates token ngrams.</synopsis>
        <help>Creates term ngrams of the input terms.</help>
    <key>generate_n_grams_terms</key>
  </operator>
    <operator>
        <name>Group Models</name>
        <synopsis>Groups the input models into a single combined model which
            might be necessary for example for applying preprocessing models.
        </synopsis>
        <help>&lt;p&gt;This operator groups all input models together into a
            grouped (combined) model. This model can be completely applied on new
            data or written into a file as once. This might become useful in
            cases where preprocessing and prediction models should be applied
            together on new and unseen data.&lt;/p&gt; &lt;p&gt;This operator
            replaces the automatic model grouping known from previous versions of
            RapidMiner. The explicit usage of this grouping operator gives the
            user more control about the grouping procedure. A grouped model can
            be ungrouped with the &lt;i&gt;ModelUngrouper&lt;/i&gt;
            operator.&lt;/p&gt; &lt;p&gt;Please note that the input models will
            be added in reverse order, i.e. the last created model, which is
            usually the first one at the start of the io object, queue will be
            added as the last model to the combined group model.&lt;/p&gt;</help>
    <key>group_models</key>
  </operator>
    <operator>
        <name>Nominal2Numeric</name>
        <synopsis>Maps all values to real values (usually simply using the
            internal indices).</synopsis>
        <help>This operator maps all non numeric attributes to real valued
            attributes. Nothing is done for numeric attributes, binary attributes
            are mapped to 0 and 1. For nominal attributes one of the following
            calculations will be done: &lt;ul&gt; &lt;li&gt;Dichotomization, i.e.
            one new attribute for each value of the nominal attribute. The new
            attribute which corresponds to the actual nominal value gets value 1
            and all other attributes gets value 0.&lt;/li&gt;
            &lt;li&gt;Alternatively the values of nominal attributes can be seen
            as equally ranked, therefore the nominal attribute will simply be
            turned into a real valued attribute, the old values results in
            equidistant real values.&lt;/li&gt; &lt;/ul&gt; At this moment the
            same applies for ordinal attributes, in a future release more
            appropriate values based on the ranking between the ordinal values
            may be included.</help>
    </operator>
    <operator>
        <name>Loop Files (Deprecated)</name>
        <synopsis>This operator iterates over all files in a directory and
            delivers their paths and names as macro to the inner operators.
        </synopsis>
        <help>This operator iterates over the files in the specified directory
            (and subdirectories if the corresponding parameter is set to true).<br/>
            Each file is delivered as FileObject to the
        	subprocess at the respective inner port.
        </help>
    <key>loop_files</key>
  </operator>
    <operator>
        <name>Loop Repository</name>
        <synopsis>This operator iterates over all IO Objects or Blob files in a repository and
            delivers their paths and names as macro to the inner operators.
        </synopsis>
        <help>This operator iterates over the IO Objects or Blob files in the specified reporitory
            (and subdirectories if the corresponding parameter is set to true).<br/>
            Each Object is delivered as IO Object or FileObject to the
        	subprocess at the respective inner port.
        </help>
        <key>loop_repository</key>
    </operator>
    <operator>
        <name>Loop Zip-File Entries</name>
        <synopsis>This operator loops over the entries of a zip file.
        </synopsis>
        <help>This operator loops over the entries of a zip file. Depending on the parameters
        loops only file and/or directory entries and filters the files by name.<br/>
        Each entry is delivered as FileObject to the
        subprocess at the respective inner port.
        </help>
        <key>loop_zipfile_entries</key>
    </operator>
    <operator>
        <name>Loop Parameters</name>
        <synopsis>This operator just iterates through all defined parameter
            combinations.</synopsis>
        <help>&lt;p&gt;In contrast to the
            &lt;i&gt;GridSearchParameterOptimizationOperator&lt;/i&gt; operator
            this operators simply uses the defined parameters and perform the
            inner operators for all possible combinations. This can be especially
            useful for plotting or logging purposes and sometimes also for simply
            configuring the parameters for the inner operators as a sort of meta
            step (e.g. learning curve generation).&lt;/p&gt; &lt;p&gt;This
            operator iterates through a set of parameters by using all possible
            parameter combinations. The parameter
            &lt;var&gt;parameters&lt;/var&gt; is a list of key value pairs where
            the keys are of the form
            &lt;code&gt;operator_name.parameter_name&lt;/code&gt; and the value
            is either a comma separated list of values (e.g. 10,15,20,25) or an
            interval definition in the format [start;end;stepsize] (e.g.
            [10;25;5]). Additionally, the format [start;end;steps;scale] is
            allowed.&lt;/p&gt; &lt;p&gt;Please note that this operator has two
            modes: synchronized and non-synchronized. In the latter, all
            parameter combinations are generated and the inner operators are
            applied for each combination. In the synchronized mode, no
            combinations are generated but the set of all pairs of the increasing
            number of parameters are used. For the iteration over a single
            parameter there is no difference between both modes. Please note that
            the number of parameter possibilities must be the same for all
            parameters in the synchronized mode.&lt;/p&gt; Compatibility note:
            This operator no longer returns all of its input. In most
            applications all that can be done with such a collection of IOOBjects
            is iterating over them again, and that can as well be done inside the
            ParameterIteration. Where this is not possible, please group them
            into a collection (using a IOCollector) and store the collection
            using an IOStorage operator.</help>
    <key>loop_parameters</key>
    <tags>
         <tag>Iterate</tag>
         <tag>Settings</tag>
         <tag>Grid</tag>
         <tag>Search</tag>
         <tag>Tune</tag>
         <tag>Optimal</tag>
    </tags>
  </operator>
    <operator>
        <name>Extract Macro</name>
        <synopsis>This operator can be used to define a single macro which can
            be used by %{my_macro} in parameter values. The macro value will be
            derived from the input &lt;i&gt;ExampleSet&lt;/i&gt;.</synopsis>
        <help>
            &lt;p&gt;This operator (re-)defines a macro for the current process. Macros will
            be replaced in the value strings of parameters by the macros' values, see Macros section below. In contrast to the usual
            &lt;a href="rm://opdoc/set_macro"&gt;Set Macro&lt;/a&gt; operator, this operator sets the value of a single macro from properties of a given input
            &lt;i&gt;ExampleSet&lt;/i&gt;, e.g. from properties like the number of examples or
            attributes or from a specific data value. The name of the macro must be specified in the &lt;b&gt;macro&lt;/b&gt; parameter and the way the value is retrieved from the &lt;i&gt;ExampleSet&lt;/i&gt; must be chosen from &lt;b&gt;macro_type&lt;/b&gt;.&lt;/p&gt;&lt;br/&gt;&lt;br/&gt;
            
            &lt;h4&gt;Macros&lt;/h4&gt;
            &lt;p&gt;A defined macro can then be used in all succeeding
            operators as parameter value for parameters. A macro must
            then be enclosed by &quot;%{&quot; and
            &quot;}&quot;.&lt;/p&gt; &lt;p&gt;There are several
            predefined macros:&lt;/p&gt; &lt;ul&gt;
            &lt;li&gt;&lt;b&gt;%{process_name}:&lt;/b&gt; will be replaced by the
            name of the process (without path and extension)&lt;/li&gt;
            &lt;li&gt;&lt;b&gt;%{process_file}:&lt;/b&gt; will be replaced by the
            file name of the process (with extension)&lt;/li&gt;
            &lt;li&gt;&lt;b&gt;%{process_path}:&lt;/b&gt; will be replaced by the
            complete absolute path of the process file&lt;/li&gt; &lt;/ul&gt;
            &lt;p&gt;In addition to those the user might define arbitrary other
            macros which will be replaced by arbitrary string during the process
            run. Please note also that several other short macros exist, e.g.
            %{a} for the number of times the current operator
            was applied.&lt;br/&gt;
            Please note also that other operators like many of the loop operators like 
            &lt;a href="rm://opdoc/loop_values"&gt;Loop Values&lt;/a&gt; or &lt;a href="rm://opdoc/loop_attributes"&gt;Loop Attributes&lt;/a&gt; also add specific
            macros.&lt;/p&gt;
        </help>
    <key>extract_macro</key>
    <tags>
         <tag>Process</tag>
         <tag>Variable</tag>
         <tag>Setting</tag>
    </tags>
  </operator>

    <operator>
        <name>Replace Tokens</name>
        <synopsis>Replaces all occurences of all specified regular expression
            within each token by its specified replacement.</synopsis>
        <help>This operator allows replacing of substrings within each token.
            Therefore the user might specify arbitrary what/by pairs in the
            replace_dictionary parameter. The left column of the table specifies
            what should be replaced and the right side the replacement. Since
            replacement is not performed over boundaries of tokens, this operator
            is best placed directly after the TextInput operator or at least
            before the tokenizer. To specify what should be replaced, regular
            expressions might be used.(Please refer to the annex of the
            RapidMiner tutorial for a description). Please remember that several
            characters have special meanings and might have to be quoted if used
            as the character itself. The replacement can be defined as an
            arbitrary string. Capturing groups of the defined regular expression
            can be accessed with $1, $2, $3... Empty strings are not allowed, but
            since a following tokenizer will simply skip additional blanks, feel
            free to replace with a blank.</help>
    <key>replace_tokens</key>
  </operator>
    <operator>
        <name>Generate Multi-Label Data</name>
        <synopsis>Generates an example set based on numerical attributes and
            with more than one label.</synopsis>
        <help>Generates a random example set for testing purposes with more
            than one label.</help>
    <key>generate_multi_label_data</key>
  </operator>
    <operator>
        <name>IOSelector</name>
        <synopsis>This operators simply selects one of the input objects of
            the specified type and brings it to the front so that following
            operators will work on this object.</synopsis>
        <help>&lt;p&gt;This operator allows to choose special IOObjects from
            the given input. Bringing an IOObject to the front of the input queue
            allows the next operator to directly perform its action on the
            selected object. Please note that counting for the parameter value
            starts with one, but usually the IOObject which was added at last
            gets the number one, the object added directly before get number two
            and so on.&lt;/p&gt; &lt;p&gt;The user can specify with the parameter
            &lt;code&gt;delete_others&lt;/code&gt; what will happen to the
            non-selected input objects of the specified type: if this parameter
            is set to true, all other IOObjects of the specified type will be
            removed by this operator. Otherwise (default), the objects will all
            be kept and the selected objects will just be brought into
            front.&lt;/p&gt;</help>
    <key>ioselector</key>
  </operator>
    <operator>
        <name>Exchange Roles</name>
        <synopsis>This operator can be used to exchange the attribute roles of
            two attributes (e.g. a label with a regular attribute).</synopsis>
        <help>This operator changes the attribute roles of two input
            attributes. This could for example be useful to exchange the roles of
            a label with a regular attribute (and vice versa), or a label with a
            batch attribute, a label with a cluster etc.</help>
    <key>exchange_roles</key>
  </operator>
    <operator>
        <name>CHAID</name>
        <synopsis>Learns a pruned decision tree based on a chi squared
            attribute relevance test.</synopsis>
        <help>The CHAID decision tree learner works like the
            &lt;i&gt;DecisionTreeLearner&lt;/i&gt; with one exception: it used a
            chi squared based criterion instead of the information gain or gain
            ratio criteria.</help>
    <key>chaid</key>
  </operator>
    <operator>
        <name>Loop Data Sets</name>
        <synopsis>Performs its inner operators for each example set found in
            input.</synopsis>
        <help>For each example set the ExampleSetIterator finds in its input,
            the inner operators are applied as if it was an OperatorChain. This
            operator can be used to conduct a process consecutively on a number
            of different data sets.</help>
    <key>loop_data_sets</key>
  </operator>
    <operator>
        <name>AbsoluteSampling</name>
        <synopsis>Creates a sample from an example set by drawing an exact
            number of examples.</synopsis>
        <help>Absolute sampling operator. This operator takes a random sample
            with the given size. For example, if the sample size is set to 50,
            the result will have exactly 50 examples randomly drawn from the
            complete data set. Please note that this operator does not sample
            during a data scan but jumps to the rows. It should therefore only be
            used in case of memory data management and not, for example, for
            database or file management.</help>
    <key>absolutesampling</key>
  </operator>
    <operator>
        <name>DirectoryIterator</name>
        <synopsis>This operator iterates over all files in a directory and
            delivers their paths and names as macro to the inner operators.
        </synopsis>
        <help>This operator iterates over the files in the specified directory
            (and subdirectories if the corresponding parameter is set to true).
        </help>
    </operator>
    <operator>
        <name>Weight by Information Gain</name>
        <synopsis>This operator calculates the relevance of the attributes
            based on the information gain.</synopsis>
        <help>This operator calculates the relevance of a feature by computing
            the information gain in class distribution, if exampleSet would be
            splitted after the feature.</help>
    <key>weight_by_information_gain</key>
    <tags>
         <tag>Weighting</tag>
         <tag>Importance</tag>
         <tag>Influence</tag>
         <tag>Significance</tag>
         <tag>Factors</tag>
         <tag>Relevance</tag>
         <tag>Entropy</tag>
    </tags>
  </operator>
    <operator>
        <name>Parse Numbers</name>
        <synopsis>Maps all nominal values to numerical values by parsing the
            numbers if possible.</synopsis>
        <help>&lt;p&gt;This operator transforms nominal attributes into
            numerical ones. In contrast to the NominalToNumeric operator, this
            operator directly parses numbers from the wrongly as nominal values
            encoded values. Please note that this operator will first check the
            stored nominal mappings for all attributes. If (old) mappings are
            still stored which actually are nominal (without the corresponding
            data being part of the example set), the attribute will not be
            converted. Please use the operator &lt;i&gt;Guess Types&lt;/i&gt;
            in these cases.&lt;/p&gt;</help>
    <key>parse_numbers</key>
    <tags>
         <tag>Nominal to numerical</tag>
         <tag>Parsing</tag>
         <tag>Formats</tag>
         <tag>Continous</tag>
         <tag>Numbers</tag>
         <tag>Quantitative</tag>
         <tag>Integers</tag>
         <tag>Reals</tag>
         <tag>Types</tag>
    </tags>
  </operator>
    <operator>
        <name>ExcelReader</name>
        <synopsis>This operator reads an example set from Excel spreadsheet
            files.</synopsis>
        <help>&lt;p&gt;This operator can be used to load data from Microsoft
            Excel spreadsheets. This operator is able to reads data from Excel
            95, 97, 2000, XP, and 2003. The user has to define which of the
            spreadsheets in the workbook should be used as data table. The table
            must have a format so that each line is an example and each column
            represents an attribute. Please note that the first line might be
            used for attribute names which can be indicated by a
            parameter.&lt;/p&gt; &lt;p&gt;The data table can be placed anywhere
            on the sheet and is allowed to contain arbitrary formatting
            instructions, empty rows, and empty columns. Missing data values are
            indicated by empty cells or by cells containing only
            &amp;quot;?&amp;quot;.&lt;/p&gt;</help>
    </operator>
    <operator>
        <name>Support Vector Machine (PSO)</name>
        <synopsis>PsoSVM uses a Particle Swarm Optimization for optimization.
        </synopsis>
        <help>This is a SVM implementation using a particle swarm optimization
            (PSO) approach to solve the dual optimization problem of a SVM. It
            turns out that on many datasets this simple implementation is as fast
            and accurate as the usual SVM implementations.</help>
    <key>support_vector_machine_pso</key>
    <shortName>SVM</shortName>
  </operator>
    <operator>
        <name>Discretize by User Specification</name>
        <synopsis>Discretize numerical attributes into user defined bins.
        </synopsis>
        <help>This operator discretizes a numerical attribute to either a
            nominal or an ordinal attribute. The numerical values are mapped to
            the classes according to the thresholds specified by the user. The
            user can define the classes by specifying the upper limits of each
            class. The lower limit of the next class is automatically specified
            as the upper limit of the previous one. A parameter defines to which
            adjacent class values that are equal to the given limits should be
            mapped. If the upper limit in the last list entry is not equal to
            Infinity, an additional class which is automatically named is added.
            If a '?' is given as class value the according numerical values are
            mapped to unknown values in the resulting attribute.</help>
    <key>discretize_by_user_specification</key>
    <tags>
         <tag>Continous</tag>
         <tag>Categorical</tag>
         <tag>Nominal</tag>
         <tag>Polynominal</tag>
         <tag>Ordinary</tag>
         <tag>Discrete</tag>
         <tag>Discretization</tag>
         <tag>Dichotomization</tag>
         <tag>Dichotomy</tag>
         <tag>Binning</tag>
         <tag>Histogram</tag>
         <tag>Types</tag>
         <tag>Qualitative</tag>
         <tag>Quantitative</tag>
         <tag>Groups</tag>
         <tag>Intervals</tag>
    </tags>
    <shortName>Discretize</shortName>
  </operator>
    <operator>
        <name>Polynominal by Binominal Classification</name>
        <synopsis>Builds a classification model for multiple classes based on
            a binary learner.</synopsis>
        <help>A metaclassifier for handling multi-class datasets with 2-class
            classifiers. This class supports several strategies for multiclass
            classification including procedures which are capable of using
            error-correcting output codes for increased accuracy.</help>
    <key>polynomial_by_binomial_classification</key>
  </operator>
  <operator>
        <name>Hierarchical Classification (Deprecated)</name>
        <synopsis>Builds a hierarchical classification model due to the specified class taxonomy.</synopsis>
        <help>This meta learner builds a hierarchical classification model due to a
        	  class taxonomy. This class taxonomy has to be specified within the
        	  hierarchy parameter list. Each list entry represents an edge in the
        	  class hierarchy which in fact represents a parent-child class relationship.</help>
        <key>hierarchical_classification</key>
  </operator>
  <operator>
        <name>Hierarchical Classification</name>
        <synopsis>Builds a hierarchical classification model due to the specified class taxonomy.</synopsis>
        <help>This meta learner builds a hierarchical classification model due to a
              class taxonomy. This class taxonomy has to be specified within the
              hierarchy parameter list. Each list entry represents an edge in the
              class hierarchy which in fact represents a parent-child class relationship. You need to specify
              one root node and assign each other node to one father.</help>
        <key>hierarchical_multi_class_classification</key>
  </operator>

        <operator>
        <name>Loop Until</name>
        <synopsis>Performs its inner operators until some condition is met.
        </synopsis>
        <help>Performs its inner operators until all given criteria are met or
            a timeout occurs.</help>
    <key>loop_until</key>
  </operator>
    <operator>
        <name>Principal Component Analysis</name>
        <synopsis>Performs a principal component analysis (PCA) using the
            covariance matrix.</synopsis>
        <help>This operator performs a principal components analysis (PCA)
            using the covariance matrix. The user can specify the amount of
            variance to cover in the original data when retaining the best number
            of principal components. The user can also specify manually the
            number of principal components. The operator outputs a
            &lt;code&gt;PCAModel&lt;/code&gt;. With the
            &lt;code&gt;ModelApplier&lt;/code&gt; you can transform the features.
        </help>
    <key>principal_component_analysis</key>
    <tags>
         <tag>PCA</tag>
         <tag>Components</tag>
         <tag>Orthogonal</tag>
         <tag>Eigenvalues</tag>
         <tag>Decompositions</tag>
         <tag>Singular</tag>
         <tag>Reduction</tag>
         <tag>Multicollinearity</tag>
    </tags>
    <shortName>PCA</shortName>
  </operator>
    <operator>
        <name>Execute Script</name>
        <synopsis>This operator executes arbitrary Groovy scripts.</synopsis>
        <help>&lt;p&gt;This operator can be used to execute arbitrary Groovy
            scripts. This basically means that analysts can write their own
            operators directly within the process by specifiying Java code and /
            or a Groovy script which will be interpreted and executed during
            process runtime. For a complete reference of Groovy scripting please
            refer to http://groovy.codehaus.org/.&lt;/p&gt; &lt;p&gt;In addition
            to the usual scripting code elements from Groovy, the RapidMiner
            scripting operator defines some special scripting elements:&lt;/p&gt;
            &lt;ul&gt; &lt;li&gt;If you use the standard
            &lt;em&gt;imports&lt;/em&gt;, all important types like Example,
            ExampleSet, Attribute, Operator etc. as well as the most important
            Java types like collections etc. are automatically imported and can
            directly be used within the script. Hence, there is no need for
            importing them in your script. However, you can of course import any
            other class you want and use this in your script.&lt;/li&gt;
            &lt;li&gt;The &lt;em&gt;current operator&lt;/em&gt; (the scripting
            operator for which you define the script) is referenced by
            &lt;code&gt;operator&lt;/code&gt;.&lt;br /&gt; Example:
            &lt;code&gt;operator.log("text")&lt;/code&gt;&lt;/li&gt;
            &lt;li&gt;All &lt;em&gt;operator methods&lt;/em&gt; like
            &lt;code&gt;log&lt;/code&gt; (see above), accessing the input or the
            complete process can directly be used by writing a preceding
            &lt;code&gt;operator&lt;/code&gt;.&lt;br /&gt; Example:
            &lt;code&gt;operator.getProcess()&lt;/code&gt;&lt;/li&gt;
            &lt;li&gt;&lt;em&gt;Input of the operator&lt;/em&gt; can be retrieved
            via the input method getInput(Class) of the surrounding
            operator.&lt;br /&gt; Example: &lt;code&gt;ExampleSet exampleSet =
            operator.getInput(ExampleSet.class)&lt;/code&gt;&lt;/li&gt;
            &lt;li&gt;You can &lt;em&gt;iterate over examples&lt;/em&gt; with the
            following construct:&lt;br /&gt; &lt;code&gt;for (Example example :
            exampleSet) { ... }&lt;/code&gt;&lt;/li&gt; &lt;li&gt;You can
            &lt;em&gt;retrieve example values&lt;/em&gt; with the shortcut&lt;br
            /&gt; &lt;code&gt;String value =
            example[&amp;quot;attribute_name&amp;quot;];&lt;/code&gt; or &lt;br
            /&gt; &lt;code&gt;double value =
            example[&amp;quot;attribute_name&amp;quot;];&lt;/code&gt;&lt;/li&gt;
            &lt;li&gt;You can &lt;em&gt;set example values&lt;/em&gt; with
            &lt;code&gt;example[&amp;quot;attribute_name&amp;quot;] =
            &amp;quot;value&amp;quot;;&lt;/code&gt; or &lt;br /&gt;
            &lt;code&gt;example[&amp;quot;attribute_name&amp;quot;] =
            5.7;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;
            &lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; Scripts written for this operator
            may access Java code. Scripts may hence become incompatible in future
            releases of RapidMiner.&lt;/p&gt;</help>
    <key>execute_script</key>
  </operator>
    <operator>
        <name>AttributeValueMapper</name>
        <synopsis>Maps certain values of an attribute to other values.
        </synopsis>
        <help>&lt;p&gt;This operator takes an
            &lt;code&gt;ExampleSet&lt;/code&gt; as input and maps the values of
            certain attributes to other values. The operator can replace nominal
            values (e.g. replace the value &amp;quot;green&amp;quot; by the value
            &amp;quot;green_color&amp;quot;) as well as numerical values (e.g.
            replace the all values &amp;quot;3&amp;quot; by
            &amp;quot;-1&amp;quot;). A single mapping can be specified using the
            parameters &lt;code&gt;replace_what&lt;/code&gt; and
            &lt;code&gt;replace_by&lt;/code&gt;. Multiple mappings can be
            specified in the parameter list
            &lt;code&gt;value_mappings&lt;/code&gt;.&lt;/p&gt;
            &lt;p&gt;Additionally, the operator allows to define (and consider) a
            default mapping. If &lt;code&gt;add_default_mapping&lt;/code&gt; is
            set to true and &lt;code&gt;default_value&lt;/code&gt; is properly
            set, all values that occur in the example set but are not listed in
            the value mappings list are replaced by the default value. This may
            be helpful in cases where only some values should be mapped
            explicitly and many unimportant values should be mapped to a default
            value (e.g. "other").&lt;/p&gt; &lt;p&gt;If the parameter
            &lt;code&gt;consider_regular_expressions&lt;/code&gt; is enabled, the
            values are replaced by the new values if the original values match
            the given regular expressions. The value corresponding to the first
            matching regular expression in the mappings list is taken as
            replacement.&lt;/p&gt; &lt;p&gt;This operator supports regular
            expressions for the attribute names, i.e. the value mapping is
            applied on all attributes for which the name fulfills the pattern
            defined by the name expression.&lt;/p&gt;</help>
    </operator>
    <operator>
        <name>De-Pivot</name>
        <synopsis>Transforms an example set by dividing examples containing
            multiple observations (in attributes) into multiple examples.
        </synopsis>
        <help>This operator converts an example set by dividing examples which
            consist of multiple observations (at different times) into multiple
            examples, where each example covers on point in time. An index
            attribute is added, which contains denotes the actual point in time
            the example belongs to after the transformation. The parameter
            &lt;code&gt;keep_missings&lt;/code&gt; specifies whether examples
            should be kept, even if if exhibits missing values for all series at
            a certain point in time. The parameter create_nominal_index is only
            applicable if only one time series per example exists. Instead of
            using a numeric index, then the names of the attributes representing
            the single time points are used as index attribute.</help>
    <key>de_pivot</key>
    <tags>
         <tag>Rotate</tag>
         <tag>Rotations</tag>
         <tag>Pivoting</tag>
         <tag>Group</tag>
         <tag>Grouping</tag>
         <tag>Group by</tag>
         <tag>Aggregate</tag>
         <tag>Cross-table</tag>
         <tag>Crosstable</tag>
         <tag>Long</tag>
         <tag>Wide</tag>
         <tag>Transpose</tag>
         <tag>Sum</tag>
         <tag>Count</tag>
    </tags>
  </operator>
    <operator>
        <name>Performance (Attribute Count)</name>
        <synopsis>This operator created a performance vector containing the
            number of features of the input example set.</synopsis>
        <help>Returns a performance vector just counting the number of
            attributes currently used for the given example set.</help>
    <key>performance_attribute_count</key>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>Provide Macro as Log Value</name>
        <synopsis>Reads the value from the specified macro and provides the
            value for logging purposes.</synopsis>
        <help>&lt;p&gt;This operator can be used to log the current value of
            the specified macro. Some operators provide the macro they define
            themselves as loggable values and in these cases this value can
            directly be logged. But in all other cases where the operator does
            not provide a loggable value for the defined macro, this operator may
            be used to define such a value from the macro.&lt;/p&gt;
            &lt;p&gt;Please note that the value will be logged as nominal value
            even if it is actually numerical. This can be later be changed by
            transforming the logged statistics into a data set.&lt;/p&gt;</help>
    <key>provide_macro_as_log_value</key>
  </operator>
    <operator>
        <name>Gaussian Process</name>
        <synopsis>An implementation of Gaussian Processes.</synopsis>
        <help>Gaussian Process (GP) Learner. The GP is a probabilistic method
            both for classification and regression.</help>
    <key>gaussian_process</key>
  </operator>
    <operator>
        <name>Classification by Regression</name>
        <synopsis>This operator chain must contain a regression learner and
            allows to learn classifications tasks with more than two classes.
        </synopsis>
        <help>For a classified dataset (with possibly more than two classes)
            builds a classifier using a regression method which is specified by
            the inner operator. For each class &lt;i&gt;i&lt;/i&gt; a regression
            model is trained after setting the label to &lt;i&gt;+1&lt;/i&gt; if
            the label equals &lt;i&gt;i&lt;/i&gt; and to &lt;i&gt;-1&lt;/i&gt; if
            it is not. Then the regression models are combined into a
            classification model. In order to determine the prediction for an
            unlabeled example, all models are applied and the class belonging to
            the regression model which predicts the greatest value is chosen.
        </help>
    <key>classification_by_regression</key>
  </operator>
    <operator>
        <name>Polynomial Regression</name>
        <synopsis>Polynomial regression.</synopsis>
        <help>&lt;p&gt;This regression learning operator fits a polynomial of
            all attributes to the given data set. If the data set contains a
            label Y and three attributes X1, X2, and X3 a function of the
            form&lt;br /&gt; &lt;br /&gt; &lt;code&gt;Y = w0 + w1 * X1 ^ d1 + w2
            * X2 ^ d2 + w3 * X3 ^ d3&lt;/code&gt;&lt;br /&gt; &lt;br /&gt; will
            be fitted to the training data.&lt;/p&gt;</help>
    <key>polynomial_regression</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Regression</tag>
         <tag>Polynomials</tag>
         <tag>Fitting</tag>
    </tags>
  </operator>
    <operator>
        <name>Process Documents</name>
        <synopsis>Generates word vectors from a text object.</synopsis>
        <help>This operator uses one single TextObject as input for generating
            a term vector. The resulting exampleset will hence consist of only
            one single example. This makes this operator especially useful for
            applying a model on one single text. But since the
            SingleTextInputOperator even provides a parameter for specifying the
            text, this one is more appropriate if used by a program, where a
            TextObject might simply be constructed and passed to the process.
        </help>
    <key>process_documents</key>
  </operator>
    <operator>
        <name>Average</name>
        <synopsis>Builds the average of input average vectors (e.g.
            performance) of the same type.</synopsis>
        <help>Collects all average vectors (e.g. PerformanceVectors) from the
            input and averages them if they are of the same type.</help>
    <key>average</key>
  </operator>
    <operator>
        <name>k-NN</name>
        <synopsis>Classification with k-NN based on an explicit similarity
            measure.</synopsis>
        <help>A k nearest neighbor implementation.</help>
    <key>k_nn</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Model</tag>
         <tag>Nearest</tag>
         <tag>Neighbors</tag>
         <tag>Neighbours</tag>
         <tag>Knn</tag>
         <tag>Instance-based</tag>
         <tag>Instancebased</tag>
         <tag>Similarity</tag>
         <tag>Similarities</tag>
         <tag>Euclidean</tag>
         <tag>Distances</tag>
    </tags>
  </operator>
    <operator>
        <name>Data to Similarity</name>
        <synopsis>Calculates a similarity measure from the given data
            (attribute based).</synopsis>
        <help>This class represents an operator that creates a similarity
            measure based on an ExampleSet.</help>
    <key>data_to_similarity</key>
    <tags>
         <tag>Similarity</tag>
         <tag>Similarities</tag>
         <tag>Euclidean</tag>
         <tag>Distances</tag>
         <tag>Cross-distances</tag>
         <tag>Crossdistances</tag>
    </tags>
  </operator>
    <operator>
        <name>Rescale Confidences</name>
        <synopsis>Turns confidence scores of boolean classifiers into
            probability estimates.</synopsis>
        <help>A scaling operator, applying the original algorithm by Platt
            (1999) to turn confidence scores of boolean classifiers into
            probability estimates. Unlike the original version this operator
            assumes that the confidence scores are already in the interval of
            [0,1], as e.g. given for the RapidMiner boosting operators. The crude
            estimates are then transformed into log odds, and scaled by the
            original transformation of Platt. The operator assumes a model and an
            example set for scaling. It outputs a PlattScalingModel, that
            contains both, the supplied model and the scaling step. If the
            example set contains a weight attribute, then this operator is able
            to fit a model to the weighted examples.</help>
    <key>rescale_confidences</key>
  </operator>
    <operator>
        <name>AttributeValueSubstring</name>
        <synopsis>Creates new attributes from nominal attributes which only
            contain substrings of the original attributes.</synopsis>
        <help>This operator creates new attributes from nominal attributes
            where the new attributes contain only substrings of the original
            values. Please note that the counting starts with 1 and that the
            first and the last character will be included in the resulting
            substring. For example, the value is &amp;quot;RapidMiner&amp;quot;
            and the first index is set to 6 and the last index is set to 9 the
            result will be &amp;quot;Mine&amp;quot;. If the last index is larger
            than the length of the word, the resulting substrings will end with
            the last character.</help>
    </operator>
    
    <operator>
        <name>DataStatistics</name>
        <synopsis>Calculates some simple data statistics usually displayed by
            the GUI (only necessary for command line processes).</synopsis>
        <help>This operators calculates some very simple statistics about the
            given example set. These are the ranges of the attributes and the
            average or mode values for numerical or nominal attributes
            respectively. These informations are automatically calculated and
            displayed by the graphical user interface of RapidMiner. Since they
            cannot be displayed with the command line version of RapidMiner this
            operator can be used as a workaround in cases where the graphical
            user interface cannot be used.</help>
    <key>datastatistics</key>
  </operator>
    <operator>
        <name>Document to Data</name>
        <synopsis>Generates an exampleSet containing the text as nominal
            attribute value.</synopsis>
        <help>This operator generates an exampleSet from a given input
            TextObject by creating a new exampleSet with a nominal attribute
            storing the text. If the TextObject contains a label value, a label
            attribute is generated. Since a TextObject only contains one text,
            the exampleSet has a size of one. Hence this operator is supposed to
            be helpful only on apply time.</help>
    <key>document_to_data</key>
  </operator>

    <operator>
        <name>Generate Macro</name>
        <synopsis>This operator can be used to calculate new macros (from
            existing ones).</synopsis>
        <help>&lt;p&gt;This operator constructs new macros from expressions
            which might also use already existing macros. The names of the new
            macros and their construction description are defined in the
            parameter list &amp;quot;functions&amp;quot;.&lt;/p&gt; &lt;p&gt;The
            following &lt;em&gt;operators&lt;/em&gt; are supported: &lt;ul&gt;
            &lt;li&gt;Addition: +&lt;/li&gt; &lt;li&gt;Subtraction: -&lt;/li&gt;
            &lt;li&gt;Multiplication: *&lt;/li&gt; &lt;li&gt;Division:
            /&lt;/li&gt; &lt;li&gt;Power: ^&lt;/li&gt; &lt;li&gt;Modulus:
            %&lt;/li&gt; &lt;li&gt;Less Than: &amp;lt;&lt;/li&gt;
            &lt;li&gt;Greater Than: &amp;gt;&lt;/li&gt; &lt;li&gt;Less or Equal:
            &amp;lt;=&lt;/li&gt; &lt;li&gt;More or Equal: &amp;gt;=&lt;/li&gt;
            &lt;li&gt;Equal: ==&lt;/li&gt; &lt;li&gt;Not Equal: !=&lt;/li&gt;
            &lt;li&gt;Boolean Not: !&lt;/li&gt; &lt;li&gt;Boolean And: two ampers
            and&lt;/li&gt; &lt;li&gt;Boolean Or: ||&lt;/li&gt; &lt;/ul&gt;
            &lt;/p&gt; &lt;p&gt;The following &lt;em&gt;log and exponential
            functions&lt;/em&gt; are supported: &lt;ul&gt; &lt;li&gt;Natural
            Logarithm: ln(x)&lt;/li&gt; &lt;li&gt;Logarithm Base 10:
            log(x)&lt;/li&gt; &lt;li&gt;Logarithm Dualis (Base 2):
            ld(x)&lt;/li&gt; &lt;li&gt;Exponential (e^x): exp(x)&lt;/li&gt;
            &lt;li&gt;Power: pow(x,y)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt;
            &lt;p&gt;The following &lt;em&gt;trigonometric functions&lt;/em&gt;
            are supported: &lt;ul&gt; &lt;li&gt;Sine: sin(x)&lt;/li&gt;
            &lt;li&gt;Cosine: cos(x)&lt;/li&gt; &lt;li&gt;Tangent:
            tan(x)&lt;/li&gt; &lt;li&gt;Arc Sine: asin(x)&lt;/li&gt;
            &lt;li&gt;Arc Cosine: acos(x)&lt;/li&gt; &lt;li&gt;Arc Tangent:
            atan(x)&lt;/li&gt; &lt;li&gt;Arc Tangent (with 2 parameters):
            atan2(x,y)&lt;/li&gt; &lt;li&gt;Hyperbolic Sine: sinh(x)&lt;/li&gt;
            &lt;li&gt;Hyperbolic Cosine: cosh(x)&lt;/li&gt; &lt;li&gt;Hyperbolic
            Tangent: tanh(x)&lt;/li&gt; &lt;li&gt;Inverse Hyperbolic Sine:
            asinh(x)&lt;/li&gt; &lt;li&gt;Inverse Hyperbolic Cosine:
            acosh(x)&lt;/li&gt; &lt;li&gt;Inverse Hyperbolic Tangent:
            atanh(x)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt; &lt;p&gt;The following
            &lt;em&gt;statistical functions&lt;/em&gt; are supported: &lt;ul&gt;
            &lt;li&gt;Round: round(x)&lt;/li&gt; &lt;li&gt;Round to p decimals:
            round(x,p)&lt;/li&gt; &lt;li&gt;Floor: floor(x)&lt;/li&gt;
            &lt;li&gt;Ceiling: ceil(x)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt;
            &lt;p&gt;The following &lt;em&gt;miscellaneous functions&lt;/em&gt;
            are supported: &lt;ul&gt; &lt;li&gt;Average: avg(x,y,z...)&lt;/li&gt;
            &lt;li&gt;Minimum: min(x,y,z...)&lt;/li&gt; &lt;li&gt;Maximum:
            max(x,y,z...)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt; &lt;p&gt;The
            following &lt;em&gt;miscellaneous functions&lt;/em&gt; are supported:
            &lt;ul&gt; &lt;li&gt;If-Then-Else: if(cond,true-evaluation,
            false-evaluation)&lt;/li&gt; &lt;li&gt;Absolute: abs(x)&lt;/li&gt;
            &lt;li&gt;Square Root: sqrt(x)&lt;/li&gt; &lt;li&gt;Signum (delivers
            the sign of a number): sgn(x)&lt;/li&gt; &lt;li&gt;Random Number
            (between 0 and 1): rand()&lt;/li&gt; &lt;li&gt;Modulus (x % y):
            mod(x,y)&lt;/li&gt; &lt;li&gt;Sum of k Numbers:
            sum(x,y,z...)&lt;/li&gt; &lt;li&gt;Binomial Coefficients: binom(n,
            i)&lt;/li&gt; &lt;li&gt;Number to String: str(x)&lt;/li&gt;
            &lt;/ul&gt; &lt;/p&gt; &lt;p&gt;Beside those operators and functions,
            this operator also supports the constants pi and e if this is
            indicated by the corresponding parameter (default: true). You can
            also use strings in formulas (for example in a conditioned
            if-formula) but the string values have to be enclosed in double
            quotes.&lt;/p&gt; &lt;p&gt;Please note that there are some
            restrictions for the usage of other macros. The values of used macros
            have to fulfill the following in order to let this operator work
            properly: &lt;ul&gt; &lt;li&gt;If the standard constants are usable,
            macro values with names like &amp;quot;e&amp;quot; or
            &amp;quot;pi&amp;quot; are not allowed.&lt;/li&gt; &lt;li&gt;Macro
            values with function or operator names are also not
            allowed.&lt;/li&gt; &lt;li&gt;Macro values containing parentheses are
            not allowed.&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt;
            &lt;p&gt;&lt;br/&gt;&lt;em&gt;Examples:&lt;/em&gt;&lt;br/&gt;
            17+sin(%{macro1}*%{macro2})&lt;br/&gt; if (%macro1}&gt;5,
            %{macro2}*%{macro3}, -abs(%{macro4}))&lt;br/&gt; &lt;/p&gt;</help>
    <key>generate_macro</key>
    <tags>
         <tag>Process</tag>
         <tag>Variable</tag>
         <tag>Setting</tag>
    </tags>
  </operator>
    <operator>
        <name>AdaBoost</name>
        <synopsis>Boosting operator allowing all learners (not restricted to
            Weka learners).</synopsis>
        <help>This AdaBoost implementation can be used with all learners
            available in RapidMiner, not only the ones which originally are part
            of the Weka package.</help>
    <key>adaboost</key>
    <tags>
         <tag>Boosting</tag>
         <tag>Ensembles</tag>
         <tag>Adaptive</tag>
         <tag>Freund</tag>
         <tag>Schapire</tag>
    </tags>
  </operator>
    <operator>
        <name>Logistic Regression (SVM)</name>
        <synopsis>MyKLRLearner provides an internal Java implementation of the
            myKLR by Stefan Rueping.</synopsis>
        <help>This is the Java implementation of &lt;em&gt;myKLR&lt;/em&gt; by
            Stefan R&amp;uuml;ping. myKLR is a tool for large scale kernel
            logistic regression based on the algorithm of Keerthi/etal/2003 and
            the code of mySVM.</help>
    <key>logistic_regression</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Model</tag>
         <tag>Glm</tag>
         <tag>Generalized</tag>
         <tag>Log-likelihood</tag>
         <tag>Loglikelihood</tag>
         <tag>Log-odds</tag>
         <tag>Logodds</tag>
    </tags>
  </operator>
    <operator>
        <name>Generate Data</name>
        <synopsis>Generates an example set based on numerical attributes.
        </synopsis>
        <help>Generates a random example set for testing purposes. Uses a
            subclass of &lt;i&gt;TargetFunction&lt;/i&gt; to create the examples
            from the attribute values. Possible target functions are: random, sum
            (of all attributes), polynomial (of the first three attributes,
            degree 3), non linear, sinus, sinus frequency (like sinus, but with
            frequencies in the argument), random classification, sum
            classification (like sum, but positive for positive sum and negative
            for negative sum), interaction classification (positive for negative
            x or positive y and negative z), sinus classification (positive for
            positive sinus values).</help>
    <key>generate_data</key>
    <tags>
         <tag>Create</tag>
         <tag>Datasets</tag>
         <tag>Samples</tag>
         <tag>Examples</tag>
         <tag>Example set</tag>
    </tags>
  </operator>
  <operator>
  	<name>Generate Data by User Specification</name>
  	<synopsis>Generates an ExampleSet containing exactly one example. The attributes and their types and roles are derived from the expressions
  	given by the parameters.</synopsis>
  	<key>generate_data_user_specification</key>
    <tags>
         <tag>Create</tag>
         <tag>Datasets</tag>
         <tag>Samples</tag>
         <tag>Examples</tag>
         <tag>Example set</tag>
    </tags>
  </operator>
    <operator>
        <name>Generate Aggregation</name>
        <synopsis>This operator constructs a new attribute by aggregating
            values of other attributes in every example.</synopsis>
        <help>Allows to generate a new attribute which consists of a function
            of several other attributes. As functions, several aggregation
            attributes are available.</help>
    <key>generate_aggregation</key>
    <tags>
         <tag>Aggregate</tag>
         <tag>Sum</tag>
         <tag>Count</tag>
         <tag>Min</tag>
         <tag>Max</tag>
         <tag>Average</tag>
         <tag>Avg</tag>
         <tag>Mean</tag>
    </tags>
  </operator>
    <operator>
        <name>Loop Data Fractions</name>
        <synopsis>Uses only a fraction of the data to apply the inner operator
            on it.</synopsis>
        <help>This operator works similar to the
            &lt;i&gt;LearningCurveOperator&lt;/i&gt;. In contrast to this, it
            just splits the ExampleSet according to the parameter "fraction" and
            learns a model only on the subset. It can be used, for example, in
            conjunction with
            &lt;i&gt;GridSearchParameterOptimizationOperator&lt;/i&gt; which sets
            the fraction parameter to values between 0 and 1. The advantage is,
            that this operator can then be used inside of a
            &lt;i&gt;XValidation&lt;/i&gt;, which delivers more stable result
            estimations.</help>
    <key>loop_data_fractions</key>
  </operator>
    <operator>
        <name>Optimize Parameters (Evolutionary)</name>
        <synopsis>This operator finds the optimal values for parameters using
            an evolutionary computation approach.</synopsis>
        <help>This operator finds the optimal values for a set of parameters
            using an evolutionary strategies approach which is often more
            appropriate than a grid search or a greedy search like the quadratic
            programming approach and leads to better results. The parameter
            &lt;var&gt;parameters&lt;/var&gt; is a list of key value pairs where
            the keys are of the form
            &lt;code&gt;operator_name.parameter_name&lt;/code&gt; and the value
            for each parameter must be a semicolon separated pair of a minimum
            and a maximum value in squared parantheses, e.g. [10;100] for a range
            of 10 until 100. &lt;br/&gt; The operator returns an optimal
            &lt;i&gt;ParameterSet&lt;/i&gt; which can as well be written to a
            file with a &lt;i&gt;ParameterSetWriter&lt;/i&gt;. This parameter set
            can be read in another process using a
            &lt;i&gt;ParameterSetLoader&lt;/i&gt;. &lt;br/&gt; The file format of
            the parameter set file is straightforward and can easily be generated
            by external applications. Each line is of the form
            &lt;center&gt;&lt;code&gt;operator_name.parameter_name =
            value&lt;/code&gt;&lt;/center&gt; &lt;br/&gt; Please refer to section
            &lt;i&gt;Advanced Processes/Parameter and performance
            analysis&lt;/i&gt; for an example application.</help>
    <key>optimize_parameters_evolutionary</key>
  </operator>
    <operator>
        <name>Decision Tree (Deprecated)</name>
        <synopsis>
    &lt;p&gt;
      Generates decision trees to classify nominal data.
    &lt;/p&gt;
  </synopsis>
        <help>
    &lt;p&gt;
      This operator learns decision trees from both nominal and numerical 
      data. Decision trees are powerful classification methods which often can 
      also easily be understood. In order to classify an example, the tree is 
      traversed bottom-down. Every node in a decision tree is labelled with an 
      attribute. The example's value for this attribute determines which of 
      the outcoming edges is taken. For nominal attributes, we have one 
      outgoing edge per possible attribute value, and for numerical attribtues 
      the outgoing edges are labelled with disjoint ranges.
    &lt;/p&gt;
    &lt;p&gt;
      
    &lt;/p&gt;
    &lt;p&gt;
      This decision tree learner works similar to Quinlan's C4.5 or CART. 
      Roughly speaking, the tree induction algorithm works as follows. 
      Whenever a new node is created at a certain stage, an attribute is 
      picked to maximise the discriminative power of that node with respect to 
      the examples assigned to the particular subtree. This discriminative 
      power is measured by a criterion which can be selected by the user 
      (information gain, gain ratio, gini index, etc.).
    &lt;/p&gt;
    &lt;p&gt;
      
    &lt;/p&gt;
    &lt;p&gt;
      The algorithm stops in various cases:
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        No attribute reaches a certain threshold (minimum_gain).
      &lt;/li&gt;
      &lt;li&gt;
        The maximal depth is reached.
      &lt;/li&gt;
      &lt;li&gt;
        There are less than a certain number of examples 
        (minimal_size_for_split) in the current subtree.
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      Finally, the tree is pruned, i.e. leaves that do not add to the 
      discriminative power of the whole tree are removed.
    &lt;/p&gt;
  </help>
    <key>decision_tree</key>
  </operator>
        
    <operator>
        <name>SimpleExampleSource</name>
        <synopsis>This operator reads an example set from file. It is a
            simpler version of the ExampleSource operator.</synopsis>
        <help>&lt;p&gt; This operator reads an example set from (a) file(s).
            Probably you can use the default parameter values for the most file
            formats (including the format produced by the ExampleSetWriter, CSV,
            ...). In fact, in many cases this operator is more appropriate for
            CSV based file formats than the &lt;i&gt;CSVExampleSource&lt;/i&gt;
            operator itself since you can better control some of the necessary
            settings like column separators etc. &lt;/p&gt; &lt;p&gt; In contrast
            to the usual ExampleSource operator this operator is able to read the
            attribute names from the first line of the data file. However, there
            is one restriction: the data can only be read from one file instead
            of multiple files. If you need a fully flexible operator for data
            loading you should use the more powerful ExampleSource operator which
            also provides more parameters tuning for example the quoting
            mechanism and other specialized settings. &lt;/p&gt; &lt;p&gt; The
            column split points can be defined with regular expressions (please
            refer to the annex of the RapidMiner tutorial). The default split
            parameter &amp;quot;,\s*|;\s*|\s+&amp;quot; should work for most file
            formats. This regular expression describes the following column
            separators &lt;ul&gt; &lt;li&gt;the character &amp;quot;,&amp;quot;
            followed by a whitespace of arbitrary length (also no white
            space)&lt;/li&gt; &lt;li&gt;the character &amp;quot;;&amp;quot;
            followed by a whitespace of arbitrary length (also no white
            space)&lt;/li&gt; &lt;li&gt;a whitespace of arbitrary length (min.
            1)&lt;/li&gt; &lt;/ul&gt; A logical XOR is defined by
            &amp;quot;|&amp;quot;. Other useful separators might be
            &amp;quot;\t&amp;quot; for tabulars, &amp;quot; &amp;quot; for a
            single whitespace, and &amp;quot;\s&amp;quot; for any whitespace.
            &lt;/p&gt; &lt;p&gt; Quoting is also possible with &amp;quot;.
            Escaping a quote is done with \&amp;quot;. Additionally you can
            specify comment characters which can be used at arbitrary locations
            of the data lines and will skip the remaining part of the lines.
            Unknown attribute values can be marked with empty strings or a
            question mark. &lt;/p&gt;</help>
    <key>simpleexamplesource</key>
  </operator>
    <operator>
        <name>MultiCriterionDecisionStump</name>
        <synopsis>A quick DecisionStump clone that allows to specify different
            utility functions.</synopsis>
        <help>A DecisionStump clone that allows to specify different utility
            functions. It is quick for nominal attributes, but does not yet apply
            pruning for continuos attributes. Currently it can only handle
            boolean class labels.</help>
    <key>multicriteriondecisionstump</key>
  </operator>
    <operator>
        <name>Transform Cases</name>
        <synopsis>Converts the characters in all terms to lower case.
        </synopsis>
        <help>Converts the characters in all terms to lower case.</help>
    <key>transform_cases</key>
  </operator>
    <operator>
        <name>Naive Bayes (Kernel)</name>
        <synopsis>Returns classification model using estimated kernel
            densities.</synopsis>
        <help>Kernel Naive Bayes learner.</help>
    <key>naive_bayes_kernel</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Model</tag>
         <tag>Distribution</tag>
         <tag>Gaussian</tag>
         <tag>Multinominal</tag>
         <tag>Likelihoods</tag>
         <tag>Probability</tag>
         <tag>Probabilities</tag>
    </tags>
  </operator>
    <operator>
        <name>Default Model</name>
        <synopsis>Learns a default value.</synopsis>
        <help>This learner creates a model, that will simply predict a default
            value for all examples, i.e. the average or median of the true labels
            (or the mode in case of classification) or a fixed specified value.
            This learner can be used to compare the results of
            &amp;quot;real&amp;quot; learning schemes with guessing.</help>
    <key>default_model</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Regression</tag>
         <tag>Lazy</tag>
         <tag>Average</tag>
         <tag>Mean</tag>
         <tag>Mode</tag>
    </tags>
  </operator>
    <operator>
        <name>Sample (Model-Based)</name>
        <synopsis>Creates a sample from an example set. The sampling is based
            on a model and is constructed to focus on examples not yet explained.
        </synopsis>
        <help>Sampling based on a learned model.</help>
    <key>sample_model_based</key>
  </operator>
    <operator>
        <name>ExampleVisualizer</name>
        <synopsis>Allows the visualization of examples (attribute values) in
            the plot view of an example set (double click on data point).
        </synopsis>
        <help>Remembers the given example set and uses the ids provided by
            this set for the query for the corresponding example and the creation
            of a generic example visualizer. This visualizer simply displays the
            attribute values of the example. Adding this operator is often
            necessary to enable the visualization of single examples in the
            provided plotter components.</help>
    <key>examplevisualizer</key>
  </operator>
    <operator>
        <name>Correlation Matrix</name>
        <synopsis>Determines the correlation between all attributes and can
            produce a weight vector based on correlations.</synopsis>
        <help>&lt;p&gt;This operator calculates the correlation matrix between
            all attributes of the input example set. Furthermore, attribute
            weights based on the correlations can be returned. This allows the
            de-selection of highly correlated attributes with the help of an
            &lt;i&gt;AttributeWeightSelection&lt;/i&gt; operator. If no weights
            should be created, this operator produces simply a correlation matrix
            which up to now cannot be used by other operators but can be
            displayed to the user in the result tab.&lt;/p&gt; &lt;p&gt;Please
            note that this simple implementation performs a data scan for each
            attribute combination and might therefore take some time for
            non-memory example tables.&lt;/p&gt;</help>
    <key>correlation_matrix</key>
    <tags>
         <tag>Relationships</tag>
         <tag>Associations</tag>
         <tag>Dependencies</tag>
         <tag>Dependences</tag>
         <tag>Pearson</tag>
         <tag>Coefficients</tag>
    </tags>
  </operator>
    <operator>
        <name>Optimize by Generation (Evolutionary Aggregation)</name>
        <synopsis>A generating genetic algorithm for unsupervised learning
            (experimental).</synopsis>
        <help>Performs an evolutionary feature aggregation. Each base feature
            is only allowed to be used as base feature, in one merged feature, or
            it may not be used at all.</help>
    <key>optimize_by_generation_evolutionary_aggregation</key>
    <shortName>Generate</shortName>
  </operator>
    <operator>
        <name>Discretize by Entropy</name>
        <synopsis>Discretizes numerical attributes. Bin boundaries are chosen
            as to minimize the entropy in the induced partitions.</synopsis>
        <help>&lt;p&gt;This operator discretizes all numeric attributes in the
            dataset into nominal attributes. The discretization is performed by
            selecting a bin boundary minimizing the entropy in the induced
            partitions. The method is then applied recursively for both new
            partitions until the stopping criterion is reached. For Details see
            a) Multi-interval discretization of continued-values attributes for
            classification learning (Fayyad,Irani) and b) Supervised and
            Unsupervised Discretization (Dougherty,Kohavi,Sahami). Skips all
            special attributes including the label.&lt;/p&gt; &lt;p&gt; Please
            note that this operator automatically removes all attributes with
            only one range (i.e. those attributes which are not actually
            discretized since the entropy criterion is not fulfilled). This
            behavior can be controlled by the remove_useless parameter.
            &lt;/p&gt;</help>
    <key>discretize_by_entropy</key>
    <shortName>Discretize</shortName>
  </operator>
    <operator>
        <name>Top Down Clustering</name>
        <synopsis>Hierarchical clustering by applying an inner flat clusterer
            scheme recursively</synopsis>
        <help>A top-down generic clustering that can be used with any (flat)
            clustering as inner operator. Note though, that the outer operator
            cannot set or get the maximal number of clusters, the inner operator
            produces. These value has to be set in the inner operator. This
            operator will create a cluster attribute if not present yet.</help>
    <key>top_down_clustering</key>
    <shortName>Clustering</shortName>
  </operator>
    <operator>
        <name>Work on Subset</name>
        <synopsis>Selects one attribute (or a subset) via a regular expression
            and applies its inner operators to the resulting subset.</synopsis>
        <help>&lt;p&gt;This operator can be used to select one attribute (or a
            subset) by defining a regular expression for the attribute name and
            applies its inner operators to the resulting subset. Please note that
            this operator will also use special attributes which makes it
            necessary for all preprocessing steps which should be performed on
            special attributes (and are normally not performed on special
            attributes).&lt;/p&gt; &lt;p&gt;This operator is also able to deliver
            the additional results of the inner operator if desired.&lt;/p&gt;
            &lt;p&gt;Afterwards, the remaining original attributes are added to
            the resulting example set if the parameter
            &amp;quot;keep_subset_only&amp;quot; is set to false
            (default).&lt;/p&gt; &lt;p&gt;Please note that this operator is very
            powerful and can be used to create new preprocessing schemes by
            combinating it with other preprocessing operators. Hoewever, there
            are two major restrictions (among some others): first, since the
            inner result will be combined with the rest of the input example set,
            the number of examples (data points) is not allowed to be changed
            inside of the subset preprocessing. Second, attribute role changes
            will not be delivered to the outside since internally all special
            attributes will be changed to regular for the inner operators and
            role changes can afterwards not be delivered.&lt;/p&gt;</help>
    <key>work_on_subset</key>
  </operator>
    <operator>
        <name>Weights to Data</name>
        <synopsis>This operator simply creates a new data set where for each
            attribute the corresponding weight is stored.</synopsis>
        <help>This operator creates a new example set from the given attribute
            weights. The example set will have two columns, the name of the
            attribute and the weight for this attribute. It will have as many
            rows as are described by the give attribute weights.</help>
    <key>weights_to_data</key>
    <tags>
         <tag>Weighting</tag>
         <tag>Importance</tag>
         <tag>Influence</tag>
         <tag>Significance</tag>
         <tag>Factors</tag>
         <tag>Relevance</tag>
    </tags>
  </operator>
    <operator>
        <name>Single Rule Induction (Single Attribute)</name>
        <synopsis>Learns a single rule using only one attribute.</synopsis>
        <help>This operator concentrates on one single attribute and
            determines the best splitting terms for minimizing the training
            error. The result will be a single rule containing all these terms.
        </help>
    <key>single_rule_induction_single_attribute</key>
  </operator>
    <operator>
        <name>FeatureGeneration</name>
        <synopsis>This operator constructs new user defined attributes from
            mathematical expressions.</synopsis>
        <help>&lt;p&gt;This operator constructs new attributes from the
            attributes of the input example set. The names of the new attributes
            and their construction description are defined in the parameter list
            &amp;quot;functions&amp;quot;.&lt;/p&gt; &lt;p&gt;The following
            &lt;em&gt;operators&lt;/em&gt; are supported: &lt;ul&gt;
            &lt;li&gt;Addition: +&lt;/li&gt; &lt;li&gt;Subtraction: -&lt;/li&gt;
            &lt;li&gt;Multiplication: *&lt;/li&gt; &lt;li&gt;Division:
            /&lt;/li&gt; &lt;li&gt;Power: ^&lt;/li&gt; &lt;li&gt;Modulus:
            %&lt;/li&gt; &lt;li&gt;Less Than: &amp;lt;&lt;/li&gt;
            &lt;li&gt;Greater Than: &amp;gt;&lt;/li&gt; &lt;li&gt;Less or Equal:
            &amp;lt;=&lt;/li&gt; &lt;li&gt;More or Equal: &amp;gt;=&lt;/li&gt;
            &lt;li&gt;Equal: ==&lt;/li&gt; &lt;li&gt;Not Equal: !=&lt;/li&gt;
            &lt;li&gt;Boolean Not: !&lt;/li&gt; &lt;li&gt;Boolean And: two ampers
            and&lt;/li&gt; &lt;li&gt;Boolean Or: ||&lt;/li&gt; &lt;/ul&gt;
            &lt;/p&gt; &lt;p&gt;The following &lt;em&gt;log and exponential
            functions&lt;/em&gt; are supported: &lt;ul&gt; &lt;li&gt;Natural
            Logarithm: ln(x)&lt;/li&gt; &lt;li&gt;Logarithm Base 10:
            log(x)&lt;/li&gt; &lt;li&gt;Logarithm Dualis (Base 2):
            ld(x)&lt;/li&gt; &lt;li&gt;Exponential (e^x): exp(x)&lt;/li&gt;
            &lt;li&gt;Power: pow(x,y)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt;
            &lt;p&gt;The following &lt;em&gt;trigonometric functions&lt;/em&gt;
            are supported: &lt;ul&gt; &lt;li&gt;Sine: sin(x)&lt;/li&gt;
            &lt;li&gt;Cosine: cos(x)&lt;/li&gt; &lt;li&gt;Tangent:
            tan(x)&lt;/li&gt; &lt;li&gt;Arc Sine: asin(x)&lt;/li&gt;
            &lt;li&gt;Arc Cosine: acos(x)&lt;/li&gt; &lt;li&gt;Arc Tangent:
            atan(x)&lt;/li&gt; &lt;li&gt;Arc Tangent (with 2 parameters):
            atan2(x,y)&lt;/li&gt; &lt;li&gt;Hyperbolic Sine: sinh(x)&lt;/li&gt;
            &lt;li&gt;Hyperbolic Cosine: cosh(x)&lt;/li&gt; &lt;li&gt;Hyperbolic
            Tangent: tanh(x)&lt;/li&gt; &lt;li&gt;Inverse Hyperbolic Sine:
            asinh(x)&lt;/li&gt; &lt;li&gt;Inverse Hyperbolic Cosine:
            acosh(x)&lt;/li&gt; &lt;li&gt;Inverse Hyperbolic Tangent:
            atanh(x)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt; &lt;p&gt;The following
            &lt;em&gt;statistical functions&lt;/em&gt; are supported: &lt;ul&gt;
            &lt;li&gt;Round: round(x)&lt;/li&gt; &lt;li&gt;Round to p decimals:
            round(x,p)&lt;/li&gt; &lt;li&gt;Floor: floor(x)&lt;/li&gt;
            &lt;li&gt;Ceiling: ceil(x)&lt;/li&gt; &lt;li&gt;Average:
            avg(x,y,z...)&lt;/li&gt; &lt;li&gt;Minimum: min(x,y,z...)&lt;/li&gt;
            &lt;li&gt;Maximum: max(x,y,z...)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt;
            &lt;p&gt;The following &lt;em&gt;miscellaneous functions&lt;/em&gt;
            are supported: &lt;ul&gt; &lt;li&gt;If-Then-Else:
            if(cond,true-evaluation, false-evaluation)&lt;/li&gt;
            &lt;li&gt;Absolute: abs(x)&lt;/li&gt; &lt;li&gt;Square Root:
            sqrt(x)&lt;/li&gt; &lt;li&gt;Signum (delivers the sign of a number):
            sgn(x)&lt;/li&gt; &lt;li&gt;Random Number (between 0 and 1):
            rand()&lt;/li&gt; &lt;li&gt;Modulus (x % y): mod(x,y)&lt;/li&gt;
            &lt;li&gt;Sum of k Numbers: sum(x,y,z...)&lt;/li&gt;
            &lt;li&gt;Binomial Coefficients: binom(n, i)&lt;/li&gt;
            &lt;li&gt;Number to String: str(x)&lt;/li&gt; &lt;li&gt;String to
            Number: parse(x)&lt;/li&gt; &lt;li&gt;Substring: cut(x, start,
            len)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt; &lt;p&gt;The following
            &lt;em&gt;process related functions&lt;/em&gt; are supported:
            &lt;ul&gt; &lt;li&gt;Retrieving a parameter value: param("operator",
            "parameter")&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt; &lt;p&gt;Beside those
            operations and functions, this operator also supports the constants
            pi and e if this is indicated by the corresponding parameter
            (default: true). You can also use strings in formulas (for example in
            a conditioned if-formula) but the string values have to be enclosed
            in double quotes.&lt;/p&gt; &lt;p&gt;Please note that there are some
            restrictions for the attribute names in order to let this operator
            work properly: &lt;ul&gt; &lt;li&gt;If the standard constants are
            usable, attribute names with names like &amp;quot;e&amp;quot; or
            &amp;quot;pi&amp;quot; are not allowed.&lt;/li&gt;
            &lt;li&gt;Attribute names with function or operator names are also
            not allowed.&lt;/li&gt; &lt;li&gt;Attribute names containing
            parentheses are not allowed.&lt;/li&gt; &lt;/ul&gt; If these
            conditions are not fulfilled, the names must be changed beforehand,
            for example with the &lt;i&gt;ChangeAttributeName&lt;/i&gt; operator.
            &lt;/p&gt;
            &lt;p&gt;&lt;br/&gt;&lt;em&gt;Examples:&lt;/em&gt;&lt;br/&gt;
            a1+sin(a2*a3)&lt;br/&gt; if (att1&gt;5, att2*att3,
            -abs(att1))&lt;br/&gt; &lt;/p&gt;</help>
    </operator>
    <operator>
        <name>Decision Tree (Weight-Based)</name>
        <synopsis>Learns a pruned decision tree based on an arbitrary feature
            relevance test (attribute weighting scheme as inner operator).
        </synopsis>
        <help>Learns a pruned decision tree based on arbitrary feature
            relevance measurements defined by an inner operator (use for example
            &lt;i&gt;InfoGainRatioWeighting&lt;/i&gt; for C4.5 and
            &lt;i&gt;ChiSquaredWeighting&lt;/i&gt; for CHAID. Works only for
            nominal attributes.</help>
    <key>decision_tree_weight_based</key>
  </operator>
    <operator>
        <name>Stacking</name>
        <synopsis>Uses the first inner learner to build a stacked model on top
            of the predictions of the other inner learners.</synopsis>
        <help>This class uses n+1 inner learners and generates n different
            models by using the last n learners. The predictions of these n
            models are taken to create n new features for the example set, which
            is finally used to serve as an input of the first inner learner.
        </help>
    <key>stacking</key>
  </operator>
    <operator>
        <name>Wrapper-X-Validation</name>
        <synopsis>Encapsulates a cross-validation to evaluate a feature
            weighting or selection method (wrapper).</synopsis>
        <help>This operator evaluates the performance of feature weighting and
            selection algorithms. The first subprocess contains the algorithm to
            be evaluated itself. It must return an attribute weights vector which
            is then applied on the test data. The same fold
            &lt;i&gt;XValidation&lt;/i&gt; of the data is used to create a new
            model during the second subprocess. This model is evaluated in the
            third subprocess which hence has to return a performance vector. This
            performance vector serves as a performance indicator for the actual
            algorithm. This implementation of a MethodValidationChain works
            similar to the &lt;i&gt;XValidation&lt;/i&gt;.</help>
    <key>wrapper_x_validation</key>
    <shortName>Validation</shortName>
  </operator>
    <operator>
        <name>WeightedBootstrapping</name>
        <synopsis>Creates a bootstrapped sample by weighted sampling with
            replacement.</synopsis>
        <help>This operator constructs a bootstrapped sample from the given
            example set which must provide a weight attribute. If no weight
            attribute was provided this operator will stop the process with an
            error message. See the operator &lt;i&gt;Bootstrapping&lt;/i&gt; for
            more information.</help>
    <key>weightedbootstrapping</key>
  </operator>
    <operator>
        <name>Independent Component Analysis</name>
        <synopsis>Performs an independent component analysis (ICA).</synopsis>
        <help>This operator performs the independent componente analysis
            (ICA). Implementation of the FastICA-algorithm of Hyvaerinen und Oja.
            The operator outputs a &lt;code&gt;FastICAModel&lt;/code&gt;. With
            the &lt;code&gt;ModelApplier&lt;/code&gt; you can transform the
            features.</help>
    <key>independent_component_analysis</key>
    <shortName>ICA</shortName>
  </operator>
    <operator>
        <name>URLReader</name>
        <synopsis>This operator reads an example set from a URL. It allows
            only a fixed data format but on the other hand is able to read data
            from arbitrary sources.</synopsis>
        <help>&lt;p&gt; This operator reads an example set from an URL. The
            format has to be a CSV format with ';' as column separator and
            nominal values have to be quoted with a double quote (&amp;quot;). A
            quote inside of a nominal value has to be escaped by a backslash like
            in \&amp;quot;. The first row is allowed to contain the column names
            which has to be indicated by the corresponding parameter. Comments
            are not allowed, unknown attribute values can be marked with empty
            strings or a question mark. &lt;/p&gt; &lt;p&gt; This operator is not
            nearly as powerful as the operators ExampleSource or
            SimpleExampleSource but is on the other hand able to read data from
            arbitrary places as long as the format fits the specification above.
            Please note also that the usage of this operator hardly allows for a
            correct meta data description which might lead to problems if the
            meta data between training and test set differ in a learning
            scenario. &lt;/p&gt; &lt;p&gt; Attribute roles can not be directly
            set during loading but the operator ChangeAttributeRole has to be
            used after loading in order to change the roles. &lt;/p&gt;</help>
    </operator>
    <operator>
        <name>Loop Clusters</name>
        <synopsis>Applies all inner operators to all clusters.</synopsis>
        <help>This operator splits up the input example set according to the
            clusters and applies its inner operators
            &lt;var&gt;number_of_clusters&lt;/var&gt; time on copies of its own
            input. This requires the example set to have a special cluster
            attribute which can be either created by a
            &lt;i&gt;AbstractClusterer&lt;/i&gt; or might be declared in the
            attribute description file that was used when the data was loaded.
        </help>
    <key>loop_clusters</key>
  </operator>
    <operator>
        <name>Map</name>
        <synopsis>Maps certain values of an attribute to other values.
        </synopsis>
        <help>&lt;p&gt;This operator takes an
            &lt;code&gt;ExampleSet&lt;/code&gt; as input and maps the values of
            certain attributes to other values. The operator can replace nominal
            values (e.g. replace the value &amp;quot;green&amp;quot; by the value
            &amp;quot;green_color&amp;quot;) as well as numerical values (e.g.
            replace the all values &amp;quot;3&amp;quot; by
            &amp;quot;-1&amp;quot;). A single mapping can be specified using the
            parameters &lt;code&gt;replace_what&lt;/code&gt; and
            &lt;code&gt;replace_by&lt;/code&gt;. Multiple mappings can be
            specified in the parameter list
            &lt;code&gt;value_mappings&lt;/code&gt;.&lt;/p&gt;
            &lt;p&gt;Additionally, the operator allows to define (and consider) a
            default mapping. If &lt;code&gt;add_default_mapping&lt;/code&gt; is
            set to true and &lt;code&gt;default_value&lt;/code&gt; is properly
            set, all values that occur in the example set but are not listed in
            the value mappings list are replaced by the default value. This may
            be helpful in cases where only some values should be mapped
            explicitly and many unimportant values should be mapped to a default
            value (e.g. "other").&lt;/p&gt; &lt;p&gt;If the parameter
            &lt;code&gt;consider_regular_expressions&lt;/code&gt; is enabled, the
            values are replaced by the new values if the original values match
            the given regular expressions. The value corresponding to the first
            matching regular expression in the mappings list is taken as
            replacement.&lt;/p&gt; &lt;p&gt;This operator supports regular
            expressions for the attribute names, i.e. the value mapping is
            applied on all attributes for which the name fulfills the pattern
            defined by the name expression.&lt;/p&gt;</help>
    <key>map</key>
    <tags>
         <tag>Replace</tag>
         <tag>Change</tag>
    </tags>
  </operator>
    <operator>
        <name>Performance</name>
        <synopsis>This operator delivers a list of performance
            values automatically determined in order to fit the learning task
            type.</synopsis>
        <help>&lt;p&gt;
            In contrast to the other performance evaluation
            methods like for example &lt;a href="rm://opdoc/performance_classification"&gt;Performance (Classification)&lt;/a&gt;, &lt;a href="rm://opdoc/performance_binominal_classification"&gt;Performance (Binominal Classification)&lt;/a&gt; or &lt;a href="rm://opdoc/performance_regression"&gt;Performance (Regression)&lt;/a&gt;, this operator can be used for all
            types of learning tasks. It will automatically determine the learning
            task type and will calculate the most common criteria for this type.&lt;br&gt;
            For more sophisticated performance calculations, you should use the operators mentioned above. If none of them suits your need, you might write your own performance measure and 
            calculate it with &lt;a href="rm://opdoc/performance_user_based"&gt;Performance (User-Based)&lt;/a&gt;.&lt;/p&gt;
            
            &lt;p&gt;&lt;br/&gt;This operator expects a test &lt;i&gt;ExampleSet&lt;/i&gt; as
            input, containing one attribute with the role &lt;i&gt;label&lt;/i&gt; and one with the role &lt;i&gt;prediction&lt;/i&gt;. See the &lt;a href="rm://opdoc/set_role"&gt;Set Role&lt;/a&gt; operator for more details. 
            On the basis of this two attributes a &lt;i&gt;PerformanceVector&lt;/i&gt; is calculated, containing the values of the performance criteria. If a &lt;i&gt;PerformanceVector&lt;/i&gt; was fed into &lt;i&gt;performance&lt;/i&gt; input, it's values are kept if it does not already contain the new criteria. Otherwise the values are averaged over the old and the new values.
            &lt;/p&gt;
            
            &lt;p&gt;&lt;br/&gt;
            The following criterias are added for binominal classification tasks:
            &lt;ul&gt;
              &lt;li&gt;Accuracy&lt;/li&gt;
              &lt;li&gt;Precision&lt;/li&gt;
              &lt;li&gt;Recall&lt;/li&gt;
              &lt;li&gt;AUC (optimistic)&lt;/li&gt;
              &lt;li&gt;AUC (neutral)&lt;/li&gt;
              &lt;li&gt;AUC (pessimistic)&lt;/li&gt;
            &lt;/ul&gt;
            The following criterias are added for polynominal classification tasks:
            &lt;ul&gt;
              &lt;li&gt;Accuracy&lt;/li&gt;
              &lt;li&gt;Kappa statistic&lt;/li&gt;
            &lt;/ul&gt;
            The following criterias are added for regression tasks:
            &lt;ul&gt;
              &lt;li&gt;Root Mean Squared Error&lt;/li&gt;
              &lt;li&gt;Mean Squared Error&lt;/li&gt;
            &lt;/ul&gt;
            
            &lt;/p&gt;</help>
    <key>performance</key>
    <tags>
         <tag>Rmse</tag>
         <tag>Accuracy</tag>
         <tag>Errors</tag>
         <tag>Precision</tag>
         <tag>Recall</tag>
         <tag>Roc</tag>
         <tag>Auc</tag>
         <tag>Validations</tag>
         <tag>Evaluations</tag>
         <tag>Metrics</tag>
    </tags>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>Performance to Data</name>
        <synopsis>This operator is used to convert a performance vector as produced by a Performance operator into an example set.</synopsis>
        <help>The operator creates an example set which contains one row for each performance criterion in the input data and a set of columns: the Criterion column contains the name of the criterion, Example Count is the number of examples that were used to calculate the criterion, whereas Value, Standard Deviation and Variance list the value and the statistical properties of the value.</help>
    <key>performance_to_data</key>
    <shortName>Performance to Data</shortName>
  </operator>
    <operator>
        <name>Optimize by Generation (AGA)</name>
        <synopsis>Another (improved) genetic algorithm for feature selection
            and feature generation (AGA).</synopsis>
        <help>Basically the same operator as the
            &lt;i&gt;GeneratingGeneticAlgorithm&lt;/i&gt; operator. This version
            adds additional generators and improves the simple GGA approach by
            providing some basic intron prevention techniques. In general, this
            operator seems to work better than the original approach but
            frequently deliver inferior results compared to the operator
            &lt;i&gt;YAGGA2&lt;/i&gt;.</help>
    <key>optimize_by_generation_aga</key>
    <shortName>Generate</shortName>
  </operator>
    <operator>
        <name>Remove Useless Attributes</name>
        <synopsis>Remove all useless attributes from an example set.
        </synopsis>
        <help>
            Removes useless attribute from the example set. Useless attributes are

            &lt;ul&gt;
            &lt;li&gt;
            nominal attributes which most frequent value is contained in more than
            nominal_useless_above percent of all examples.
            &lt;/li&gt;
            &lt;/ul&gt;
            &lt;ul&gt;
            &lt;li&gt;
            nominal attributes which most frequent value is contained in less than
            nominal_useless_below percent of all examples.
            &lt;/li&gt;
            &lt;li&gt;
            numerical attributes which standard deviation is less or equal to a
            given deviation threshold &lt;code&gt;t&lt;/code&gt;.
            &lt;/li&gt;
            &lt;/ul&gt; </help>
    <key>remove_useless_attributes</key>
    <tags>
         <tag>Filter</tag>
         <tag>Keep</tag>
         <tag>Remove</tag>
         <tag>Drop</tag>
         <tag>Delete</tag>
         <tag>Columns</tag>
         <tag>Variables</tag>
         <tag>Features</tag>
         <tag>Feature Set</tag>
         <tag>Constant</tag>
         <tag>Deviation</tag>
         <tag>Variance</tag>
    </tags>
  </operator>
    <operator>
        <name>Filter Stopwords (German)</name>
        <synopsis>Standard stopwords list for german texts.</synopsis>
        <help>A standard stopword operator for German texts, which removes
            every token equal to a stopword.</help>
    <key>filter_stopwords_german</key>
  </operator>
    <operator>
        <name>Series2WindowExamples</name>
        <synopsis>Creates examples from an univariate value series data by
            windowing and using a label value with respect to a user defined
            prediction horizon.</synopsis>
        <help>&lt;p&gt;This operator transforms a given example set containing
            series data into a new example set containing single valued examples.
            For this purpose, windows with a specified window and step size are
            moved across the series and the series value lying horizon values
            after the window end is used as label which should be predicted. This
            operator can only be used for univariate series prediction. For the
            multivariate case, please use the operator
            &lt;i&gt;MultivariateSeries2WindowExamples&lt;/i&gt;.&lt;/p&gt;
            &lt;p&gt; The series data must be given as ExampleSet. The parameter
            &amp;quot;series_representation&amp;quot; defines how the series data
            is represented by the ExampleSet:&lt;/p&gt; &lt;ul&gt;
            &lt;li&gt;encode_series_by_examples&lt;/li&gt;: the series index
            variable (e.g. time) is encoded by the examples, i.e. there is a
            &lt;em&gt;single&lt;/em&gt; attribute and a set of examples. Each
            example encodes the value for a new time point.
            &lt;li&gt;encode_series_by_attributes&lt;/li&gt;: the series index
            variable (e.g. time) is encoded by the attributes, i.e. there is a
            (set of) examples and a set of attributes. Each attribute value
            encodes the value for a new time point. If there is more than one
            example, the windowing is performed for each example independently
            and all resulting window examples are merged into a complete example
            set. &lt;/ul&gt; &lt;p&gt;Please note that the encoding as examples
            is usually more efficient with respect to the memory usage. To ensure
            backward compatibility, the default representation is, however, set
            to time_as_attributes.&lt;/p&gt;</help>
    <key>series2windowexamples</key>
  </operator>
    <operator>
        <name>Weight by Rule</name>
        <synopsis>This operator measures the relevance of features by
            constructing a single rule for each attribute and calculating the
            errors.</synopsis>
        <help>This operator calculates the relevance of a feature by computing
            the error rate of a OneR Model on the exampleSet without this
            feature.</help>
    <key>weight_by_rule</key>
  </operator>
    <operator>
        <name>Wrapper Split Validation</name>
        <synopsis>A simple validation method to check the performance of a
            feature weighting or selection wrapper.</synopsis>
        <help>This operator evaluates the performance of feature weighting
            algorithms including feature selection. The first inner operator is
            the weighting algorithm to be evaluated itself. It must return an
            attribute weights vector which is applied on the data. Then a new
            model is created using the second inner operator and a performance is
            retrieved using the third inner operator. This performance vector
            serves as a performance indicator for the actual algorithm. This
            implementation is described for the
            &lt;i&gt;RandomSplitValidationChain&lt;/i&gt;.</help>
    <key>wrapper_split_validation</key>
    <shortName>Validation</shortName>
  </operator>
    <operator>
        <name>Split File by Point</name>
        <synopsis>Segments documents by defining the splitting point.
        </synopsis>
        <help>Operator that allows to extract segments from a set of text
            documents in a directory based on a splitting the single documents
            into parts. The split point is described by a regular expression.
        </help>
    <key>split_file_by_point</key>
  </operator>
    
    <operator>
        <name>Agglomerative Clustering</name>
        <synopsis>Agglomerative buttom-up clustering</synopsis>
        <help>This operator implements agglomerative clustering, providing the
            three different strategies SingleLink, CompleteLink and AverageLink.
            The last is also called UPGMA. The result will be a hierarchical
            cluster model, providing distance information to plot as a dendogram.
        </help>
    <key>agglomerative_clustering</key>
    <tags>
         <tag>Unsupervised</tag>
         <tag>Clustering</tag>
         <tag>Segmentation</tag>
         <tag>Grouping</tag>
         <tag>Similarity</tag>
         <tag>Similarities</tag>
         <tag>Euclidean</tag>
         <tag>Distances</tag>
         <tag>Hierarchical</tag>
         <tag>Bottom-up</tag>
         <tag>Bottomup</tag>
         <tag>Dendrogram</tag>
    </tags>
    <shortName>Clustering</shortName>
  </operator>
    <operator>
        <name>Generate Attributes</name>
        <synopsis>This operator constructs new user defined attributes from
            mathematical expressions.</synopsis>
        <help>
            &lt;p&gt;This operator constructs new attributes from the
            attributes of the input &lt;i&gt;ExampleSet&lt;/i&gt; and arbitrary constants. The names of the new attributes
            and their construction description are defined in the parameter
            &lt;b&gt;functions&lt;/b&gt;. The attribute names might be used as variables in the construction description. When the descriptions are evaluated on each single example during application of this operator, these variables will be filled with the example's attribute weights. &lt;/p&gt;
            
            &lt;p&gt;Please note that there are some
            restrictions for the attribute names in order to let this operator
            work properly: &lt;ul&gt;
            &lt;li&gt;Attribute names containing
            parentheses are not allowed.&lt;/li&gt;
            &lt;li&gt;Attribute names containing blanks are not allowed&lt;/li&gt;
            &lt;li&gt;Attribute names with function or operator names are also
            not allowed.&lt;/li&gt; 
            &lt;li&gt;If the standard constants (see below) are
            usable, attribute names with names like &quot;e&quot; or
            &quot;pi&quot; are not allowed.&lt;/li&gt;
            &lt;/ul&gt; If these
            conditions are not fulfilled, the names must be changed beforehand,
            for example with the &lt;a href="rm://opdoc/rename"&gt;Rename&lt;/a&gt; operator. When replacing several attributes following a certain schema, the &lt;a href="rm://opdoc/rename_by_replacing"&gt;Rename by Replacing&lt;/a&gt; migth prove useful.
            &lt;br /&gt;
            &lt;/p&gt;
            &lt;br /&gt;
            
            &lt;h4&gt;Supported Expressions&lt;/h4&gt;
            &lt;p&gt;The following
            &lt;em&gt;operations&lt;/em&gt; are supported: &lt;ul&gt;
            &lt;li&gt;Addition: +&lt;/li&gt; &lt;li&gt;Subtraction: -&lt;/li&gt;
            &lt;li&gt;Multiplication: *&lt;/li&gt; &lt;li&gt;Division:
            /&lt;/li&gt; &lt;li&gt;Power: ^&lt;/li&gt; &lt;li&gt;Modulus:
            %&lt;/li&gt; &lt;li&gt;Less Than: &lt;&lt;/li&gt;
            &lt;li&gt;Greater Than: &gt;&lt;/li&gt; &lt;li&gt;Less or Equal:
            &lt;=&lt;/li&gt; &lt;li&gt;More or Equal: &gt;=&lt;/li&gt;
            &lt;li&gt;Equal: ==&lt;/li&gt; &lt;li&gt;Not Equal: !=&lt;/li&gt;
            &lt;li&gt;Boolean Not: !&lt;/li&gt; &lt;li&gt;Boolean And: two ampers
            and&lt;/li&gt; &lt;li&gt;Boolean Or: ||&lt;/li&gt; &lt;/ul&gt;
            &lt;/p&gt; &lt;p&gt;The following &lt;em&gt;log and exponential
            functions&lt;/em&gt; are supported: &lt;ul&gt; &lt;li&gt;Natural
            Logarithm: ln(x)&lt;/li&gt; &lt;li&gt;Logarithm Base 10:
            log(x)&lt;/li&gt; &lt;li&gt;Logarithm Dualis (Base 2):
            ld(x)&lt;/li&gt; &lt;li&gt;Exponential (e^x): exp(x)&lt;/li&gt;
            &lt;li&gt;Power: pow(x,y)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt;
            &lt;p&gt;The following &lt;em&gt;trigonometric functions&lt;/em&gt;
            are supported: &lt;ul&gt; &lt;li&gt;Sine: sin(x)&lt;/li&gt;
            &lt;li&gt;Cosine: cos(x)&lt;/li&gt; &lt;li&gt;Tangent:
            tan(x)&lt;/li&gt; &lt;li&gt;Arc Sine: asin(x)&lt;/li&gt;
            &lt;li&gt;Arc Cosine: acos(x)&lt;/li&gt; &lt;li&gt;Arc Tangent:
            atan(x)&lt;/li&gt; &lt;li&gt;Arc Tangent (with 2 parameters):
            atan2(x,y)&lt;/li&gt; &lt;li&gt;Hyperbolic Sine: sinh(x)&lt;/li&gt;
            &lt;li&gt;Hyperbolic Cosine: cosh(x)&lt;/li&gt; &lt;li&gt;Hyperbolic
            Tangent: tanh(x)&lt;/li&gt; &lt;li&gt;Inverse Hyperbolic Sine:
            asinh(x)&lt;/li&gt; &lt;li&gt;Inverse Hyperbolic Cosine:
            acosh(x)&lt;/li&gt; &lt;li&gt;Inverse Hyperbolic Tangent:
            atanh(x)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt; &lt;p&gt;The following
            &lt;em&gt;statistical functions&lt;/em&gt; are supported: &lt;ul&gt;
            &lt;li&gt;Round: round(x)&lt;/li&gt; &lt;li&gt;Round to p decimals:
            round(x,p)&lt;/li&gt; &lt;li&gt;Floor: floor(x)&lt;/li&gt;
            &lt;li&gt;Ceiling: ceil(x)&lt;/li&gt; &lt;li&gt;Average:
            avg(x,y,z...)&lt;/li&gt; &lt;li&gt;Minimum: min(x,y,z...)&lt;/li&gt;
            &lt;li&gt;Maximum: max(x,y,z...)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt;
            &lt;p&gt;The following &lt;em&gt;miscellaneous functions&lt;/em&gt;
            are supported: &lt;ul&gt; &lt;li&gt;If-Then-Else:
            if(cond,true-evaluation, false-evaluation)&lt;/li&gt;
            &lt;li&gt;Absolute: abs(x)&lt;/li&gt; &lt;li&gt;Square Root:
            sqrt(x)&lt;/li&gt; &lt;li&gt;Signum (delivers the sign of a number):
            sgn(x)&lt;/li&gt; &lt;li&gt;Random Number (between 0 and 1):
            rand()&lt;/li&gt; &lt;li&gt;Modulus (x % y): mod(x,y)&lt;/li&gt;
            &lt;li&gt;Sum of k Numbers: sum(x,y,z...)&lt;/li&gt;
            &lt;li&gt;Binomial Coefficients: binom(n, i)&lt;/li&gt;
            &lt;li&gt;Number to String: str(x)&lt;/li&gt; &lt;li&gt;String to
            Number: parse(x)&lt;/li&gt; &lt;li&gt;Substring: cut(x, start,
            len)&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt; &lt;p&gt;The following
            &lt;em&gt;process related functions&lt;/em&gt; are supported:
            &lt;ul&gt; &lt;li&gt;Retrieving a parameter value: param("operator",
            "parameter")&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt; &lt;p&gt;Beside attributes and those
            operations and functions mentioned above, this operator also supports the constants
            pi and e if this is indicated by the corresponding parameter &lt;b&gt;use standard constants&lt;/b&gt;
            (default: true). You can also use strings in formulas (for example in
            a conditioned if-formula) but the string values have to be enclosed
            in double quotes (").&lt;/p&gt; 
            &lt;br/&gt;&lt;h4&gt;Examples&lt;/h4&gt;
            a1+sin(a2*a3)&lt;br/&gt; 
            if (att1&gt;5, att2*att3,-abs(att1))&lt;br/&gt; &lt;/p&gt;
      </help>
    <key>generate_attributes</key>
    <tags>
         <tag>Calculate</tag>
         <tag>Create</tag>
         <tag>New</tag>
         <tag>Extract</tag>
         <tag>Expressions</tag>
         <tag>Features</tag>
         <tag>Variables</tag>
         <tag>Columns</tag>
         <tag>Functions</tag>
    </tags>
  </operator>
    <operator>
        <name>IOMultiplier</name>
        <synopsis>This operators simply multiplies selected input objects.
        </synopsis>
        <help>In some cases you might want to apply different parts of the
            process on the same input object. You can use this operator to create
            &lt;code&gt;k&lt;/code&gt; copies of the given input
            object.&lt;br/&gt; This operator is deprecated and should be replaced
            by IOMultiplier2. This happens automatically when old processes are
            imported.</help>
    <key>iomultiplier</key>
  </operator>
    <operator>
        <name>AMLReader</name>
        <synopsis>This operator reads an example set from file. The operator
            can be configured to read almost all file formats.</synopsis>
        <help>&lt;p&gt; This operator reads an example set from (a) file(s).
            Probably you can use the default parameter values for the most file
            formats (including the format produced by the ExampleSetWriter, CSV,
            ...). Please refer to section &lt;i&gt;First steps/File
            formats&lt;/i&gt; for details on the attribute description file set
            by the parameter &lt;var&gt;attributes&lt;/var&gt; used to specify
            attribute types. You can use the wizard of this operator or the tool
            Attribute Editor in order to create those meta data .aml files for
            your datasets. &lt;/p&gt; &lt;p&gt; This operator supports the
            reading of data from multiple source files. Each attribute (including
            special attributes like labels, weights, ...) might be read from
            another file. Please note that only the minimum number of lines of
            all files will be read, i.e. if one of the data source files has less
            lines than the others, only this number of examples will be read.
            &lt;/p&gt; &lt;p&gt; The split points can be defined with regular
            expressions (please refer to the annex of the RapidMiner tutorial for
            an overview). The default split parameter
            &amp;quot;,\s*|;\s*|\s+&amp;quot; should work for most file formats.
            This regular expression describes the following column separators
            &lt;ul&gt; &lt;li&gt;the character &amp;quot;,&amp;quot; followed by
            a whitespace of arbitrary length (also no white space)&lt;/li&gt;
            &lt;li&gt;the character &amp;quot;;&amp;quot; followed by a
            whitespace of arbitrary length (also no white space)&lt;/li&gt;
            &lt;li&gt;a whitespace of arbitrary length (min. 1)&lt;/li&gt;
            &lt;/ul&gt; A logical XOR is defined by &amp;quot;|&amp;quot;. Other
            useful separators might be &amp;quot;\t&amp;quot; for tabulars,
            &amp;quot; &amp;quot; for a single whitespace, and
            &amp;quot;\s&amp;quot; for any whitespace. &lt;/p&gt; &lt;p&gt;
            Quoting is also possible with &amp;quot;. You can escape quotes with
            a backslash, i.e. \&amp;quot;. Please note that you can change these
            characters by adjusting the corresponding settings. &lt;/p&gt;
            &lt;p&gt; Additionally you can specify comment characters which can
            be used at arbitrary locations of the data lines. Any content after
            the comment character will be ignored. Unknown attribute values can
            be marked with empty strings (if this is possible for your column
            separators) or by a question mark (recommended). &lt;/p&gt;</help>
    </operator>
    <operator>
        <name>Filter by Length</name>
        <synopsis>Filters terms based on a minimal number of characters they
            must contain.</synopsis>
        <help>Filter tokens based on their length (the number of characters
            they contain).</help>
    <key>filter_by_length</key>
  </operator>

    <operator>
        <name>Find Threshold</name>
        <synopsis>Finds a threshold for given prediction confidences (soft
            predictions) , costs and distributional information in order to turn
            it into a crisp classification. The optimization step is based on ROC
            analysis.</synopsis>
        <help>This operator finds the best threshold for crisp classifying
            based on user defined costs.</help>
    <key>find_threshold</key>
    <tags>
         <tag>Platt</tag>
         <tag>Scoring</tag>
         <tag>Scores</tag>
         <tag>Confidences</tag>
         <tag>Thresholds</tag>
    </tags>
  </operator>
    <operator>
        <name>Impute Missing Values</name>
        <synopsis>Replaces missing values in examples by applying a model
            learned for missing values.</synopsis>
        <help>The operator MissingValueImpution imputes missing values by
            learning models for each attribute (except the label) and applying
            those models to the data set. The learner which is to be applied has
            to be given as inner operator. In order to specify a subset of the
            example set in which the missing values should be imputed (e.g. to
            limit the imputation to only numerical attributes) the corresponding
            attributes might be chosen by the filter parameters. Please be aware
            that depending on the ability of the inner operator to handle missing
            values this operator might not be able to impute all missing values
            in some cases. This behavior leads to a warning. It might hence be
            useful to combine this operator with a subsequent
            MissingValueReplenishment. ATTENTION: This operator is currently
            under development and does not properly work in all cases. We do not
            recommend the usage of this operator in production systems.</help>
    <key>impute_missing_values</key>
    <tags>
         <tag>Nulls</tag>
         <tag>Empty</tag>
         <tag>Cleansing</tag>
         <tag>Quality</tag>
         <tag>Missings</tag>
         <tag>Handle</tag>
         <tag>Replace</tag>
         <tag>Na</tag>
         <tag>Nan</tag>
         <tag>Fill na</tag>
    </tags>
  </operator>
    <operator>
        <name>RVM</name>
        <synopsis>An implementation of a relevance vector machine.</synopsis>
        <help>Relevance Vector Machine (RVM) Learner. The RVM is a
            probabilistic method both for classification and regression. The
            implementation of the relevance vector machine is based on the
            original algorithm described by Tipping/2001. The fast version of the
            marginal likelihood maximization (Tipping/Faul/2003) is also
            available if the parameter &amp;quot;rvm_type&amp;quot; is set to
            &amp;quot;Constructive-Regression-RVM&amp;quot;.</help>
    </operator>
    <operator>
        <name>Process Documents from Files</name>
        <synopsis>Generates word vectors from a text collection stored in
            multiple files.</synopsis>
        <help/>
    <key>process_document_from_file</key>
  </operator>
    
    <operator>
        <name>Self-Organizing Map</name>
        <synopsis>Trains a self-organizing map and applyes the examples on the
            map. The resulting coordinates are used as new attributes.</synopsis>
        <help>This operator performs a dimensionality reduction based on a SOM
            (Self Organizing Map, aka Kohonen net).</help>
    <key>self_organizing_map</key>
    <shortName>SOM</shortName>
  </operator>
    <operator>
        <name>WeightedBootstrappingValidation</name>
        <synopsis>This operator encapsulates an iterated weighted
            bootstrapping sampling with performance evaluation on the remaining
            examples.</synopsis>
        <help>&lt;p&gt;This validation operator performs several bootstrapped
            samplings (sampling with replacement) on the input set and trains a
            model on these samples. The remaining samples, i.e. those which were
            not sampled, build a test set on which the model is evaluated. This
            process is repeated for the specified number of iterations after
            which the average performance is calculated.&lt;/p&gt; &lt;p&gt;The
            basic setup is the same as for the usual cross validation operator.
            The first inner operator must provide a model and the second a
            performance vector. Please note that this operator does not regard
            example weights, i.e. weights specified in a weight column.&lt;/p&gt;
            &lt;p&gt;This validation operator provides several values which can
            be logged by means of a &lt;i&gt;ProcessLogOperator&lt;/i&gt;. All
            performance estimation operators of RapidMiner provide access to the
            average values calculated during the estimation. Since the operator
            cannot ensure the names of the delivered criteria, the ProcessLog
            operator can access the values via the generic value names:&lt;/p&gt;
            &lt;ul&gt; &lt;li&gt;performance: the value for the main criterion
            calculated by this validation operator&lt;/li&gt;
            &lt;li&gt;performance1: the value of the first criterion of the
            performance vector calculated&lt;/li&gt; &lt;li&gt;performance2: the
            value of the second criterion of the performance vector
            calculated&lt;/li&gt; &lt;li&gt;performance3: the value of the third
            criterion of the performance vector calculated&lt;/li&gt;
            &lt;li&gt;for the main criterion, also the variance and the standard
            deviation can be accessed where applicable.&lt;/li&gt; &lt;/ul&gt;
        </help>
    <key>weightedbootstrappingvalidation</key>
  </operator>
    <operator>
        <name>MyKLR</name>
        <synopsis>MyKLRLearner provides an internal Java implementation of the
            myKLR by Stefan Rueping.</synopsis>
        <help>This is the Java implementation of &lt;em&gt;myKLR&lt;/em&gt; by
            Stefan R&amp;uuml;ping. myKLR is a tool for large scale kernel
            logistic regression based on the algorithm of Keerthi/etal/2003 and
            the code of mySVM.</help>
    </operator>
    <operator>
        <name>Similarity to Data</name>
        <synopsis>Calculates a an example set from a similarity measure.
        </synopsis>
        <help>&lt;p&gt;This operator creates an example set from a given
            similarity measure. It can either produce a long table format, i.e.
            something like&lt;br /&gt; &lt;br /&gt; id1 id2 sim&lt;br /&gt; id1
            id3 sim&lt;br /&gt; id1 id4 sim&lt;br /&gt; ...&lt;br /&gt; id2 id1
            sim&lt;br /&gt; ...&lt;br /&gt; &lt;br /&gt; or a matrix format like
            here&lt;br /&gt; &lt;br /&gt; id id1 id2 id3 ...&lt;br /&gt; id1 sim
            sim sim...&lt;br /&gt; ... &lt;br /&gt;&lt;/p&gt;</help>
    <key>similarity_to_data</key>
  </operator>
    <operator>
        <name>Random Forest (Deprecated)</name>
        <synopsis>Learns a set of random trees, i.e. for each split only a
            random subset of attributes is available. The resulting model is a
            voting model of all trees.</synopsis>
        <help>This operators learns a random forest. The resulting forest
            model contains serveral single random tree models.</help>
    <key>random_forest</key>
  </operator>
    <operator>
        <name>Union</name>
        <synopsis>This operator first builds the union set / superset for both
            input example sets and merges both extended sets into a new one.
        </synopsis>
        <help>This operator performs two steps: first, it build the union set
            / superset of features of both input example sets where common
            features are kept and both feature sets are extended in a way that
            the feature sets are equal for both example sets. The second step
            then merges both example sets and will deliver the resulting example
            set.</help>
    <key>union</key>
    <tags>
         <tag>Combine</tag>
         <tag>Merge</tag>
         <tag>Match</tag>
         <tag>Append</tag>
    </tags>
  </operator>

    <operator>
        <name>Weight by Chi Squared Statistic</name>
        <synopsis>This operator calculates the relevance of a feature by
            computing for each attribute of the input example set the value of
            the chi-squared statistic with respect to the class attribute.
        </synopsis>
        <help>This operator calculates the relevance of a feature by computing
            for each attribute of the input example set the value of the
            chi-squared statistic with respect to the class attribute.</help>
    <key>weight_by_chi_squared_statistic</key>
    <tags>
         <tag>Weighting</tag>
         <tag>Importance</tag>
         <tag>Influence</tag>
         <tag>Significance</tag>
         <tag>Factors</tag>
         <tag>Relevance</tag>
    </tags>
  </operator>
    <operator>
        <name>AbsoluteSplitChain</name>
        <synopsis>Splits an example set in two parts based on user defined set
            sizes and uses the output of the first child and the second part as
            input for the second child.</synopsis>
        <help>&lt;p&gt;An operator chain that split an
            &lt;i&gt;ExampleSet&lt;/i&gt; into two disjunct parts and applies the
            first child operator on the first part and applies the second child
            on the second part and the result of the first child. The total
            result is the result of the second operator.&lt;/p&gt; &lt;p&gt;The
            input example set will be splitted based on a user defined absolute
            numbers.&lt;/p&gt;</help>
    <key>absolutesplitchain</key>
  </operator>

    <operator>
        <name>Generate n-Grams (Characters)</name>
        <synopsis>Creates ngrams of each input token.</synopsis>
        <help>Creates ngrams of the input terms. If an input term is shorter
            than the specified length of output ngrams it is directly passed to
            the output token stream.</help>
    <key>generate_n_grams_characters</key>
  </operator>

    
    <operator>
        <name>Stem (Dictionary)</name>
        <synopsis>Replaces terms by pattern matching rules.</synopsis>
        <help>Reduces terms to a base form using an external file with
            replacement rules. The file must contain a rule per line:
            targetExpression : patter1 patter2 ... where targetExpression is the
            term to which the input terms are reduced, if it matches any of the
            patterns. patterX is a simple string or a regular expression as
            described in appendix A. A simple example would be a mapping like:
            weekday : .*day Please keep in mind, that very short words are
            filtered out in the default setting of the TextInput operators.
        </help>
    <key>stem_dictionary</key>
  </operator>

    <operator>
        <name>Optimize Selection (Evolutionary)</name>
        <synopsis>A genetic algorithm for feature selection.</synopsis>
        <help>A genetic algorithm for feature selection (mutation=switch
            features on and off, crossover=interchange used features). Selection
            is done by roulette wheel. Genetic algorithms are general purpose
            optimization / search algorithms that are suitable in case of no or
            little problem knowledge. &lt;br/&gt; A genetic algorithm works as
            follows &lt;ol&gt; &lt;li&gt;Generate an initial population
            consisting of &lt;code&gt;population_size&lt;/code&gt; individuals.
            Each attribute is switched on with probability
            &lt;code&gt;p_initialize&lt;/code&gt;&lt;/li&gt; &lt;li&gt;For all
            individuals in the population &lt;ul&gt; &lt;li&gt;Perform mutation,
            i.e. set used attributes to unused with probability
            &lt;code&gt;p_mutation&lt;/code&gt; and vice versa.&lt;/li&gt;
            &lt;li&gt;Choose two individuals from the population and perform
            crossover with probability &lt;code&gt;p_crossover&lt;/code&gt;. The
            type of crossover can be selected by
            &lt;code&gt;crossover_type&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;
            &lt;/li&gt; &lt;li&gt;Perform selection, map all individuals to
            sections on a roulette wheel whose size is proportional to the
            individual's fitness and draw
            &lt;code&gt;population_size&lt;/code&gt; individuals at random
            according to their probability.&lt;/li&gt; &lt;li&gt;As long as the
            fitness improves, go to 2&lt;/li&gt; &lt;/ol&gt; If the example set
            contains value series attributes with blocknumbers, the whole block
            will be switched on and off.</help>
    <key>optimize_selection_evolutionary</key>
  </operator>
    <operator>
        <name>Subgroup Discovery</name>
        <synopsis>Performs an exhaustive subgroup discovery.</synopsis>
        <help>This operator discovers subgroups (or induces a rule set,
            respectively) by generating hypotheses exhaustively. Generation is
            done by stepwise refining the empty hypothesis (which contains no
            literals). The loop for this task hence iterates over the depth of
            the search space, i.e. the number of literals of the generated
            hypotheses. The maximum depth of the search can be specified.
            Furthermore the search space can be pruned by specifying a minimum
            coverage of the hypothesis or by using only a given amount of
            hypotheses which have the highest coverage. From the hypotheses,
            rules are derived according to the users preference. The operator
            allows the derivation of positive rules (Y+) and negative rules (Y-)
            separately or the combination by deriving both rules or only the one
            which is the most probable due to the examples covered by the
            hypothesis (hence: the actual prediction for that subset). All
            generated rules are evaluated on the example set by a user specified
            utility function and stored in the final rule set if they (1) exceed
            a minimum utility threshold or (2) are among the k best rules. The
            desired behavior can be specified as well.</help>
    <key>subgroup_discovery</key>
  </operator>
    <operator>
        <name>ChangeAttributeType</name>
        <synopsis>This operator can be used to change the attribute type
            (regular, special, label, id...).</synopsis>
        <help>&lt;p&gt; This operator can be used to change the attribute type
            of an attribute of the input example set. If you want to change the
            attribute name you should use the
            &lt;i&gt;ChangeAttributeName&lt;/i&gt; operator. &lt;/p&gt; &lt;p&gt;
            The target type indicates if the attribute is a regular attribute
            (used by learning operators) or a special attribute (e.g. a label or
            id attribute). The following target attribute types are possible:
            &lt;/p&gt; &lt;ul&gt; &lt;li&gt;regular: only regular attributes are
            used as input variables for learning tasks&lt;/li&gt; &lt;li&gt;id:
            the id attribute for the example set&lt;/li&gt; &lt;li&gt;label:
            target attribute for learning&lt;/li&gt; &lt;li&gt;prediction:
            predicted attribute, i.e. the predictions of a learning
            scheme&lt;/li&gt; &lt;li&gt;cluster: indicates the memebership to a
            cluster&lt;/li&gt; &lt;li&gt;weight: indicates the weight of the
            example&lt;/li&gt; &lt;li&gt;batch: indicates the membership to an
            example batch&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; Users can also define
            own attribute types by simply using the desired name. &lt;/p&gt;
        </help>
    </operator>
    <operator>
        <name>Filter Example Range</name>
        <synopsis>This only allows examples in the specified index range.
        </synopsis>
        <help>This operator keeps only the examples of a given range
            (including the borders). The other examples will be removed from the
            input example set.</help>
    <key>filter_example_range</key>
    <tags>
         <tag>Select</tag>
         <tag>Keep</tag>
         <tag>Remove</tag>
         <tag>Drop</tag>
         <tag>Delete</tag>
         <tag>Rows</tag>
         <tag>Cases</tag>
         <tag>Instances</tag>
         <tag>Lines</tag>
         <tag>Observations</tag>
         <tag>Sample</tag>
         <tag>Sampling</tag>
         <tag>Limits</tag>
    </tags>
  </operator>
    <operator>
        <name>Set Role</name>
        <synopsis>&lt;p&gt;This operator can be used to change the attribute role (regular, special,     label, id...).&lt;/p&gt;</synopsis>
        <help>&lt;p&gt;      This operator can be used to change the role of an attribute of the       input &lt;i&gt;ExampleSet&lt;/i&gt;. If you want to change the attribute name you       should use the     &lt;/p&gt;    &lt;a href="rm://opdoc/rename"&gt;Rename&lt;/a&gt;  operator.    &lt;p&gt;      The target role indicates if the attribute is a regular attribute (used       by learning operators) or a special attribute (e.g. a label or id       attribute). The following target attribute types are possible:    &lt;/p&gt;    &lt;ul&gt;      &lt;li&gt;        &lt;b&gt;regular: &lt;/b&gt;only regular attributes are used as input variables         for learning tasks      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;id:&lt;/b&gt; the id attribute for the example set      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;label:&lt;/b&gt; target attribute for learning      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;prediction:&lt;/b&gt; predicted attribute, i.e. the predictions of a         learning scheme      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;cluster:&lt;/b&gt; indicates the membership to a cluster      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;weight: &lt;/b&gt;indicates the weight of the example      &lt;/li&gt;      &lt;li&gt;        &lt;b&gt;batch:&lt;/b&gt; indicates the membership to an example batch      &lt;/li&gt;    &lt;/ul&gt;    &lt;p&gt;      Users can also define own attribute types by simply using the desired       name.    &lt;/p&gt;    &lt;p&gt;          &lt;/p&gt;    &lt;p&gt;      &lt;b&gt;Please be aware that roles have to be unique!&lt;/b&gt; Assigning a non       regular role the second time will cause the first attribute to be       dropped from the example set. If you want to keep this attribute, you       have to change it's role first.    &lt;/p&gt;</help>
    <key>set_role</key>
    <tags>
         <tag>Label</tag>
         <tag>Target</tag>
         <tag>Id</tag>
         <tag>Class</tag>
         <tag>Dependent</tag>
         <tag>Independent</tag>
         <tag>Special</tag>
         <tag>Regular</tag>
         <tag>Inputs</tag>
         <tag>Columns</tag>
         <tag>Attributes</tag>
         <tag>Features</tag>
         <tag>Variables</tag>
         <tag>Types</tag>
    </tags>
  </operator>
  <operator>
    <name>Move Repository Entry</name>
    <synopsis></synopsis>
    <help></help>
    <key>move_repository_entry</key>
  </operator>
  <operator>
    <name>Copy Repository Entry</name>
    <synopsis></synopsis>
    <help></help>
    <key>copy_repository_entry</key>
  </operator>
  <operator>
    <name>Rename Repository Entry</name>
    <synopsis></synopsis>
    <help></help>
    <key>rename_repository_entry</key>
  </operator>
  <operator>
    <name>Delete Repository Entry</name>
    <synopsis></synopsis>
    <help></help>
    <key>delete_repository_entry</key>
  </operator>
    <operator>
        <name>Generate Products</name>
        <synopsis>Creates all products based on the attributes specified by
            regular expressions.</synopsis>
        <help>This operator creates all products of the specified attributes.
            The attribute names can be specified by regular expressions.</help>
    <key>generate_products</key>
  </operator>
    <operator>
        <name>Write Message</name>
        <synopsis>This operator simply writes the given text into the
            specified file (can be useful in combination with a process branch).
        </synopsis>
        <help>This operator simply writed the specified text into the
            specified file. This can be useful in combination with the
            &lt;i&gt;ProcessBranch&lt;/i&gt; operator. For example, one could
            write the success or non-success of a process into the same file
            depending on the condition specified by a process branch.</help>
    <key>write_message</key>
  </operator>

    <operator>
        <name>Nominal2Binary</name>
        <synopsis>Maps all nominal values to binary attributes.</synopsis>
        <help>This operator maps the values of all nominal values to binary
            attributes. For example, if a nominal attribute with name
            &amp;quot;costs&amp;quot; and possible nominal values
            &amp;quot;low&amp;quot;, &amp;quot;moderate&amp;quot;, and
            &amp;quot;high&amp;quot; is transformed, the result is a set of three
            binominal attributes &amp;quot;costs = low&amp;quot;, &amp;quot;costs
            = moderate&amp;quot;, and &amp;quot;costs = high&amp;quot;. Only one
            of the values of each attribute is true for a specific example, the
            other values are false.</help>
    </operator>
    <operator>
        <name>Weight by Deviation</name>
        <synopsis>Computes weights based on the (normalized) standard
            deviation of the attributes.</synopsis>
        <help>&lt;p&gt; Creates weights from the standard deviations of all
            attributes. The values can be normalized by the average, the minimum,
            or the maximum of the attribute. &lt;/p&gt;</help>
    <key>weight_by_deviation</key>
  </operator>
    <operator>
        <name>FP-Growth</name>
        <synopsis>This learner efficiently calculates all frequent item sets
            from the given data.</synopsis>
        <help>&lt;p&gt;This operator calculates all frequent items sets from a
            data set by building a FPTree data structure on the transaction data
            base. This is a very compressed copy of the data which in many cases
            fits into main memory even for large data bases. From this FPTree all
            frequent item set are derived. A major advantage of FPGrowth compared
            to Apriori is that it uses only 2 data scans and is therefore often
            applicable even on large data sets.&lt;/p&gt; &lt;p&gt;Please note
            that the given data set is only allowed to contain binominal
            attributes, i.e. nominal attributes with only two different values.
            Simply use the provided preprocessing operators in order to transform
            your data set. The necessary operators are the discretization
            operators for changing the value types of numerical attributes to
            nominal and the operator Nominal2Binominal for transforming nominal
            attributes into binominal / binary ones. &lt;/p&gt; &lt;p&gt;The
            frequent item sets are mined for the positive entries in your data
            base, i.e. for those nominal values which are defined as positive in
            your data base. If you use an attribute description file (.aml) for
            the &lt;i&gt;ExampleSource&lt;/i&gt; operator this corresponds to the
            second value which is defined via the classes attribute or inner
            value tags.&lt;/p&gt; &lt;p&gt; If your data does not specify the
            positive entries correctly, you may set them using the parameter
            positive_value. This only works if all your attributes contain this
            value!&lt;/p&gt; &lt;p&gt;This operator has two basic working modes:
            finding at least the specified number of item sets with highest
            support without taking the min_support into account (default) or
            finding all item sets with a support large than
            min_support.&lt;/p&gt;</help>
    <key>fp_growth</key>
    <tags>
         <tag>Associations</tag>
         <tag>Market</tag>
         <tag>Basket</tag>
         <tag>Upselling</tag>
         <tag>Up-selling</tag>
         <tag>Crossselling</tag>
         <tag>Cross-selling</tag>
         <tag>Itemset</tag>
         <tag>Item-set</tag>
         <tag>Item set</tag>
         <tag>Mining</tag>
         <tag>Frequent</tag>
         <tag>Patterns</tag>
    </tags>
  </operator>
    <operator>
        <name>Optimize Weights (PSO)</name>
        <synopsis>Weight the features with a particle swarm optimization
            approach.</synopsis>
        <help>This operator performs the weighting of features with a particle
            swarm approach.</help>
    <key>optimize_weights_pso</key>
  </operator>
    <operator>
        <name>Trim</name>
        <synopsis>Creates new attributes from nominal attributes which contain
            the trimmed original values.</synopsis>
        <help>This operator creates new attributes from nominal attributes
            where the new attributes contain the trimmed original values, i.e.
            leading and trailing spaces will be removed.</help>
    <key>trim</key>
  </operator>
    <operator>
        <name>Discretize by Size</name>
        <synopsis>Discretize numerical attributes into bins with user defined
            number of contained examples.</synopsis>
        <help>This operator discretizes all numeric attributes in the dataset
            into nominal attributes. This discretization is performed by binning
            examples into bins of same size. The specified number of equally
            sized bins is created and the numerical values are simply sorted into
            those bins, so that all bins contain the same number of examples.
            Skips all special attributes including the label.</help>
    <key>discretize_by_size</key>
    <tags>
         <tag>Continous</tag>
         <tag>Categorical</tag>
         <tag>Nominal</tag>
         <tag>Polynominal</tag>
         <tag>Ordinary</tag>
         <tag>Discrete</tag>
         <tag>Discretization</tag>
         <tag>Dichotomization</tag>
         <tag>Dichotomy</tag>
         <tag>Binning</tag>
         <tag>Histogram</tag>
         <tag>Types</tag>
         <tag>Qualitative</tag>
         <tag>Quantitative</tag>
         <tag>Groups</tag>
         <tag>Intervals</tag>
    </tags>
    <shortName>Discretize</shortName>
  </operator>
    <operator>
        <name>Mutual Information Matrix</name>
        <synopsis>Determines the mutual information between all attributes.
        </synopsis>
        <help>&lt;p&gt;This operator calculates the mutual information matrix
            between all attributes of the input example set. This operator
            produces a dependency matrix which can be displayed to the user in
            the result tab.&lt;/p&gt; &lt;p&gt;Please note that this simple
            implementation performs a data scan for each attribute combination
            and might therefore take some time for non-memory example
            tables.&lt;/p&gt;</help>
    <key>mututal_information_matrix</key>
  </operator>
    <operator>
        <name>Apply Model</name>
        <synopsis>Applies a model to an example set. This might be a
            prediction or another data transformation model.</synopsis>
        <help>&lt;p&gt;      This operator applies a &lt;i&gt;Model&lt;/i&gt; to an &lt;i&gt;ExampleSet&lt;/i&gt;. &lt;i&gt;Models &lt;/i&gt;usually       contain information about the data they have been trained on. This       information can be used for predicting the value of a possibly unknown       label, reproduce some transformations as during training or performing       other changes. All needed parameters are stored within the model object.    &lt;/p&gt;    &lt;p&gt;          &lt;/p&gt;    &lt;p&gt;      Please pay attention to the fact, that the application of &lt;i&gt;Models &lt;/i&gt;will       need the same attributes during application on an &lt;i&gt;ExampleSet &lt;/i&gt;that       where part of the &lt;i&gt;ExampleSet&lt;/i&gt; it was trained on. Some minor       changes like adding attributes might be possible, but might cause severe       calculation errors. Please make sure, that &lt;b&gt;the attributes' number,       order, type and role &lt;/b&gt;are consistent during training and application.    &lt;/p&gt;    &lt;p&gt;          &lt;/p&gt;    &lt;p&gt;      If the model supports views, it is possible to create a view instead of       changing the underlying data. In order to advise the Apply Model       operator to do so, simply switch on the create view parameter. The       transformation that would be normaly performed directly on the data will       then be computed every time a value is requested and the result is       returned without changing the data. Please keep in mind, that not all       models support views.    &lt;/p&gt;    &lt;p&gt;          &lt;/p&gt;    &lt;p&gt;          &lt;/p&gt;    &lt;p&gt;      If you have to apply serveral models in a row, like for example when you       have to apply a few preprocessing models before applying a prediction       model, then you would like to group models. This is possible using the &lt;a href="rm://opdoc/group_models"&gt;Group       Models &lt;/a&gt;operator in a convenient way.    &lt;/p&gt;</help>
    <key>apply_model</key>
    <tags>
         <tag>Predict</tag>
         <tag>Predictions</tag>
         <tag>Forecasts</tag>
         <tag>Scores</tag>
         <tag>Scoring</tag>
         <tag>Trained</tag>
    </tags>
  </operator>
    <operator>
        <name>ExcelWriter</name>
        <synopsis>This operator writes an example set to Excel spreadsheet
            files.</synopsis>
        <help>&lt;p&gt;This operator can be used to write data into Microsoft
            Excel spreadsheets. This operator creates Excel files readable by
            Excel 95, 97, 2000, XP, 2003 and newer. Missing data values are
            indicated by empty cells.&lt;/p&gt;</help>
    </operator>
    <operator>
        <name>Transformed Regression</name>
        <synopsis>This learner performs regression by transforming the labels
            and calling an inner regression learner.</synopsis>
        <help>This meta learner applies a transformation on the label before
            the inner regression learner is applied.</help>
    <key>transformed_regression</key>
  </operator>
    <operator>
        <name>Fourier Transformation</name>
        <synopsis>Uses the label as function of each attribute and calculates
            the fourier transformations as new attributes.</synopsis>
        <help>Creates a new example set consisting of the result of a fourier
            transformation for each attribute of the input example set.</help>
    <key>fourier_transformation</key>
  </operator>
    <operator>
        <name>Create Document</name>
        <synopsis>Generates a textObject of a single text.</synopsis>
        <help>This operator allows to create a TextObject filled with the text
            of the parameter text.</help>
    <key>create_document</key>
  </operator>
    <operator>
        <name>Create Lift Chart</name>
        <synopsis>Generates a lift chart for the given model and input data
            set based on discretized confidences and a Pareto chart.</synopsis>
        <help>This operator creates a Lift chart based on a Pareto plot for
            the discretized confidence values for the given example set and
            model. The model will be applied on the example set and a lift chart
            will be produced afterwards. Please note that a predicted label of
            the given example set will be removed during the application of this
            operator.</help>
    <key>create_lift_chart</key>
    <tags>
         <tag>Roc</tag>
         <tag>Marketing</tag>
         <tag>Pareto</tag>
         <tag>Direct</tag>
    </tags>
  </operator>
    <operator>
        <name>Shuffle</name>
        <synopsis>Permutates the examples in the table. Caution: will increase
            memory usage!</synopsis>
        <help>This operator creates a new, shuffled ExampleSet by making
            &lt;em&gt;a new copy&lt;/em&gt; of the exampletable in main memory!
            Caution! System may run out of memory, if the example table is too
            large.</help>
    <key>shuffle</key>
  </operator>
    
    <operator>
        <name>Replace (Dictionary)</name>
        <synopsis>Replaces occurrances in nominal attributes specified by a
            second input example set.</synopsis>
        <help>This operator takes two example sets and transforms the second
            into a dictionary. The second example set must contain two nominal
            attributes. For every example in this set a dictionary entry is
            created matching the first attribute value to the second. Finally,
            this dictionary is used to replace substrings in the first example
            set to replacements.</help>
    <key>replace_dictionary</key>
  </operator>
    
    <operator>
        <name>Performance (Min-Max)</name>
        <synopsis>Puts all input criteria into a min-max criterion which
            delivers the minimum instead of the average or arbitrary weighted
            combinations.</synopsis>
        <help>Wraps a &lt;i&gt;MinMaxCriterion&lt;/i&gt; around each
            performance criterion of type MeasuredPerformance. This criterion
            uses the minimum fitness achieved instead of the average fitness or
            arbitrary weightings of both. Please note that the average values
            stay the same and only the fitness values change.</help>
    <key>performance_min_max</key>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>Find Threshold (Meta)</name>
        <synopsis>Determines confidence thresholds based on misclassification
            costs, also possible to define costs for the option non-classified.
        </synopsis>
        <help>&lt;p&gt;This operator uses a set of class weights and also
            allows a weight for the fact that an example is not classified at all
            (marked as unknown). Based on the predictions of the model of the
            inner learner this operator optimized a set of thresholds regarding
            the defined weights.&lt;/p&gt; &lt;p&gt; This operator might be very
            useful in cases where it is better to not classify an example then to
            classify it in a wrong way. This way, it is often possible to get
            very high accuracies for the remaining examples (which are actually
            classified) for the cost of having some examples which must still be
            manually classified. &lt;/p&gt;</help>
    <key>find_threshold_meta</key>
  </operator>
    <operator>
        <name>Create Formula</name>
        <synopsis>Generates a formula from a given model for models which are
            capable of producing formulas.</synopsis>
        <help>This operator extracts a prediction calculation formula from the
            given model and stores the formula in a formula result object which
            can then be written to a file, e.g. with the ResultWriter operator.
            Please note that not all RapidMiner models provide a formula.</help>
    <key>create_formula</key>
  </operator>
    
    <operator>
        <name>Generate Churn Data</name>
        <synopsis>Generates data for testing purposes based on a churn
            reduction data set.</synopsis>
        <help>Generates a random example set for testing purposes. The data
            represents a direct mailing example set.</help>
    <key>generate_churn_data</key>
  </operator>
    <operator>
        <name>Keep Document Parts</name>
        <synopsis>Extracts the text of a textobject that matchs a given
            regular expression and returns it.</synopsis>
        <help>This operator allows to extract a part of a token using regular
            expressions. It searches the first region within the text that
            matches the given regular expression and returns this region as new
            token. If no such region can be found this token is discarded. Since
            this probably will work best when the tokens are long enough, this
            operator is especially useful before the actual tokenization is
            applied during word vector creation.</help>
    <key>keep_document_parts</key>
  </operator>
    
    <operator>
        <name>Create Association Rules</name>
        <synopsis>This operator generated a set of association rules for a
            given set of frequent item sets.</synopsis>
        <help>&lt;p&gt;This operator generates association rules from frequent
            item sets. In RapidMiner, the process of frequent item set mining is
            divided into two parts: first, the generation of frequent item sets
            and second, the generation of association rules from these
            sets.&lt;/p&gt; &lt;p&gt;For the generation of frequent item sets,
            you can use for example the operator &lt;i&gt;FPGrowth&lt;/i&gt;. The
            result will be a set of frequent item sets which could be used as
            input for this operator.&lt;/p&gt;</help>
    <key>create_association_rules</key>
    <tags>
         <tag>Associations</tag>
         <tag>Market</tag>
         <tag>Basket</tag>
         <tag>Upselling</tag>
         <tag>Up-selling</tag>
         <tag>Crossselling</tag>
         <tag>Cross-selling</tag>
         <tag>Itemset</tag>
         <tag>Item-set</tag>
         <tag>Item set</tag>
         <tag>Mining</tag>
         <tag>Frequent</tag>
         <tag>Patterns</tag>
    </tags>
  </operator>
    <operator>
        <name>Regularized Discriminant Analysis</name>
        <synopsis>A regularized generale discriminant function for binominal
            labels and numerical attributes.</synopsis>
        <help>&lt;p&gt;This is a regularized discriminant analysis (RDA) which
            is a generalization of the LDA or QDA. Both algorithms are special
            cases of this algorithm, using parameter alpha = 1 respectively alpha
            = 0.&lt;/p&gt;</help>
    <key>regularized_discriminant_analysis</key>
    <shortName>RDA</shortName>
  </operator>
    <operator>
        <name>DBSCAN</name>
        <synopsis>Clustering with DBSCAN</synopsis>
        <help>This operator provides the DBScan cluster algorithm. If no id
            attribute is present, the operator will create one.</help>
    <key>dbscan</key>
    <tags>
         <tag>Unsupervised</tag>
         <tag>Clustering</tag>
         <tag>Segmentation</tag>
         <tag>Grouping</tag>
         <tag>Density</tag>
         <tag>Densities</tag>
    </tags>
    <shortName>Clustering</shortName>
  </operator>
    <operator>
        <name>SimpleReader</name>
        <synopsis>This operator reads an example set from file. It is a
            simpler version of the ExampleSource operator.</synopsis>
        <help>&lt;p&gt; This operator reads an example set from (a) file(s).
            Probably you can use the default parameter values for the most file
            formats (including the format produced by the ExampleSetWriter, CSV,
            ...). In fact, in many cases this operator is more appropriate for
            CSV based file formats than the &lt;i&gt;CSVExampleSource&lt;/i&gt;
            operator itself since you can better control some of the necessary
            settings like column separators etc. &lt;/p&gt; &lt;p&gt; In contrast
            to the usual ExampleSource operator this operator is able to read the
            attribute names from the first line of the data file. However, there
            is one restriction: the data can only be read from one file instead
            of multiple files. If you need a fully flexible operator for data
            loading you should use the more powerful ExampleSource operator which
            also provides more parameters tuning for example the quoting
            mechanism and other specialized settings. &lt;/p&gt; &lt;p&gt; The
            column split points can be defined with regular expressions (please
            refer to the annex of the RapidMiner tutorial). The default split
            parameter &amp;quot;,\s*|;\s*|\s+&amp;quot; should work for most file
            formats. This regular expression describes the following column
            separators &lt;ul&gt; &lt;li&gt;the character &amp;quot;,&amp;quot;
            followed by a whitespace of arbitrary length (also no white
            space)&lt;/li&gt; &lt;li&gt;the character &amp;quot;;&amp;quot;
            followed by a whitespace of arbitrary length (also no white
            space)&lt;/li&gt; &lt;li&gt;a whitespace of arbitrary length (min.
            1)&lt;/li&gt; &lt;/ul&gt; A logical XOR is defined by
            &amp;quot;|&amp;quot;. Other useful separators might be
            &amp;quot;\t&amp;quot; for tabulars, &amp;quot; &amp;quot; for a
            single whitespace, and &amp;quot;\s&amp;quot; for any whitespace.
            &lt;/p&gt; &lt;p&gt; Quoting is also possible with &amp;quot;.
            Escaping a quote is done with \&amp;quot;. Additionally you can
            specify comment characters which can be used at arbitrary locations
            of the data lines and will skip the remaining part of the lines.
            Unknown attribute values can be marked with empty strings or a
            question mark. &lt;/p&gt;</help>
    </operator>
    <operator>
        <name>Sample (Kennard-Stone)</name>
        <synopsis>Creates a sample from an example set using the Kennard-Stone
            algorithm.</synopsis>
        <help>This operator performs a Kennard-Stone Sampling. This sampling
            Algorithm works as follows: First find the two points most separated
            in the training set. For each candidate point, find the smallest
            distance to any object already selected. Select that point for the
            training set which has the largest of these smallest distances As
            described above, this algorithm always gives the same result, due to
            the two starting points which are always the same. This
            implementation reduces number of iterations by holding a list with
            candidates of the largest smallest distances. The parameters controll
            the number of examples in the sample</help>
    <key>sample_kennard_stone</key>
  </operator>

    <operator>
        <name>Fast Large Margin</name>
        <synopsis>A fast learning method for large margin optimizations.
        </synopsis>
        <help>Applies a fast margin learner based on the linear support vector
            learning scheme proposed by R.-E. Fan, K.-W. Chang, C.-J. Hsieh,
            X.-R. Wang, and C.-J. Lin. Although the result is similar to those
            delivered by classical SVM or logistic regression implementations,
            this linear classifier is able to work on data set with millions of
            examples and attributes.</help>
    <key>fast_large_margin</key>
  </operator>
    <operator>
        <name>LiftChart</name>
        <synopsis>Generates a lift chart for the given binominal model and
            input data set.</synopsis>
        <help>This operator creates a Lift chart for the given example set and
            model. The model will be applied on the example set and a lift chart
            will be produced afterwards. Please note that a predicted label of
            the given example set will be removed during the application of this
            operator.</help>
    <key>liftchart</key>
  </operator>
    
    <operator>
        <name>Nominal to Numerical</name>
        <synopsis>Maps all values to real values (usually simply using the
            internal indices).</synopsis>
        <help>This operator maps all non numeric attributes to real valued
            attributes. Nothing is done for numeric attributes, binary attributes
            are mapped to 0 and 1. For nominal attributes one of the following
            calculations will be done: &lt;ul&gt; &lt;li&gt;Dichotomization, i.e.
            one new attribute for each value of the nominal attribute. The new
            attribute which corresponds to the actual nominal value gets value 1
            and all other attributes gets value 0.&lt;/li&gt;
            &lt;li&gt;Alternatively the values of nominal attributes can be seen
            as equally ranked, therefore the nominal attribute will simply be
            turned into a real valued attribute, the old values results in
            equidistant real values.&lt;/li&gt; &lt;/ul&gt; At this moment the
            same applies for ordinal attributes, in a future release more
            appropriate values based on the ranking between the ordinal values
            may be included.</help>
    <key>nominal_to_numerical</key>
    <tags>
         <tag>Categorical</tag>
         <tag>Ordinal</tag>
         <tag>Qualitative</tag>
         <tag>Quantitative</tag>
         <tag>Dummy</tag>
         <tag>Coding</tag>
         <tag>One hot</tag>
         <tag>Encoding</tag>
         <tag>Index</tag>
         <tag>Continous</tag>
         <tag>Types</tag>
    </tags>
  </operator>

    <operator>
        <name>Create Threshold</name>
        <synopsis>Creates a user defined threshold for given prediction
            confidences (soft predictions) in order to turn it into a crisp
            classifier.</synopsis>
        <help>This operator creates a user defined threshold for crisp
            classifying based on prediction confidences.</help>
    <key>create_threshold</key>
  </operator>
    <operator>
        <name>Singular Value Decomposition</name>
        <synopsis>Performs a dimensionality reduction based on Singular Value
            Decomposition (SVD).</synopsis>
        <help>A dimensionality reduction method based on Singular Value
            Decomposition. TODO: see super class</help>
    <key>singular_value_decomposition</key>
    <tags>
         <tag>PCA</tag>
         <tag>Components</tag>
         <tag>Orthogonal</tag>
         <tag>Eigenvalues</tag>
         <tag>Decompositions</tag>
         <tag>Reduction</tag>
         <tag>Multicollinearity</tag>
         <tag>SVD</tag>
    </tags>
    <shortName>SVD</shortName>
  </operator>
    <operator>
        <name>Neural Net</name>
        <synopsis>Learns a neural net from the input data.</synopsis>
        <help>&lt;p&gt;This operator learns a model by means of a feed-forward
            neural network trained by a backpropagation algorithm (multi-layer
            perceptron). The user can define the structure of the neural network
            with the parameter list &amp;quot;hidden_layers&amp;quot;. Each list
            entry describes a new hidden layer. The key of each entry must
            correspond to the layer name. The value of each entry must be a
            number defining the size of the hidden layer. A size value of -1
            indicates that the layer size should be calculated from the number of
            attributes of the input example set. In this case, the layer size
            will be set to (number of attributes + number of classes) / 2 +
            1.&lt;/p&gt; &lt;p&gt;If the user does not specify any hidden layers,
            a default hidden layer with sigmoid type and size (number of
            attributes + number of classes) / 2 + 1 will be created and added to
            the net. If only a single layer without nodes is specified, the input
            nodes are directly connected to the output nodes and no hidden layer
            will be used.&lt;/p&gt; &lt;p&gt;The used activation function is the
            usual sigmoid function. Therefore, the values ranges of the
            attributes should be scaled to -1 and +1. This is also done by this
            operator if not specified otherwise by the corresponding parameter
            setting. The type of the output node is sigmoid if the learning data
            describes a classification task and linear for numerical regression
            tasks.&lt;/p&gt;</help>
    <key>neural_net</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Regression</tag>
         <tag>Model</tag>
         <tag>Deep</tag>
         <tag>Backpropagation</tag>
         <tag>Back-propagation</tag>
         <tag>Perceptron</tag>
         <tag>Network</tag>
         <tag>Artificial</tag>
         <tag>Anns</tag>
    </tags>
  </operator>
    <operator>
        <name>Loop Attribute Subsets</name>
        <synopsis>Performs its inner operator for all specified feature
            subsets (useful for brute force evaluations in combination with the
            ProcessLog operator).</synopsis>
        <help>&lt;p&gt;This meta operator iterates through all possible
            feature subsets within the specified range and applies the inner
            operators on the feature subsets. This might be useful in combination
            with the ProcessLog operator and, for example, a performance
            evaluation. In contrast to the BruteForce feature selection, which
            performs a similar task, this iterative approach needs much less
            memory and can be performed on larger data sets.&lt;/p&gt;</help>
    <key>loop_attribute_subsets</key>
    <shortName>Loop Subsets</shortName>
  </operator>
    <operator>
        <name>MergeNominalValues</name>
        <synopsis>Merges two nominal values of a specified attribute.
        </synopsis>
        <help>Merges two nominal values of a given regular attribute. To
            process special attributes like labels, wrap this operator by an
            AttributeSubsetPreprocessing operator with the parameter
            process_special_attributes enabled.</help>
    </operator>
    <operator>
        <name>Optimize by Generation (YAGGA2)</name>
        <synopsis>Improved version of Yet Another GGA (Generating Geneting
            Algorithm).</synopsis>
        <help>&lt;p&gt;YAGGA is an acronym for Yet Another Generating Genetic
            Algorithm. Its approach to generating new attributes differs from the
            original one. The (generating) mutation can do one of the following
            things with different probabilities:&lt;/p&gt; &lt;ul&gt;
            &lt;li&gt;Probability &lt;i&gt;p/4&lt;/i&gt;: Add a newly generated
            attribute to the feature vector&lt;/li&gt; &lt;li&gt;Probability
            &lt;i&gt;p/4&lt;/i&gt;: Add a randomly chosen original attribute to
            the feature vector&lt;/li&gt; &lt;li&gt;Probability
            &lt;i&gt;p/2&lt;/i&gt;: Remove a randomly chosen attribute from the
            feature vector&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thus it is guaranteed
            that the length of the feature vector can both grow and shrink. On
            average it will keep its original length, unless longer or shorter
            individuals prove to have a better fitness.&lt;/p&gt; &lt;p&gt;In
            addition to the usual YAGGA operator, this operator allows more
            feature generators and provides several techniques for intron
            prevention. This leads to smaller example sets containing less
            redundant features.&lt;/p&gt; &lt;p&gt;Since this operator does not
            contain algorithms to extract features from value series, it is
            restricted to example sets with only single attributes. For
            (automatic) feature extraction from values series the value series
            plugin for RapidMiner should be used.&lt;/p&gt; &lt;p&gt;For more
            information please refer to&lt;/p&gt; &lt;p&gt;Mierswa, Ingo (2007):
            &lt;em&gt;RobustGP: Intron-Free Multi-Objective Feature
            Construction&lt;/em&gt; (to appear)&lt;/p&gt;</help>
    <key>optimize_by_generation_yagga2</key>
    <shortName>Generate</shortName>
  </operator>
    <operator>
        <name>Superset</name>
        <synopsis>This operator gets two example sets and adds new features to
            each of both example sets so that both example sets consist of the
            same set of features.</synopsis>
        <help>This operator gets two example sets and adds new features to
            each of both example sets so that both example sets consist of the
            same set of features. This set is the union or the superset of both
            original feature sets. The values of the new features are set to
            missing. This operator only works on the regular attributes and will
            not change, add, or otherwise modify the existing special attributes.
        </help>
    <key>superset</key>
  </operator>
    <operator>
        <name>Hyper Hyper</name>
        <synopsis>This is a minimal SVM implementation. The model is built
            with only one positive and one negative example. Typically this
            operater is used in combination with a boosting method.</synopsis>
        <help>This is a minimal SVM implementation. The model is built with
            only one positive and one negative example. Typically this operator
            is used in combination with a boosting method.</help>
    <key>hyper_hyper</key>
    <shortName>SVM</shortName>
  </operator>
    <operator>
        <name>Scale by Weights</name>
        <synopsis>Deselects attributes with weight 0 and calculates new values
            for numeric attributes.</synopsis>
        <help>&lt;p&gt;This operator deselects attributes with a weight value
            of 0.0. The values of the other numeric attributes will be
            recalculated based on the weights delivered as
            &lt;i&gt;AttributeWeights&lt;/i&gt; object in the input.&lt;/p&gt;
            &lt;p&gt;This operator can hardly be used to select a subset of
            features according to weights determined by a former weighting
            scheme. For this purpose the operator
            &lt;i&gt;AttributeWeightSelection&lt;/i&gt; should be used which will
            select only those attribute fulfilling a specified weight
            relation.&lt;/p&gt;</help>
    <key>scale_by_weights</key>
  </operator>
    <operator>
        <name>Generate TFIDF</name>
        <synopsis>Performs a TF-IDF filtering to the input data set.
        </synopsis>
        <help>
    This operator generates TF-IDF values from the input data. The input 
    example set must contain either the binary occurrences, which will be 
    normalized during calculation of the term frequency TF, or it already 
    contains the calculated term frequency values (in this case no 
    normalization will be done). The behavior can be selected using the 
    parameter calculate term frequencies.
  </help>
    <key>generate_tfidf</key>
  </operator>
    <operator>
        <name>Support Vector Machine (Evolutionary)</name>
        <synopsis>EvoSVM uses an Evolutionary Strategy for optimization.
        </synopsis>
        <help>&lt;p&gt;This is a SVM implementation using an evolutionary
            algorithm (ES) to solve the dual optimization problem of a SVM. It
            turns out that on many datasets this simple implementation is as fast
            and accurate as the usual SVM implementations. In addition, it is
            also capable of learning with Kernels which are not positive
            semi-definite and can also be used for multi-objective learning which
            makes the selection of C unecessary before learning.&lt;/p&gt;
            &lt;p&gt;Mierswa, Ingo. Evolutionary Learning with Kernels: A Generic
            Solution for Large Margin Problems. In Proc. of the Genetic and
            Evolutionary Computation Conference (GECCO 2006), 2006.&lt;/p&gt;
        </help>
    <key>support_vector_machine_evolutionary</key>
    <shortName>SVM</shortName>
  </operator>
    <operator>
        <name>InteractiveAttributeWeighting</name>
        <synopsis>Shows a window with feature weights and allows users to
            change them.</synopsis>
        <help>This operator shows a window with the currently used attribute
            weights and allows users to change the weight interactively.</help>
    <key>interactiveattributeweighting</key>
  </operator>

    <operator>
        <name>Set Macro</name>
        <synopsis>This operator can be used to define a single arbitrary macro
            which can be used by %{my_macro} in parameter values.</synopsis>
        <help>&lt;p&gt;(Re-)Define macros for the current process. Macros will
            be replaced in the value strings of parameters by the macro values
            defined as a parameter of this operator. In contrast to the usual
            MacroDefinitionOperator, this operator only supports the definition
            of a single macro and can hence be used inside of parameter
            iterations.&lt;/p&gt; &lt;p&gt;You have to define the macro name
            (without the enclosing brackets) and the macro value. The defined
            macro can then be used in all succeeding operators as parameter
            value. A macro must then be enclosed by
            &amp;quot;MACRO_START&amp;quot; and
            &amp;quot;MACRO_END&amp;quot;.&lt;/p&gt; &lt;p&gt;There are several
            predefined macros:&lt;/p&gt; &lt;ul&gt;
            &lt;li&gt;MACRO_STARTprocess_nameMACRO_END: will be replaced by the
            name of the process (without path and extension)&lt;/li&gt;
            &lt;li&gt;MACRO_STARTprocess_fileMACRO_END: will be replaced by the
            file name of the process (with extension)&lt;/li&gt;
            &lt;li&gt;MACRO_STARTprocess_pathMACRO_END: will be replaced by the
            complete absolute path of the process file&lt;/li&gt; &lt;/ul&gt;
            &lt;p&gt;In addition to those the user might define arbitrary other
            macros which will be replaced by arbitrary strings during the process
            run. Please note also that several other short macros exist, e.g.
            MACRO_STARTaMACRO_END for the number of times the current operator
            was applied. Please refer to the section about macros in the
            RapidMiner tutorial. Please note also that other operators like the
            &lt;i&gt;FeatureIterator&lt;/i&gt; also add specific
            macros.&lt;/p&gt;</help>
    <key>set_macro</key>
    <tags>
         <tag>Process</tag>
         <tag>Variable</tag>
         <tag>Setting</tag>
    </tags>
  </operator>
    <operator>
        <name>Naive Bayes</name>
        <synopsis>Returns classification model using estimated normal
            distributions.</synopsis>
        <help>Naive Bayes learner.</help>
    <key>naive_bayes</key>
    <tags>
         <tag>Supervised</tag>
         <tag>Classification</tag>
         <tag>Model</tag>
         <tag>Distribution</tag>
         <tag>Gaussian</tag>
         <tag>Multinominal</tag>
         <tag>Likelihoods</tag>
         <tag>Probability</tag>
         <tag>Probabilities</tag>
    </tags>
  </operator>
    <operator>
        <name>Apply Cluster Model</name>
        <synopsis>Labels an example set with the cluster ids from a given non
            hierarchical cluster model.</synopsis>
        <help>This Operator clusters an exampleset given a cluster model. If
            an exampleSet does not contain an id attribute it is probably not the
            same as the cluster model has been created on. Since cluster models
            depend on a static nature of the id attributes, the outcome on
            another exampleset with different values but same ids will be
            unpredictable and hence not automatically creation of ids is
            performed. Only centroid based clusterings support assiging unseen
            examples to clusters. *</help>
    <key>apply_cluster_model</key>
  </operator>
    <operator>
        <name>Extract Cluster Prototypes</name>
        <synopsis>Generates an ExampleSet consisting of the Cluster Prototypes.</synopsis>
        <help>Flat cluster algorithms like KMeans or KMedoids cluster the data around some
        prototypical data vectors. For example KMeans uses the centroid of all examples of a 
        cluster. This operator now extracts these prototypes and stores them in an ExampleSet
        for further use. 
        </help>
    <key>extract_prototypes</key>
  </operator>
    <operator>
        <name>Experiment</name>
        <synopsis>The root operator chain, which needs to be the outer most
            operator of any experiment.</synopsis>
        <help>Each process must contain exactly one operator of this class and
            it must be the root operator of the process. The only purpose of this
            operator is to provide some parameters that have global relevance.
        </help>
    </operator>
    <operator>
        <name>Free Memory</name>
        <synopsis>Frees unused memory. Might be useful after large
            preprocessing chains with a lot of (now) unused views or even data
            copies. Can be very useful after the data set was (again)
            materialized in memory.</synopsis>
        <help>Cleans up unused memory resources. Might be very useful in
            combination with the &lt;i&gt;MaterializeDataInMemory&lt;/i&gt;
            operator after large preprocessing trees using lot of views or data
            copies. Internally, this operator simply invokes a garbage collection
            from the underlying Java programming language.</help>
    <key>free_memory</key>
  </operator>
    <operator>
        <name>Bagging</name>
        <synopsis>Bagging operator allowing all learners (not restricted to
            Weka learners).</synopsis>
        <help>This Bagging implementation can be used with all learners
            available in RapidMiner, not only the ones which originally are part
            of the Weka package.</help>
    <key>bagging</key>
    <tags>
         <tag>Ensembles</tag>
         <tag>Bootstrap</tag>
         <tag>Aggregating</tag>
         <tag>Breiman</tag>
         <tag>Variance</tag>
         <tag>Bias</tag>
         <tag>Stability</tag>
    </tags>
  </operator>
    <operator>
        <name>Handle Exception</name>
        <synopsis>This operator performs the inner operator and neglects any
            errors. In this case, no inner output will be returned.</synopsis>
        <help>&lt;p&gt;This operator performs the inner operators and delivers
            the result of the inner operators. If any error occurs during this
            subprocess, this error will be neglected and this operator simply
            will return no additional input.&lt;/p&gt; &lt;p&gt;Please use this
            operator with care since it will also cover errors which are not
            expected by the analyst. In combination with a process branch,
            however, it can be used to handle exceptions in the analysis process
            (i.e. expected errors). &lt;/p&gt;</help>
    <key>handle_exception</key>
  </operator>
  <operator>
  		<name>Throw Exception</name>
  		<synopsis>This operator makes an exception occur.</synopsis>
  		<help>This operator will make an exception rise, which will cause the 
  		process to fail if the exception is not neglected by a Handle Exception operator.</help>
  	<key>throw_exception</key>
  </operator>
    <operator>
        <name>Cluster Density Performance</name>
        <synopsis>Delivers a performance based on cluster densities.
        </synopsis>
        <help>This operator is used to evaluate a non-hierarchical cluster
            model based on the average within cluster similarity/distance. It is
            computed by averaging all similarities / distances between each pair
            of examples of a cluster.</help>
    <key>cluster_density_performance</key>
    <shortName>Performance</shortName>
  </operator>
    <operator>
        <name>Optimize by Generation (YAGGA)</name>
        <synopsis>Yet Another GGA (Generating Geneting Algorithm). On average
            individuals (= selected attribute sets) will keep their original
            length, unless longer or shorther ones prove to have a better
            fitness.</synopsis>
        <help>YAGGA is an acronym for Yet Another Generating Genetic
            Algorithm. Its approach to generating new attributes differs from the
            original one. The (generating) mutation can do one of the following
            things with different probabilities: &lt;ul&gt; &lt;li&gt;Probability
            &lt;i&gt;p/4&lt;/i&gt;: Add a newly generated attribute to the
            feature vector&lt;/li&gt; &lt;li&gt;Probability
            &lt;i&gt;p/4&lt;/i&gt;: Add a randomly chosen original attribute to
            the feature vector&lt;/li&gt; &lt;li&gt;Probability
            &lt;i&gt;p/2&lt;/i&gt;: Remove a randomly chosen attribute from the
            feature vector&lt;/li&gt; &lt;/ul&gt; Thus it is guaranteed that the
            length of the feature vector can both grow and shrink. On average it
            will keep its original length, unless longer or shorter individuals
            prove to have a better fitness. Since this operator does not contain
            algorithms to extract features from value series, it is restricted to
            example sets with only single attributes. For (automatic) feature
            extraction from values series the value series plugin for RapidMiner
            written by Ingo Mierswa should be used. It is available at &lt;a
            href="http://rapid-i.com"&gt;http://rapid-i.com&lt;/a&gt;.</help>
    <key>optimize_by_generation_yagga</key>
    <shortName>Generate</shortName>
  </operator>
    <operator>
        <name>Stem (Lovins)</name>
        <synopsis>The Lovins stemmer for English texts.</synopsis>
        <help>The Lovinsstemmer for english texts.</help>
    <key>stem_lovins</key>
  </operator>
    <operator>
        <name>Add Noise</name>
        <synopsis>Adds noise to existing attributes or add random attributes.
        </synopsis>
        <help>This operator adds random attributes and white noise to the
            data. New random attributes are simply filled with random data which
            is not correlated to the label at all. Additionally, this operator
            might add noise to the label attribute or to the regular attributes.
            In case of a numerical label the given
            &lt;code&gt;label_noise&lt;/code&gt; is the percentage of the label
            range which defines the standard deviation of normal distributed
            noise which is added to the label attribute. For nominal labels the
            parameter &lt;code&gt;label_noise&lt;/code&gt; defines the
            probability to randomly change the nominal label value. In case of
            adding noise to regular attributes the parameter
            &lt;code&gt;default_attribute_noise&lt;/code&gt; simply defines the
            standard deviation of normal distributed noise without using the
            attribute value range. Using the parameter list it is possible to set
            different noise levels for different attributes. However, it is not
            possible to add noise to nominal attributes.</help>
    <key>add_noise</key>
  </operator>
    <operator>
        <name>Optimize Parameters (Quadratic)</name>
        <synopsis>This operator finds the optimal values for parameters using
            a quadratic interaction model.</synopsis>
        <help>This operator finds the optimal values for a set of parameters
            using a quadratic interaction model. The parameter
            &lt;var&gt;parameters&lt;/var&gt; is a list of key value pairs where
            the keys are of the form
            &lt;code&gt;OperatorName.parameter_name&lt;/code&gt; and the value is
            a comma separated list of values (as for the
            GridParameterOptimization operator). &lt;br/&gt; The operator returns
            an optimal &lt;i&gt;ParameterSet&lt;/i&gt; which can as well be
            written to a file with a &lt;i&gt;ParameterSetLoader&lt;/i&gt;. This
            parameter set can be read in another process using an
            &lt;i&gt;ParameterSetLoader&lt;/i&gt;. &lt;br/&gt; The file format of
            the parameter set file is straightforward and can also easily be
            generated by external applications. Each line is of the form
            &lt;center&gt;&lt;code&gt;operator_name.parameter_name =
            value&lt;/code&gt;&lt;/center&gt;.</help>
    <key>optimize_parameters_quadratic</key>
  </operator>
    <operator>
        <name>Relative Regression</name>
        <synopsis>Learns a regression model for predictions relative to
            another attribute value.</synopsis>
        <help>This meta regression learner transforms the label on-the-fly
            relative to the value of the specified attribute. This is done right
            before the inner regression learner is applied. This can be useful in
            order to allow time series predictions on data sets with large
            trends.</help>
    <key>relative_regression</key>
  </operator>
    <operator>
        <name>Map Clustering on Labels</name>
        <synopsis>This operator converts the cluster attribute into an
            prediction attribut, choosing the best fitting pairs between cluster
            and label.</synopsis>
        <help>This operator estimates a mapping between a given clustering and
            a prediction. It adjusts the given clusters with the given labels and
            so estimates the best fitting pairs.</help>
    <key>map_clustering_on_labels</key>
  </operator>
    <operator>
        <name>X-Validation</name>
        <synopsis>X-Validation encapsulates a cross-validation in order to
            estimate the performance of a learning operator.</synopsis>
        <help>&lt;p&gt;      &lt;code&gt;X-Validation&lt;/code&gt; performs a cross-validation process. The input &lt;i&gt;ExampleSet&lt;/i&gt;       &lt;i&gt;S&lt;/i&gt; is split up into &lt;var&gt;number of validations&lt;/var&gt; subsets &lt;i&gt;S_i&lt;/i&gt;.       The inner subprocesses are applied &lt;var&gt;number of validations&lt;/var&gt;       times using &lt;i&gt;S_i&lt;/i&gt; as the test set (input of the &lt;i&gt;Testing&lt;/i&gt;       subprocess) and &lt;i&gt;S \ S_i&lt;/i&gt; as training set (input of the &lt;i&gt;Training&lt;/i&gt;       subprocess).    &lt;/p&gt;    &lt;p&gt;      The &lt;i&gt;Training&lt;/i&gt; subprocess must return a model, which is usually       trained on the input &lt;i&gt;ExampleSet.&lt;/i&gt; The &lt;i&gt;Testing&lt;/i&gt; subprocess       must return a &lt;i&gt;PerformanceVector&lt;/i&gt;. This is usually generated by       applying the model and measuring it's performance. Additional objects       might be passed from the &lt;i&gt;Training&lt;/i&gt; to the &lt;i&gt;Testing&lt;/i&gt;       subprocess using the through ports. Please note that the performance calculated by this estimation scheme is only an estimation of the performance which would be achieved with the model built on the complete delivered data set instead of an exact calculation. Exactly this model, hence the one built on the complete input data, is delivered at the corresponding port in order to give convenient access to this model. &lt;/p&gt;    &lt;p&gt;          &lt;/p&gt;    &lt;p&gt;      Like other validation schemes the RapidMiner cross validation can use       several types of sampling for building the subsets    &lt;/p&gt;    &lt;p&gt;      Linear sampling simply divides the example set into partitions without       changing the order of the examples. Shuffled sampling build random       subsets from the data. Stratifed sampling builds random subsets and       ensures that the class distribution in the subsets is the same as in the       whole example set. For having the random splits independent from the rest of the process, a local random seed might be used. See the parameters       for details.    &lt;/p&gt;    &lt;p&gt;          &lt;/p&gt;    &lt;p&gt;      The cross validation operator provides several values which can be       logged by means of a     &lt;/p&gt;    &lt;a href="rm://opdoc/log"&gt;Log&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;    &lt;p&gt;      . Of course the number of the current iteration can be logged which       might be useful for ProcessLog operators wrapped inside a cross       validation. Beside that, all performance estimation operators of       RapidMiner provide access to the average values calculated during the       estimation. Since the operator cannot ensure the names of the delivered       criteria, the ProcessLog operator can access the values via the generic       value names:    &lt;/p&gt;    &lt;ul&gt;      &lt;li&gt;        performance: the value for the main criterion calculated by this         validation operator      &lt;/li&gt;      &lt;li&gt;        performance1: the value of the first criterion of the performance         vector calculated      &lt;/li&gt;      &lt;li&gt;        performance2: the value of the second criterion of the performance         vector calculated      &lt;/li&gt;      &lt;li&gt;        performance3: the value of the third criterion of the performance         vector calculated      &lt;/li&gt;      &lt;li&gt;        for the main criterion, also the variance and the standard deviation         can be accessed where applicable.      &lt;/li&gt;    &lt;/ul&gt;</help>
    <key>x_validation</key>
    <tags>
         <tag>Cross-Validations</tag>
         <tag>Cross-validations</tag>
         <tag>Folds</tag>
         <tag>K-Folds</tag>
         <tag>K-folds</tag>
         <tag>Validations</tag>
         <tag>Estimations</tag>
         <tag>Evaluations</tag>
         <tag>Performances</tag>
         <tag>Splitting</tag>
    </tags>
    <shortName>Validation</shortName>
  </operator>
    <operator>
        <name>Format Numbers</name>
        <synopsis>Reformats all numerical attributes according to the
            specified settings and change the attributes to nominal.</synopsis>
        <help>&lt;p&gt;This operator tries to parse numerical values and
            formats them in the specified number format. It also supports
            different kinds of numbers, including integers (123), fixed-point
            numbers (123.4), scientific notation (1.23E4), percentages (12%), and
            currency amounts ($123). The format type parameter specifies the
            basic format, in all cases but for &amp;quot;pattern&amp;quot; the
            specified locale will be used. In case of pattern the locale is
            ignored and the specified pattern is used instead.&lt;/p&gt;
            &lt;p&gt;Please note that this operator only works on numerical
            attributes and the result will be in any case a nominal attribute no
            matter if the resulting format would again be a parsable
            number.&lt;/p&gt; &lt;p&gt;In case of the pattern format type, a
            pattern parameter is used to define the format. If two different
            formats for positive and negative numbers should be used, those
            formats can be defined by a separating ';'. The pattern must have the
            following structure: &lt;br /&gt;&lt;br /&gt; pattern :=
            subpattern{;subpattern} &lt;br /&gt; subpattern :=
            {prefix}integer{.fraction}{suffix} &lt;br /&gt; prefix := any
            character combination including white space &lt;br /&gt; suffix :=
            any character combination including white space &lt;br /&gt; integer
            := '#'* '0'* '0' &lt;br /&gt; fraction := '0'* '#'* &lt;br /&gt;
            &lt;/p&gt; &lt;p&gt;The following placeholders can be used within the
            pattern parameter: &lt;br /&gt;&lt;br /&gt; 0 &amp;nbsp;&amp;nbsp;a
            digit &lt;br /&gt; # &amp;nbsp;&amp;nbsp;a digit, zero shows as
            absent &lt;br /&gt; . &amp;nbsp;&amp;nbsp;placeholder for decimal
            separator &lt;br /&gt; , &amp;nbsp;&amp;nbsp;placeholder for grouping
            separator. &lt;br /&gt; E &amp;nbsp;&amp;nbsp;separates mantissa and
            exponent for exponential formats. &lt;br /&gt; -
            &amp;nbsp;&amp;nbsp;default negative prefix. &lt;br /&gt; %
            &amp;nbsp;&amp;nbsp;multiply by 100 and show as percentage &lt;br
            /&gt; X &amp;nbsp;&amp;nbsp;any other characters can be used in the
            prefix or suffix &lt;br /&gt; ' &amp;nbsp;&amp;nbsp;used to quote
            special characters in a prefix or suffix. &lt;br /&gt; &lt;/p&gt;
        </help>
    <key>format_numbers</key>
  </operator>
    <operator>
        <name>IteratingGSS</name>
        <synopsis>Combines Generic Sequential Sampling by Scheffer/Wrobel with
            Knowledge-Based Sampling by Scholz.</synopsis>
        <help>This operator implements the IteratingGSS algorithms presented
            in the diploma thesis 'Effiziente Entdeckung unabh&amp;auml;ngiger
            Subgruppen in grossen Datenbanken' at the Department of Computer
            Science, University of Dortmund.</help>
    <key>iteratinggss</key>
  </operator>
    <operator>
        <name>Create Learning Curve</name>
        <synopsis>Iterates its inner operator for an increasing number of
            samples and collects the performances.</synopsis>
        <help>This operator first divides the input example set into two
            parts, a training set and a test set according to the parameter
            &amp;quot;training_ratio&amp;quot;. It then uses iteratively bigger
            subsets from the fixed training set for learning (the first
            subprocess) and calculates the corresponding performance values on
            the fixed test set (with the second subprocess).</help>
    <key>create_learning_curve</key>
  </operator>
    <operator>
        <name>Generate Item Set Indicators</name>
        <synopsis>Creates attributes from frequent item sets.</synopsis>
        <help>This operator takes a FrequentItemSet set within IOObjects and
            creates attributes for every frequent item set. This attributes
            indicate if the examples contains all items of this set. The
            attributes will contain values 0 or 1 and are numerical.</help>
    <key>generate_item_set_indicators</key>
  </operator>
    <operator>
        <name>Filter Stopwords (French)</name>
        <synopsis>Standard stopwords list for french texts.</synopsis>
        <help>A standard stopword operator for french texts, which removes
            every token equal to a stopword.</help>
    <key>filter_stopwords_french</key>
  </operator>
    <operator>
        <name>ExampleSet2ClusterModel</name>
        <synopsis>Clustering based on one nominal attribute.</synopsis>
        <help>This operator creates a flat cluster model using a nominal
            attribute and dividing the exampleset by this attribute over the
            clusters. Every value is mapped onto a cluster, including the unknown
            value. This operator will create a cluster attribute if not present
            yet.</help>
    <key>exampleset2clustermodel</key>
  </operator>
    <operator>
        <name>TextObjectLoader</name>
        <synopsis>Loads a textobject from file.</synopsis>
        <help>This operator loads a textObject from a textFile.</help>
    </operator>

    <operator>
        <name>Write CSV</name>
        <synopsis>This operator can write csv files.</synopsis>
        <help>&lt;p&gt;This operator can be used to write data into CSV files
            (Comma Separated Values). The values and columns are separated by
            &amp;quot;;&amp;quot;. Missing data values are indicated by empty
            cells.&lt;/p&gt;</help>
    <key>write_csv</key>
    <tags>
         <tag>Save</tag>
         <tag>Export</tag>
         <tag>Write</tag>
         <tag>Datasets</tag>
         <tag>Files</tag>
         <tag>Text</tag>
         <tag>Commas</tag>
    </tags>
  </operator>
    <operator>
        <name>Relevance Vector Machine</name>
        <synopsis>An implementation of a relevance vector machine.</synopsis>
        <help>Relevance Vector Machine (RVM) Learner. The RVM is a
            probabilistic method both for classification and regression. The
            implementation of the relevance vector machine is based on the
            original algorithm described by Tipping/2001. The fast version of the
            marginal likelihood maximization (Tipping/Faul/2003) is also
            available if the parameter &amp;quot;rvm_type&amp;quot; is set to
            &amp;quot;Constructive-Regression-RVM&amp;quot;.</help>
    <key>relevance_vector_machine</key>
  </operator>
    <operator>
        <name>Remove Attribute Range</name>
        <synopsis>This operator removes a range of features.</synopsis>
        <help>This operator removes the attributes of a given range. The first
            and last attribute of the range will be removed, too. Counting starts
            with 1.</help>
    <key>remove_attribute_range</key>
  </operator>
    <operator>
        <name>Real to Integer</name>
        <synopsis>Replaces all real-valued attributes by corresponding integer
            attributes (rounded or cutted).</synopsis>
        <help>Converts all real valued attributes to integer valued
            attributes. Each real value is simply cutted (default) or rounded. If
            the value is missing, the new value will be missing.</help>
    <key>real_to_integer</key>
  </operator>
    <operator>
        <name>Optimize Weights (Forward)</name>
        <synopsis>Assumes that features are independent and optimizes the
            weights of the attributes with a linear search.</synopsis>
        <help>This operator performs the weighting under the naive assumption
            that the features are independent from each other. Each attribute is
            weighted with a linear search. This approach may deliver good results
            after short time if the features indeed are not highly correlated.
        </help>
    <key>optimize_weights_forward</key>
  </operator>
    <operator>
        <name>Tree to Rules</name>
        <synopsis>Determines a set of rules from a given decision tree model.
        </synopsis>
        <help>This meta learner uses an inner tree learner and creates a rule
            model from the learned decision tree.</help>
    <key>tree_to_rules</key>
  </operator>
    <operator>
        <name>Generate Sales Data</name>
        <synopsis>Generates data for testing purposes based on a sales data
            set.</synopsis>
        <help>Generates a random example set for testing purposes. The data
            represents a sales example set.</help>
    <key>generate_sales_data</key>
  </operator>

    <operator>
        <name>Discretize by Frequency</name>
        <synopsis>Discretize numerical attributes into a user defined number
            of bins with equal frequency.</synopsis>
        <help>This operator discretizes all numeric attributes in the dataset
            into nominal attributes. This discretization is performed by equal
            frequency binning, i.e. the thresholds of all bins is selected in a
            way that all bins contain the same number of numerical values. The
            number of bins is specified by a parameter, or, alternatively, is
            calculated as the square root of the number of examples with
            non-missing values (calculated for every single attribute). Skips all
            special attributes including the label. Note that it is possible to
            get bins with different numbers of examples. This might occur, if the
            attributes's values are not unique, since the algorithm can not split
            between examples with same value.</help>
    <key>discretize_by_frequency</key>
    <tags>
         <tag>Continous</tag>
         <tag>Categorical</tag>
         <tag>Nominal</tag>
         <tag>Polynominal</tag>
         <tag>Ordinary</tag>
         <tag>Discrete</tag>
         <tag>Discretization</tag>
         <tag>Dichotomization</tag>
         <tag>Dichotomy</tag>
         <tag>Binning</tag>
         <tag>Histogram</tag>
         <tag>Types</tag>
         <tag>Qualitative</tag>
         <tag>Quantitative</tag>
         <tag>Groups</tag>
         <tag>Intervals</tag>
    </tags>
    <shortName>Discretize</shortName>
  </operator>
    <operator>
        <name>MissingValueReplenishmentView</name>
        <synopsis>Replaces missing values in examples. In contrast to the
            usual missing value replenishment, this operator does not change the
            underlying data but replaces the missing values on the fly by using a
            new view on the data.</synopsis>
        <help>This operator simply creates a new view on the input data
            without changing the actual data or creating a new data table. The
            new view will not contain any missing values for regular attributes
            but the mean value (or mode) of the non-missing values instead.
        </help>
    <key>missingvaluereplenishmentview</key>
  </operator>
    <operator>
        <name>Filter Stopwords (Czech)</name>
        <synopsis>Standard stopwords list for czech texts.</synopsis>
        <help>A standard stopword operator for czech texts, which removes
            every token equal to a stopword.</help>
    <key>filter_stopwords_czech</key>
  </operator>
  <operator>
    <name>IOContainerReader</name>
  <key>iocontainerreader</key>
  </operator>
  <operator>
    <name>IOContainerWriter</name>
  <key>iocontainerwriter</key>
  </operator>
  <operator>
    <name>Decision Tree (Multiway)</name>
  <key>decision_tree_multiway</key>
  <tags>
       <tag>Supervised</tag>
       <tag>Classification</tag>
       <tag>Id3</tag>
       <tag>J48</tag>
       <tag>J4.8</tag>
       <tag>C45</tag>
       <tag>C4.5</tag>
       <tag>C50</tag>
       <tag>C5.0</tag>
       <tag>Cart</tag>
       <tag>Chaid</tag>
  </tags>
  </operator>
  <operator>
    <name>Weight by Correlation</name>
    <synopsis>Performs an example weighting based upon the residuals of an unweighted local polynomial regression.</synopsis>
    <help>&lt;p&gt;This operator provides a weighting scheme based upon correlation. It
      calculates the correlation of each attribute with the label attribute and returns
      the absolute or squared value as its weight.&lt;/p&gt;  
      &lt;p&gt;Please keep in mind, that polynomial classes provide no information about 
      their ordering, so that the weights are more or less random, because depending on 
      the internal numerical representation of the classes. Binominal labels work because
      of the representation as 0 and 1, as do numerical ones.&lt;/p&gt;
    </help>
  <key>weight_by_correlation</key>
  <tags>
       <tag>Weighting</tag>
       <tag>Importance</tag>
       <tag>Influence</tag>
       <tag>Significance</tag>
       <tag>Factors</tag>
       <tag>Relevance</tag>
  </tags>
  </operator>  
<operator>
    <key>loop_collection</key>
    <name>Loop Collection</name>
  <synopsis>
    Iterates over a collection of objects.
  </synopsis>
    <help>
    The subprocess of this operator is executed once for every entry in the 
    input collection. If &amp;quot;unfold&amp;quot; is checked, the input list is flattened, 
    i.e. nested collections are replaced by their entries. The outputs of the 
    iterations are grouped into collections again. Since the inner subprocess 
    can have multiple results, the output of the &lt;i&gt;Loop Collection&lt;/i&gt; 
    operator consits of multiple collections, one for each result of the 
    subprocess.&lt;br/&gt;
    Objects can be grouped into a collection using the
    &lt;a href="rm://opdoc/collect"&gt;Collect&lt;/a&gt; operator.
  </help>
  </operator>
  <operator>
    <key>write_document</key>
    <name>Write Document</name>
  </operator>
  <operator>
    <key>read_document</key>
    <name>Read Document</name>
  <synopsis/>
  </operator>
  <operator>
    <key>transpose</key>
    <tags>
         <tag>Rotate</tag>
         <tag>Rotations</tag>
         <tag>Transpositions</tag>
         <tag>Pivoting</tag>
         <tag>Long</tag>
         <tag>Wide</tag>
    </tags>
    <name>Transpose</name>
  </operator>
  <operator>
    <key>select</key>
    <tags>
         <tag>Collections</tag>
    </tags>
    <name>Select</name>
  <synopsis>
    Selects a single entry from an object collection.
  </synopsis>
    <help>
    Selects the index-th entry from a collection of objects. If unfold is 
    checked, the index refers to the index in the flattened list, i.e. the 
    list obtained from the input list by replacing all nested collections by 
    their elements. Objects can be grouped into a collection using the
    &lt;a href="rm://opdoc/collect"&gt;Collect&lt;/a&gt; operator.
  </help>
  </operator>
  <operator>
    <key>flatten_collection</key>
    <name>Flatten Collection</name>
  <synopsis>
    Receives a collection of collections and unions all content into a single collection.
  </synopsis>
    <help>
    This operator receives a collection of collections and unions the content of each
    collection into a single collection. If your collection contains three collections
    which contain 5 IOObject the result will be a single collection with 15 IOObjects. 
	&lt;br/&gt;
	After merging objects into a collection, 
    the &lt;a href="rm://opdoc/loop_collection"&gt;Loop Collection&lt;/a&gt; operator can be used to iterate over this 
    collection. The &lt;a href="rm://opdoc/select"&gt;Select&lt;/a&gt; operator can be used to retrieve a certain 
    element of the collection.&lt;br/&gt;
    
    Note: In the process layout view, collections are indicated by double lines.
  </help>
  </operator>
  <operator>
    <key>collect</key>
    <name>Collect</name>
  <synopsis>
    Joins multiple input objects to a single collection.
  </synopsis>
    <help>
    This operator joins a variable number of input objects to a single 
    collection. If the input objects are collections themselves and the 
    &amp;quot;unfold&amp;quot; parameter is checked, the output will be the union of the 
    elements of all input collections. After merging objects into a collection, 
    the &lt;a href="rm://opdoc/loop_collection"&gt;Loop Collection&lt;/a&gt; operator can be used to iterate over this 
    collection. The &lt;a href="rm://opdoc/select"&gt;Select&lt;/a&gt; operator can be used to retrieve a certain 
    element of the collection.&lt;br/&gt;
    Note: In the process layout view, collections are indicated by double lines.
  </help>
  </operator>
<group>
    <key>deprecated</key>
    <name>Deprecated</name>
  </group>
  <group>
    <key>deprecated.todo</key>
    <name>Todo</name>
  </group>
  <group>
    <key>deprecated.useless</key>
    <name>Useless</name>
  </group>
  <group>
    <key>deprecated.deleted</key>
    <name>Deleted</name>
  </group>
  <group>
    <key>process_control</key>
    <name>Process Control</name>
  </group>
  <group>
    <key>process_control.parameter</key>
    <name>Parameter</name>
  </group>
  <group>
    <key>process_control.loop</key>
    <name>Loop (Deprecated)</name>
  </group>
  <group>
    <key>process_control.branch</key>
    <name>Branch</name>
  </group>
  <group>
    <key>process_control.collections</key>
    <name>Collections</name>
  </group>
  <group>
    <key>utility</key>
    <name>Utility</name>
  </group>
  <group>
    <key>utility.macros</key>
    <name>Macros</name>
  </group>
  <group>
    <key>utility.logging</key>
    <name>Logging</name>
  </group>
  <group>
    <key>utility.execution</key>
    <name>Execution</name>
  </group>
  <group>
    <key>utility.files</key>
    <name>Files</name>
  </group>
  <group>
    <key>utility.data_generation</key>
    <name>Data Generation</name>
  </group>
  <group>
    <key>repository_access</key>
    <name>Repository Access</name>
  </group>
  <group>
    <key>import</key>
    <name>Import</name>
  </group>
  <group>
    <key>import.data</key>
    <name>Data</name>
  </group>
  <group>
    <key>import.models</key>
    <name>Models</name>
  </group>
  <group>
    <key>import.attributes</key>
    <name>Attributes</name>
  </group>
  <group>
    <key>import.results</key>
    <name>Results</name>
  </group>
  <group>
    <key>import.other</key>
    <name>Other</name>
  </group>
  <group>
    <key>export</key>
    <name>Export</name>
  </group>
  <group>
    <key>export.data</key>
    <name>Data</name>
  </group>
  <group>
    <key>export.models</key>
    <name>Models</name>
  </group>
  <group>
    <key>export.attributes</key>
    <name>Attributes</name>
  </group>
  <group>
    <key>export.results</key>
    <name>Results</name>
  </group>
  <group>
    <key>export.other</key>
    <name>Other</name>
  </group>
  <group>
    <key>data_transformation</key>
    <name>Data Transformation</name>
  </group>
  <group>
    <key>data_transformation.type_conversion</key>
    <name>Type Conversion</name>
  </group>
  <group>
    <key>data_transformation.type_conversion.discretization</key>
    <name>Discretization</name>
  </group>
  <group>
    <key>data_transformation.attribute_space_transformation.generation</key>
    <name>Generation</name>
  </group>
  <group>
    <key>data_transformation.attribute_space_transformation.generation.optimization</key>
    <name>Optimization</name>
  </group>
  <group>
    <key>data_transformation.attribute_space_transformation.transformation</key>
    <name>Transformation</name>
  </group>
  <group>
    <key>data_transformation.attribute_space_transformation.selection</key>
    <name>Selection</name>
  </group>
  <group>
    <key>data_transformation.attribute_space_transformation.selection.optimization</key>
    <name>Optimization</name>
  </group>
  <group>
    <key>data_transformation.value_modification</key>
    <name>Value Modification</name>
  </group>
  <group>
    <key>data_transformation.data_cleansing</key>
    <name>Data Cleansing</name>
  </group>
  <group>
    <key>data_transformation.data_cleansing.outlier_detection</key>
    <name>Outlier Detection</name>
  </group>
  <group>
    <key>data_transformation.filtering</key>
    <name>Filtering</name>
  </group>
  <group>
    <key>data_transformation.filtering.sampling</key>
    <name>Sampling</name>
  </group>
  <group>
    <key>data_transformation.sorting</key>
    <name>Sorting</name>
  </group>
  <group>
    <key>data_transformation.rotation</key>
    <name>Rotation</name>
  </group>
  <group>
    <key>data_transformation.aggregation</key>
    <name>Aggregation</name>
  </group>
  <group>
    <key>data_transformation.set_operations</key>
    <name>Set Operations</name>
  </group>
  <group>
    <key>modeling</key>
    <name>Modeling</name>
  </group>
  <group>
    <key>modeling.classification_and_regression.lazy_modeling</key>
    <name>Lazy Modeling</name>
  </group>
  <group>
    <key>modeling.classification_and_regression.bayesian_modeling</key>
    <name>Bayesian Modeling</name>
  </group>
  <group>
    <key>modeling.classification_and_regression.tree_induction</key>
    <name>Tree Induction</name>
  </group>
  <group>
    <key>modeling.classification_and_regression.rule_induction</key>
    <name>Rule Induction</name>
  </group>
  <group>
    <key>modeling.classification_and_regression.neural_net_training</key>
    <name>Neural Net Training</name>
  </group>
  <group>
    <key>modeling.classification_and_regression.function_fitting</key>
    <name>Function Fitting</name>
  </group>
  <group>
    <key>modeling.classification_and_regression.logistic_regression</key>
    <name>Logistic Regression</name>
  </group>
  <group>
    <key>modeling.classification_and_regression.discriminant_analysis</key>
    <name>Discriminant Analysis</name>
  </group>
  <group>
    <key>modeling.weighting.optimization</key>
    <name>Optimization</name>
  </group>
  <group>
    <key>modeling.application.thresholds</key>
    <name>Thresholds</name>
  </group>
  <group>
    <key>modeling.application.confidences</key>
    <name>Confidences</name>
  </group>
  <group>
    <key>evaluation</key>
    <name>Evaluation</name>
  </group>
  <group>
    <key>evaluation.validation</key>
    <name>Validation</name>
  </group>
  <group>
    <key>evaluation.performance.attributes</key>
    <name>Attributes</name>
  </group>
  <group>
    <key>evaluation.performance.clustering</key>
    <name>Clustering</name>
  </group>
  <group>
    <key>evaluation.significance</key>
    <name>Significance</name>
  </group>
  <group>
    <key>series</key>
    <name>Series</name>
  </group>
  <group>
    <key>series.cleansing</key>
    <name>Cleansing</name>
  </group>
  <group>
    <key>series.transformation</key>
    <name>Transformation</name>
  </group>
  <group>
    <key>series.windowing</key>
    <name>Windowing</name>
  </group>
  <group>
    <key>series.evaluation</key>
    <name>Evaluation</name>
  </group>
  <group>
    <key>series.evaluation.validation</key>
    <name>Validation</name>
  </group>
  <group>
    <key>series.evaluation.performance</key>
    <name>Performance</name>
  </group>
  <group>
    <key>text_processing</key>
    <name>Text Processing</name>
  </group>
  <group>
    <key>text_processing.tokenization</key>
    <name>Tokenization</name>
  </group>
  <group>
    <key>text_processing.filtering</key>
    <name>Filtering</name>
  </group>
  <group>
    <key>text_processing.stemming</key>
    <name>Stemming</name>
  </group>
  <group>
    <key>text_processing.transformation</key>
    <name>Transformation</name>
  </group>
<operator>
    <key>dummy</key>
    <name>dummy</name>
    <help>Operator's description is missing in referenced OperatorDoc.</help>
  </operator>
<group>
    <key>pmml</key>
    <name>Pmml</name>
  </group>
  <operator>
      <key>apply_association_rules</key>
      <name>Apply Association Rules</name>
      <synopsis>This operator applies given association rules on an example set.</synopsis>
      <help>This operator creates a new confidence attribute for each item occurring in at least one
         conclusion of an association rule. Then it checks for each example and for each rule, if
         the example fulfills the premise of the rule. An example fulfills a premise, if the example
         covers all items in the premise. An example covers an item, if the attribute representing
         the item contains the positive value. If the check is positive, a confidence value for each
         item in the conclusion is derived. Which value is used, depends on the selected confidence
         aggregation method. There are two types: The binary choice will set a 1, for any item
         contained inside a fulfilled rule's conclusion. This is independent of how confident the
         rule was. Any aggregation choice will select the maximum of the previous and the new value
         of the selected confidence function.</help>
   </operator>
      <operator>
      <key>generalized_sequential_patterns</key>
      <shortName>GSP</shortName>
      <name>Generalized Sequential Patterns</name>
      <synopsis>This operator applies given association rules on an example set.</synopsis>
      <help>
      &lt;p&gt;
        This operator searches sequential patterns in a set of transactions. Each transaction must be encoded as an single
  example and must contain one attribute for the time and for the customer. This pair of attribute is used for generate
  one sequence per customer containing each single transaction ordered by the time of each transaction. The algorithm
  then searches sequential patterns in the form of: If a customer bought a and c in one transaction, he bought b in the
  next: &amp;lt;a, c&amp;gt; then &amp;lt;b&amp;gt;. The minimal support describes how many customer must support such a pattern for regarding it
  as frequent. Infrequent patterns will be dropped. A customer supports such a pattern, if there are some parts of his
  sequence including the pattern. The above pattern would be supported by a customer with this transactions: &amp;lt;s, g&amp;gt;
  then &amp;lt;a, s, c&amp;gt; then &amp;lt;b&amp;gt; then &amp;lt;f, h&amp;gt;.&lt;/p&gt;
  
  &lt;p&gt;
  The parameters min_gap, max_gap and window_size determine how transaction are handled. For example, if the above
  customer forgot to by c, and had to return 5 minutes later to buy c, then his transactions would look like that: &amp;lt;s,
  g&amp;gt; then &amp;lt;a, s&amp;gt; then &amp;lt;c&amp;gt; then &amp;lt;b&amp;gt; then &amp;lt;f  ^111, h&amp;gt; This would not support the pattern &amp;lt;a, c&amp;gt; then &amp;lt;b&amp;gt;. To avoid this
  problem, the window size determines, how long a subsequent transaction is treated as the same transaction. If the
  window size is larger than 5 minutes, the &amp;lt;c&amp;gt; would be treated as being part of the second transaction and hence this
  customer would support the above pattern.&lt;/p&gt;
  
  &lt;p&gt;
  The max_gap parameter causes a customers sequence not to support a pattern, if the transactions containing this
  pattern are to widely separated in time. The min_gap parameter does the same if they are to near.&lt;/p&gt;
      </help>
   </operator>
  
  <operator>
    <key>send_mail</key>
    <name>Send Mail</name>
	<synopsis>This operator sends a mail.</synopsis>    
    <help>This operator sends a mail with a given subject and body to a receiver.</help>
  </operator>
  

	<operator>
      <name>k-Means (fast)</name>
      <synopsis>This operator represents an implementation of k-Means according to C. Elkan.
      This operator will create a cluster attribute if not present yet. 
	</synopsis>
      <key>fast_k_means</key>
      <help>In contrast to the standard
      implementation of k-means, this implementation is much faster in many cases, especially for data sets with
      many attributes and a high k value, but it also needs more additional memory. For more information, please see paper:
     - Using the Triangle Inequality to Accelerate k-Means -
      Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003), Washington DC, 2003</help>
      <shortName>Clustering</shortName>
   </operator>
   
   <operator>
      <name>X-Means</name>
      <synopsis>Clustering using X-Means. This operator implements the algorithm publisehd by Dan Pelleg and Andrew Moore.</synopsis>
      <key>x_means</key>
     <tags>
         <tag>Unsupervised</tag>
         <tag>Clustering</tag>
         <tag>Segmentation</tag>
         <tag>Grouping</tag>
         <tag>Similarity</tag>
         <tag>Similarities</tag>
         <tag>Euclidean</tag>
         <tag>Distances</tag>
         <tag>Centroids</tag>
         <tag>X Means</tag>
         <tag>X means</tag>
         <tag>Xmeans</tag>
     </tags>
      <help>X-Means is a clustering algorithm which determines the correct number of centroids based on a heuristic. It begins with a minimum set of centroids and then iteratively exploits if using more centroids makes sense according to the data. If a cluster is split into two sub-clusters is determined by the Bayesian Information Criteria (BIC), balancing the trade-off between precision and model complexity. Original publication: "X-means: Extending K-means with Efficient Estimation of the Number of Clusters" by Dan Pelleg and Andrew Moore, Proceedings of the Seventeenth International Conference on Machine Learning, 2000.</help>
      <shortName>X-Means</shortName>
   </operator>
   
     
        <operator>
      <name>AutoMLP</name>
      <synopsis>AutoMLP learner.</synopsis>
      <help> 
      AutoMLP is a simple algorithm for both learning rate
and size adjustment of neural networks during training. The algorithm combines ideas from
genetic algorithms and stochastic optimization. It maintains a small ensemble of networks
that are trained in parallel with different rates and different numbers of hidden units. After
a small, fixed number of epochs, the error rate is determined on a validation set and the
worst performers are replaced with copies of the best networks, modified to have different
numbers of hidden units and learning rates. Hidden unit numbers and learning rates are
drawn according to probability distributions derived from successful rates and sizes.

For more information please be kindly referred to the following paper: AutoMLP: Simple, Effective, Fully Automated Learning Rate and Size Adjustment by 
Thomas Breuel, Faisal Shafait. The Learning Workshop, Cliff Lodge, Snowbird, Utah, United States, Online, 4/2010.

      </help>
      <key>auto_mlp</key>
   </operator>
        <operator>
      <name>Select Recall</name>
      <synopsis>This operator searches the threshold that would cause the recall to rise or fall to the given level.</synopsis>
      <help> 
      The given example set must contain a label and a compatible predicted label attribute together with all confidence attributes. Since Recall supports only 
      one or two classes, this operator will only work on data sets with one or two label values at most. The label value that should be treated positive, can be 
      defined in the operator.
      This operator will sort the given example set according to the confidence attribute of the positive label and searches the confidence threhsold, that would
      cause the desired recall. Of course this will affect the precision.
      </help>
      <key>select_recall</key>
   </operator>
     
   <operator>
      <name>Open File</name>
      <synopsis>Opens a file for processing by parsing operators.</synopsis>
      <help>Opens a file for processing by parsing operators. Even if this file points to a data file, like Excel or CSV, this
      operator returns an uninterpreted blob. You still need an operator for parsing the file like Read CSV or Read Excel.
      In most cases, you don't need an OpenFile operator to open such a file, you can simply use the corresponging read operator.
      The main use is in combination with some loop or with in a RapidMiner Server service reacting to a POST request.</help>
      <key>open_file</key>
   </operator>

   <operator>
      <name>Write File</name>
      <synopsis>Writes a file object to a file or to the repository as a blob.</synopsis>
      <help>This operator writes a file object to a file or to the repository as a blob. Note that you cannot use this to save an example set, model, or other object. This is used only for uninterpreted data delivered by Open File, etc.</help>
      <key>write_file</key>
   </operator>
   
   <operator>
	   <name>Create Archive File</name>
	   <key>create_archive_file</key>
   </operator>
     
   <operator>
	   <name>Add Entry to Archive File</name>
	   <key>add_entry_to_archive_file</key>
   </operator>

   <operator>
	   <name>Annotate</name>
	   <key>annotate</key>
   </operator>
   
   <operator>
	   <name>Annotations to Data</name>
	   <key>annotations_to_data</key>
   </operator>
   
   <operator>
	   <name>Data to Annotations</name>
	   <key>data_to_annotations</key>
   </operator>
   
   <operator>
     <name>Extract Macro from Annotation</name>
     <key>extract_macro_from_annotation</key>
   </operator>

</operatorHelp>